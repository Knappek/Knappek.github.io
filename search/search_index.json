{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my Technical Scratchpad","text":"<p>I use this page to document all kinds of stuff because I am getting old and rusty and forget things. So this page is mainly for me but I'm happy if others can benefit from it as well .</p> <p>If you have any questions, feedback or other comments to certain pages, please let me know by either creating a github issue or email me at andy.knapp.ak@gmail.com.</p>"},{"location":"#upcoming","title":"Upcoming","text":"<ul> <li>TKGS integration with vSphere CSI controller</li> <li>demistify Tanzu Kubernetes Grid flavours - TKGS, TKGM, TKGI</li> <li>Test TKGS/m disaster recover scenarios. What happens when:<ul> <li>CP node crashes</li> <li>kubelet on CP node crashes</li> <li>apiserver on CP node crashes</li> <li>etcd on CP node crashes</li> </ul> </li> </ul>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>Every content on this page represents my personal experiences and personal view only.</p>"},{"location":"homelab/","title":"Homelab","text":"<p>When I joined VMware in February 2021 I have built a homelab to be able to quickly spin up test environments using VMware products with a primary focus on Tanzu Labs product portfolio.</p>"},{"location":"homelab/#bom","title":"BOM","text":"Component Item CPU HP/AMD EPYC 7551 PS7551BDVIHAF 2GHz 64MB 32 Core Processor Motherboard Supermicro H11SSL-i Socket SP3 ATX RAM Samsung 4x 64GB 256GB DDR4 ECC RAM 2933 Mhz RDIMM SSD Crucial MX500 2TB NVMe Samsung MZ-V7S2T0BW SSD 970 EVO Plus 2 TB M.2 Internal NVMe SSD (up to 3.500 MB/s) Cooler Noctua NH U12s TR4 SP3 GPU Asus GeForce GT 710 1GB Case Fractal Design Core 2500 PSU Kolink Enclave 500W"},{"location":"homelab/#setup","title":"Setup","text":"<p>There is a \"Management vCenter\" (<code>vcenter-mgmt</code>) VM deployed on the physical host that manages the physical host. As a result, the Management vCenter looks like this:</p> <p></p> <p>Here you can see:</p> <ul> <li><code>192.168.178.100</code>: the physical ESXi host</li> <li><code>jumpbox01</code>: an Ubuntu jumpbox for testing purposes</li> <li>VMs prefixed with <code>tkgs-</code>: a nested lab environment (more details see Nested Lab Setup)</li> <li><code>vcenter-mgmt</code>: Management vCenter</li> <li><code>vrli</code>: vRealize Log Insight</li> <li><code>vrops</code>: vRealize Operations Manager</li> <li><code>vyos</code>: the VyOS Router, responsible for the entire homelab network</li> <li><code>windows</code>: a Windows VM</li> </ul>"},{"location":"homelab/#networking-routing","title":"Networking &amp; Routing","text":"<p>The physical ESXi host has a virtual Switch <code>vSwitch0</code> with 2 port groups <code>Management Network</code> (the default port group) and <code>Home Network</code>:</p> <p></p> <p>Apart from the physical host there is only <code>vyos</code> running in the same network. The VyOS router acts as the entry point and default gateway to the homelab. My home router has a static IP route configured to forward requests to my internal homelab IP ranges <code>172.20.0.0/16</code> and <code>172.30.0.0/16</code> to the VyOS router.</p> <p>The VyOS router has another NIC in a trunk port group deployed on a virtual distributed switch (<code>vds-internal</code>) via the management vCenter:</p> <p></p> <p>Looking into the vyos configuration we thus have two network interfaces - <code>eth0</code> is the <code>Home Network</code> and <code>eth1</code> is the trunk port group:</p> <pre><code>vyos@vyos# show interfaces\n ethernet eth0 {\n     address 192.168.178.101/24\n     hw-id 00:0c:29:85:a5:3b\n     offload {\n         gro\n         gso\n         sg\n         tso\n     }\n }\n ethernet eth1 {\n     hw-id 00:0c:29:85:a5:45\n     mtu 9000\n }\n loopback lo {\n }\n</code></pre> <p>Refer to the VyOS Quick Start guide for more information how to configure interfaces, protocols, firewalls and more.</p>"},{"location":"homelab/#enable-mac-learning","title":"Enable Mac Learning","text":"<p>Read Native MAC Learning in vSphere 6.7 removes the need for Promiscuous mode for Nested ESXi - I am not the best person to explain this .</p> <p>TL;DR: It's best to enable mac learning on all port groups deployed on the vDS <code>vds-internal</code> used for nested labs.</p> <p>Steps:</p> <ol> <li>Install Powershell</li> <li>open it in a terminal emulator: <code>pwsh</code></li> <li> <p>Connect to vCenter:</p> <pre><code>Connect-VIServer -Server 192.168.178.102 -Protocol https -User administrator@vsphere.local -Password VMware1!\n</code></pre> </li> <li> <p>download the powershell functions <code>Get-MacLearn</code> and <code>Set-MacLearn</code> from here</p> </li> <li> <p>Set Mac learning on a port group:</p> <pre><code>Set-MacLearn -DVPortgroupName @(\"Nested-01-DVPG\") -EnableMacLearn $true -EnablePromiscuous $false -EnableForgedTransmit $true -EnableMacChange $false\n</code></pre> </li> <li> <p>Get Mac learning details:</p> <pre><code>Get-MacLearn -DVPortgroupName @(\"Nested-01-DVPG\")\n</code></pre> </li> </ol>"},{"location":"homelab/#create-a-network-for-a-nested-lab-environment","title":"Create a network for a nested lab environment","text":"<p>In order to create different networks for different nested lab environments I have to:</p> <ol> <li>create distributed port group on the vDS <code>vds-internal</code> with a specific VLAN ID</li> <li>create an interface with the same VLAN ID number as <code>vif</code> on <code>eth1</code> in VyOS</li> </ol>"},{"location":"homelab/#nested-lab-network-example","title":"Nested Lab network example","text":"<p>We create a distributed port group called <code>TKGM</code> with VLAN ID <code>12</code>:</p> <p></p> <p>As a consequence we create a virtual interface in VyOS with ID <code>12</code> and an IP range of my choice:</p> <pre><code>set interfaces ethernet eth1 vif 12 address 172.20.12.1/22\nset interfaces ethernet eth1 vif 12 description TKGM\nset interfaces ethernet eth1 vif 12 mtu 9000\ncommit \nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show interfaces\n ethernet eth0 {\n     address 192.168.178.101/24\n     hw-id 00:0c:29:85:a5:3b\n     offload {\n         gro\n         gso\n         sg\n         tso\n     }\n }\n ethernet eth1 {\n     hw-id 00:0c:29:85:a5:45\n     mtu 9000\n     vif 12 {\n         address 172.20.12.1/22\n         description TKGM\n         mtu 9000\n     }\n }\n loopback lo {\n }\n</code></pre> <p>We can then specify this port group in the nested lab setup config file - see details below.</p>"},{"location":"homelab/#nested-lab-setup","title":"Nested Lab Setup","text":"<p>To bootstrap nested lab environments I am using vmware-lab-builder, Kudos to Matt .</p> <p>An example config I am using to deploy a TKGm environment is</p> <pre><code>---\n# SOFTWARE_DIR must contain all required software\nvc_iso: \"{{ lookup('env', 'SOFTWARE_DIR') }}/VMware-VCSA-all-7.0.3-19234570.iso\"\nesxi_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/Nested_ESXi7.0u3c_Appliance_Template_v1.ova\"\nnsx_alb_controller_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/controller-20.1.6-9132.ova\"\ntkgm_os_kubernetes_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/photon-3-kube-v1.25.7+vmware.2-tkg.1-8795debf8031d8e671660af83b673daa.ova\"\n\nenvironment_tag: \"tanzu-multi-cloud-avi\"  # Used to prepend object names in hosting vCenter\ndns_server: \"192.168.178.1\"\ndns_domain: \"home.local\"\nntp_server_ip: \"192.168.178.1\"\ndisk_mode: thin  # How all disks should be deployed\n# This will be set everywhere!\nnested_host_password: \"{{ opinionated.master_password }}\"\n\nhosting_vcenter:  # This is the vCenter which will be the target for nested vCenters and ESXi hosts\n  ip: \"192.168.178.102\"\n  username: \"{{ lookup('env', 'PARENT_VCENTER_USERNAME') }}\"\n  password: \"{{ lookup('env', 'PARENT_VCENTER_PASSWORD') }}\"\n  datacenter: \"Home\"  # Target for all VM deployment\n\n# This section is only referenced by other variables in this file\nopinionated:\n  master_password: \"VMware1!\"\n  number_of_hosts: 1  # number of ESXi VMs to deploy\n  nested_hosts:\n    cpu_cores: 12  # CPU count per nested host\n    ram_in_gb: 96  # memory per nested host\n    local_disks:  # (optional) this section can be removed to not modify local disks\n      - size_gb: 500\n        datastore_prefix: \"datastore\"  # omit this to not have a datastore created\n  hosting_cluster: Physical\n  hosting_datastore: NVME\n  hosting_network:\n    base:\n      port_group: TKGM\n      cidr: \"172.20.12.0/22\"\n      gateway: \"172.20.12.1\"\n      # A TKGM deployment requires 5 contiguous IPs. vCenter, Avi Controller, Esxi, 2 x Avi Service Engines.\n      starting_addr: \"172.20.12.10\"\n    # If using your own network you must provide at least a /24.\n    # In the example below the same subnet is used for both the workload nodes and the VIPs\n    # Avi will use the IP range defined vip_ip_range for VIPs\n    # TKG needs DHCP to be configured for the first half of the subnet\n    workload:\n      port_group: trunk\n      vlan_id: 24\n      cidr: \"172.20.24.0/24\"\n      gateway: \"172.20.24.1\"\n      vip_ip_range: \"172.20.24.128-172.20.24.254\"\n  avi_control_plane_ha_provider: true\n  # This public key will be assigned to the created VMs, so you must have the private key to be able to use SSH\n  ssh_public_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDNre3RSDIQIbswU/AbFMrdGDTRNIxXs1L1aY9ozDm/TsTKBf85Kr/0Bi3Az1DgOifR3s7SblZFhqKtGueqyv4NKoNy8dgxUsFzGKaRBmwGfCn9rp0hAV/r6BdyhRGloltaZ4KuR3v3AQTTjpyWPsf55wUIMYtOtrQ1tnNspmZmEyh11e8Hbwsq6jaVDCpSWkLPgir4lDTFANRpA/MaU9XUG3PjYszaNFgIScUwQVl0otmCpFgZgf9jXwy4K5CpG4u/1CSIA6H+5XAJaAzDNGcAIGQKaIAj6Cvd8QyUs+UEV5n6rJSsp+gnfu0gEYx4QmeWwcVdGu1Re18qHVgAP56gF58uV7p/V60Tlf9IFqkz67lwLlOuq9dfNmPGIJ/lydcLgcvXmJObyntm1jFi5ChtIrBd7uShh9b6wwOLhekv3TwOn0nHPjXDabVDcmfXVhAgyJyknCFv1Hm3UFwTngoc4WFm38wgxzajOzxt83b8bXXhMNaU8L8VXtVpfIrDHbAKhho6Aaf8JahKa46UxFa4yjkVfN75N4++8CdCpGiruLUp1rW4zYkrjxBuIBcib/06QTsvdPIsYVsPic/mYoxBVTclvLi/ELVdwFFSavwNKI+XwH6ENI7vckkVs6HpM0GJu4qsvmhrlkGVmNM0BkaJN1i1CQgn01OTz+um1rjr+Q== andy.knapp.ak@gmail.com\n\n#####################################################################\n### No need to edit below this line for an opinionated deployment ###\n#####################################################################\n\nnested_vcenter:  # the vCenter appliance that will be deployed\n  ip: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # vCenter ip address\n  mask: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n  gw: \"{{ opinionated.hosting_network.base.gateway }}\"\n  host_name: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # FQDN if there is working DNS server, otherwise put the ip as a name\n  username: \"administrator@vsphere.local\"\n  password: \"{{ opinionated.master_password }}\"\n  datacenter: \"Lab\"  # DC to create after deployment\n  # Below are properties of parent cluster\n  hosting_network: \"{{ opinionated.hosting_network.base.port_group }}\"  # Parent port group where the vCenter VM will be deployed\n  hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # Parent cluster where the vCenter VM will be deployed\n  hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Parent datastore where the vCenter VM will be deployed\n\nnested_clusters:  # You can add clusters in this section by duplicating the existing cluster\n  compute:  # This will be the name of the cluster in the nested  vCenter. Below are the minimum settings.\n    enable_drs: true\n    # HA can only be enabled if there is are datastores accessible by all hosts.\n    enable_ha: true\n    ha_host_monitoring: disabled\n    # Below are properties of the hosting cluster\n    hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # The nested ESXi VMs will be deployed here\n    hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Datastore target for nested ESXi VMs\n    # Settings below are assigned to each host in the cluster\n    vswitch0_vm_port_group_name: vm-network\n    vswitch0_vm_port_group_vlan: \"0\"\n    cpu_cores: \"{{ opinionated.nested_hosts.cpu_cores }}\"  # CPU count\n    ram_in_gb: \"{{ opinionated.nested_hosts.ram_in_gb }}\"  # memory\n    # In order list of disks to assign to the nested host. All will be marked as SSD.\n    # Datastore names will be automatically be pre-pended with the hostname. E.g esx1\n    # If the datastore_prefix property is removed the disk will not be set as a datastore\n    # To leave the default OVA disks in place, delete this section.\n    nested_hosts_disks: \"{{ opinionated.nested_hosts.local_disks | default(omit) }}\"\n    # Added in vmnic order, these port groups must exist on the physical host\n    # Must specify at least 2 port groups, up to a maximum of 10\n    vmnic_physical_portgroup_assignment:\n      - name: \"{{ opinionated.hosting_network.base.port_group }}\"\n      - name: \"{{ opinionated.hosting_network.workload.port_group }}\"\n    resource_pools:  # List of resource pools, remove if not needed\n      - tkc\n\n# Below specifies how many IPs are reserved for other functions\nopinionated_host_ip_ofset: 4\n# You can add nested ESXi hosts below\nnested_hosts: &gt;-\n  [\n    {% for host_number in range(opinionated.number_of_hosts) %}\n    {\n      \"name\": \"esx{{ host_number + 1 }}\",\n      \"ip\": \"{{ opinionated.hosting_network.base.starting_addr | ipmath(opinionated_host_ip_ofset + host_number) }}\",\n      \"mask\": \"{{ opinionated.hosting_network.base.cidr | ansible.netcommon.ipaddr('netmask') }}\",\n      \"gw\": \"{{ opinionated.hosting_network.base.gateway }}\",\n      \"nested_cluster\": \"compute\"\n    },\n    {% endfor %}\n  ]\n\ndistributed_switches:  # To not create any distributed switches, comment this section.\n  - vds_name: vds\n    mtu: 1500\n    vds_version: 7.0.0\n    clusters:  # distributed switch will be attached to all hosts in the clusters defined\n      - compute\n    uplink_quantity: 1\n    vmnics:\n      - vmnic1\n    distributed_port_groups:\n      - port_group_name: workload-pg\n        vlan_id: \"{{ opinionated.hosting_network.workload.vlan_id |default(0) }}\"\n\nvm_templates:\n  - local_path: \"{{ tkgm_os_kubernetes_ova }}\"\n    vcenter_server: \"{{ nested_vcenter.ip }}\"\n    vcenter_username: \"{{ nested_vcenter.username }}\"\n    vcenter_password: \"{{ nested_vcenter.password }}\"\n    vcenter_datacenter: \"{{ nested_vcenter.datacenter }}\"\n    vcenter_datastore: datastore-esx1\n    vcenter_network: workload-pg\n    vsphere_clusters: \"{{ nested_clusters.keys() | list }}\"\n\ntanzu_multi_cloud:\n  generated_config_file_name: cluster-config.yml\n\n  avi_cloud_name: \"{{ nsx_alb.cloud_name | default('Default-Cloud') }}\"\n  avi_controller: \"{{ nsx_alb.controller_ip }}\"\n  avi_data_network: \"{{ nsx_alb.se_vip_port_group }}\"\n  avi_data_network_cidr: \"{{ nsx_alb.se_vip_network_cidr }}\"\n  avi_password: \"{{ nsx_alb.controller_password }}\"\n  avi_service_engine_group: \"{{ nsx_alb.se_group_name | default('Default-Group') }}\"\n  avi_username: \"{{ nsx_alb.controller_username }}\"\n  avi_control_plane_ha_provider: \"{{ opinionated.avi_control_plane_ha_provider }}\"\n\n  # used in TKGM cluster config yaml\n  vsphere_control_plane_endpoint: \"{{ opinionated.hosting_network.workload.cidr | ipmath(2) }}\"\n  vsphere_datacenter_path: \"/{{ nested_vcenter.datacenter }}\"\n  vsphere_datastore_path: \"/{{ nested_vcenter.datacenter }}/datastore/datastore-esx1\"\n  vsphere_folder_path: \"/{{ nested_vcenter.datacenter }}/vm\"\n  vsphere_network: workload-pg\n  vsphere_password: \"{{ nested_vcenter.password }}\"\n  vsphere_resource_pool_path: \"/{{ nested_vcenter.datacenter }}/host/compute/Resources\"\n  vsphere_server: \"{{ nested_vcenter.ip }}\"\n  vsphere_ssh_authorized_key: \"{{ opinionated.ssh_public_key }}\"\n  vsphere_username: \"{{ nested_vcenter.username }}\"\n\nnsx_alb:\n  api_version: \"20.1.6\"\n  controller_username: admin\n  controller_password: \"{{ opinionated.master_password }}\"\n  controller_ssh_public_key: \"{{ opinionated.ssh_public_key }}\"\n  controller_default_password: \"{{ lookup('env', 'AVI_DEFAULT_PASSWORD') }}\"\n\n  controller_ip: \"{{ opinionated.hosting_network.base.starting_addr | ipmath(1) }}\"\n  controller_netmask: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n  controller_gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n  dns_server: \"{{ dns_server }}\"\n  ntp_server: \"{{ ntp_server_ip }}\"\n\n  controller_vcenter_ip: \"{{ hosting_vcenter.ip }}\"\n  controller_vcenter_username: \"{{ hosting_vcenter.username }}\"\n  controller_vcenter_password: \"{{ hosting_vcenter.password }}\"\n  controller_vcenter_datacenter: \"{{ hosting_vcenter.datacenter }}\"\n  controller_vcenter_cluster: \"{{ opinionated.hosting_cluster }}\"\n  controller_vcenter_datastore: \"{{ opinionated.hosting_datastore }}\"\n  controller_port_group: \"{{ opinionated.hosting_network.base.port_group }}\"\n\n  cloud_vcenter_username: \"{{ nested_vcenter.username }}\"\n  cloud_vcenter_password: \"{{ nested_vcenter.password }}\"\n  cloud_vcenter_url: \"{{ nested_vcenter.ip }}\"\n  cloud_vcenter_datacenter: \"{{ nested_vcenter.datacenter }}\"\n\n  se_vcenter_cluster: compute\n  # The management network will host he service engine management interface\n  se_management_port_group: vm-network\n  se_management_network_cidr: \"{{ opinionated.hosting_network.base.cidr }}\"\n  se_management_network_range: &gt;-\n    {{ opinionated.hosting_network.base.starting_addr | ipmath(2) }}-{{ opinionated.hosting_network.base.starting_addr | ipmath(3) }}\n  se_management_network_gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n  # The vip network will contain the virtual servers created by Avi\n  se_vip_port_group: workload-pg\n  se_vip_network_cidr: \"{{ opinionated.hosting_network.workload.cidr }}\"\n  se_vip_network_range: \"{{ opinionated.hosting_network.workload.vip_ip_range }}\"\n  se_vip_network_gateway: \"{{ opinionated.hosting_network.workload.cidr | ipmath(1) }}\"\n</code></pre> <p>In this example you can see we refer to a distributed port group named <code>TKGM</code> with cidr <code>172.20.12.0/22</code>, as mentioned in Nested Lab network example.</p> <p>We can use this config file to deploy a TKGm nested lab environment by running the Ansible playbook as explained here.</p>"},{"location":"homelab/airgapped-tkgs/","title":"Set up an airgapped TKGS homelab environment","text":"<p>We will create a fully airgapped TKGS (aka vSphere with Tanzu) environment. This is a common setup of lots of our customers who run vSphere with Tanzu without any internet connection, not even via a proxy.</p> <p>As described in Nested Lab Setup we will use vmware-lab-builder to bootstrap nested lab environments. This Ansible playbook is not (yet) implemented to work in airgapped environments as it creates a subscribed content library. Hence, we will first run <code>vmware-lab-builder</code> with internet access to bootstrap a TKGS Supervisor Cluster and afterwards, we create a Local Content Library to provision guest clusters without internet access.</p>"},{"location":"homelab/airgapped-tkgs/#initial-setup-with-internet-access","title":"Initial Setup with internet access","text":"<p>As mentioned in the lab network setup we use a virtualized VyOS router to route all traffic in our homelab. For the TKGS with NSX-T environment we configure the following interface in VyOS</p> <pre><code>set interfaces ethernet eth1 vif 16 address 172.20.16.1/22\nset interfaces ethernet eth1 vif 16 description tanzu-without-dhcp\nset interfaces ethernet eth1 vif 16 mtu 9000\nset interfaces ethernet eth1 vif 20 address 172.20.20.1/22\nset interfaces ethernet eth1 vif 20 description nsxt-tep\nset interfaces ethernet eth1 vif 20 mtu 9000\ncommit \nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show interfaces\n ethernet eth0 {\n     address 192.168.178.101/24\n     hw-id 00:0c:29:85:a5:3b\n     offload {\n         gro\n         gso\n         sg\n         tso\n     }\n }\n ethernet eth1 {\n     hw-id 00:0c:29:85:a5:45\n     mtu 9000\n     vif 16 {\n         address 172.20.16.1/22\n         description tanzu-without-dhcp\n         mtu 9000\n     }\n     vif 20 {\n         address 172.20.20.1/22\n         description nsxt-tep\n         mtu 9000\n     }\n }\n loopback lo {\n }\n</code></pre> <p>where <code>vif 16</code> is used for all Virtual Machines and <code>vif 20</code> for the NSX-T TEP network infrastructure.</p> <p>For the Supervisor Management Network, for the Egress and Ingress range, we create a VyOS protocol (static route)</p> <pre><code>set protocols static route 172.30.4.0/24 next-hop 172.20.16.103\ncommit\nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show protocols\n static {\n     route 172.30.4.0/24 {\n         next-hop 172.20.16.103 {\n         }\n     }\n }\n</code></pre> <p>here, <code>172.20.16.103</code> will be the Tier-0 Gateway sitting on <code>vif 16</code>.</p> <p>My <code>vmware-lab-builder</code> config looks like this:</p> <pre><code>---\n# SOFTWARE_DIR must contain all required software\nvc_iso: \"{{ lookup('env', 'SOFTWARE_DIR') }}/VMware-VCSA-all-7.0.3-19717403.iso\"\nesxi_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/Nested_ESXi7.0u3c_Appliance_Template_v1.ova\"\nnsxt_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/nsx-unified-appliance-4.1.2.1.0.22667794.ova\"\n\nenvironment_tag: \"tkgs-nsxt\"  # Used to prepend object names in hosting vCenter\ndns_server: \"192.168.178.1\"\ndns_domain: \"home.local\"\nntp_server_ip: \"192.168.178.1\"  # Must be set to an IP address!\ndisk_mode: thin  # How all disks should be deployed\nnested_host_password: \"{{ opinionated.master_password }}\"\n\nhosting_vcenter:  # This is the vCenter which will be the target for nested vCenters and ESXi hosts\n  ip: \"192.168.178.102\"\n  username: \"{{ lookup('env', 'PARENT_VCENTER_USERNAME') }}\"\n  password: \"{{ lookup('env', 'PARENT_VCENTER_PASSWORD') }}\"\n  datacenter: \"Home\"  # Target for all VM deployment\n\n# This section describes what will be created\nopinionated:\n  master_password: \"VMware1!\"\n  nested_hosts:\n    cpu_cores: 12  # CPU count per nested host\n    ram_in_gb: 128  # memory per nested host\n    local_disks:\n      - size_gb: 500\n        datastore_prefix: \"datastore\"\n  hosting_cluster: Physical\n  hosting_datastore: NVME\n  hosting_network:\n    base:\n      port_group: tanzu-without-dhcp\n      cidr: \"172.20.16.0/22\"\n      gateway: \"172.20.16.1\"\n      # A NSX-T deployment requires 4 IPs, plus 1 per esxi host. They MUST be contiguous.\n      starting_addr: \"172.20.16.100\"\n    # nsxt tep pool will not be routed, but should not clash with routeable ranges\n    nsxt_tep:\n      port_group: nsxt-tep\n      vlan_id: 0\n      cidr: \"172.20.20.0/22\"  # Should be at least a 29 which supports up to 5 hosts and 1 edge\n  tanzu_vsphere:\n    # This network must be minimum of a /25\n    # 1/8 of the network will be used for the supervisor\n    # 3/8 of the network will be used for egress IP range\n    # 1/2 of the network will be used for ingress IP range\n    routeable_super_net: 172.30.4.0/24\n    # This network is private and should not overlap with any routable networks\n    internal_pod_network: 172.32.0.0/22\n    # This network is private and should not overlap with any routable networks\n    internal_kubernetes_services_network: 172.32.4.0/22\n\n#####################################################################\n### No need to edit below this line for an opinionated deployment ###\n#####################################################################\n\nnested_vcenter:  # the vCenter appliance that will be deployed\n  ip: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # vCenter ip address\n  mask: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n  gw: \"{{ opinionated.hosting_network.base.gateway }}\"\n  host_name: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # FQDN if there is working DNS server, otherwise put the ip as a name\n  username: \"administrator@vsphere.local\"\n  password: \"{{ opinionated.master_password }}\"\n  datacenter: \"Lab\"  # DC to create after deployment\n  # Below are properties of parent cluster\n  hosting_network: \"{{ opinionated.hosting_network.base.port_group }}\"  # Parent port group where the vCenter VM will be deployed\n  hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # Parent cluster where the vCenter VM will be deployed\n  hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Parent datastore where the vCenter VM will be deployed\n\nnested_clusters:  # You can add clusters in this section by duplicating the existing cluster\n  compute:  # This will be the name of the cluster in the nested  vCenter. Below are the minimum settings.\n    enable_drs: true\n    enable_ha: true\n    # Below are properties of the hosting cluster\n    hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # The nested ESXi VMs will be deployed here\n    hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Datastore target for nested ESXi VMs\n    # Settings below are assigned to each host in the cluster\n    vswitch0_vm_port_group_name: vm-network\n    vswitch0_vm_port_group_vlan: \"0\"\n    cpu_cores: \"{{ opinionated.nested_hosts.cpu_cores }}\"  # CPU count\n    ram_in_gb: \"{{ opinionated.nested_hosts.ram_in_gb }}\"  # memory\n    # In order list of disks to assign to the nested host. All will be marked as SSD.\n    # Datastore names will be automatically be pre-pended with the hostname. E.g esx1\n    # If the datastore_prefix property is removed the disk will not be set as a datastore\n    # To leave the default OVA disks in place, delete this section.\n    nested_hosts_disks: \"{{ opinionated.nested_hosts.local_disks | default(omit) }}\"\n    # Added in vmnic order, these port groups must exist on the physical host\n    # Must specify at least 2 port groups, up to a maximum of 10\n    vmnic_physical_portgroup_assignment:\n      - name: \"{{ opinionated.hosting_network.base.port_group }}\"\n      - name: \"{{ opinionated.hosting_network.nsxt_tep.port_group }}\"\n\nnested_hosts:\n  - name: \"esx1\"\n    ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(4) }}\"\n    mask: \"{{ opinionated.hosting_network.base.cidr | ansible.utils.ipaddr('netmask') }}\"\n    gw: \"{{ opinionated.hosting_network.base.gateway }}\"\n    nested_cluster: compute\n\ndistributed_switches:  # (optional) - section can be removed to not create any distributed switches\n  - vds_name: nsxt-vds\n    mtu: 1600\n    vds_version: 7.0.0  # Should be 7.0.0, 6.7.0\n    clusters:  # distributed switch will be attached to all hosts in the clusters listed\n      - compute\n    uplink_quantity: 1\n    vmnics:\n      - vmnic1\n\n\ntspbm:  # Tag-based Storage Policy Based Management\n  tag_categories:\n    - category_name: tkgs-storage-category\n      description: \"TKGS tag category\"\n      tags:\n        - tag_name: tkgs-storage-tag\n          description: \"Tag for datastores used by TKGS\"\n  datastore_tags:\n    - datastore_name: \"{{ opinionated.nested_hosts.local_disks[0].datastore_prefix }}-esx1\"\n      tag_names:\n        - tkgs-storage-tag\n  vm_storage_policies:\n    - storage_policy_name: tkgs-storage-policy\n      description: \"TKGS storage performance policy\"\n      tag_name: tkgs-storage-tag\n      tag_category: tkgs-storage-category\n\ntanzu_vsphere:\n  services_cidr: \"{{ opinionated.tanzu_vsphere.internal_kubernetes_services_network }}\"  # This is private within each cluster\n  content_library_datastore: \"{{ opinionated.nested_hosts.local_disks[0].datastore_prefix }}-esx1\"\n  content_library_name: tkgs-library\n  content_library_url: \"http://wp-content.vmware.com/v2/latest/lib.json\"\n  default_content_library: tkgs-library\n  dns_server_list: [\"{{ dns_server }}\"]\n  ephemeral_storage_policy: \"{{ tspbm.vm_storage_policies[0].storage_policy_name }}\"\n  fluentbit_enabled: false\n  image_storage_policy: \"{{ tspbm.vm_storage_policies[0].storage_policy_name }}\"\n  ntp_server_list: [\"{{ ntp_server_ip }}\"]\n  management_dns_servers: [\"{{ dns_server }}\"]\n  management_port_group: supervisor-seg\n  management_gateway: \"{{ opinionated.tanzu_vsphere.routeable_super_net | ansible.utils.ipmath(1) }}\"\n  management_netmask: &gt;-\n    {{ opinionated.tanzu_vsphere.routeable_super_net |\n    ansible.utils.ipsubnet((opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int)+3, 0) |\n    ansible.utils.ipaddr('netmask') }}\n  management_starting_address: \"{{ opinionated.tanzu_vsphere.routeable_super_net | ansible.utils.ipmath(2) }}\"\n  master_storage_policy: \"{{ tspbm.vm_storage_policies[0].storage_policy_name }}\"\n  network_provider: NSXT_CONTAINER_PLUGIN\n  supervisor_size: tiny\n  vsphere_cluster: compute\n  workload_dns_servers: [\"{{ dns_server }}\"]\n\n  nsxt:\n    cluster_distributed_switch: \"{{ distributed_switches[0].vds_name }}\"\n    egress_cidrs:\n      - &gt;-\n        {{ opinionated.tanzu_vsphere.routeable_super_net |\n        ansible.utils.ipsubnet((opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int)+3, 1) }}\n      - &gt;-\n        {{ opinionated.tanzu_vsphere.routeable_super_net |\n        ansible.utils.ipsubnet((opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int)+2, 1) }}\n    ingress_cidrs:\n      - &gt;-\n        {{ opinionated.tanzu_vsphere.routeable_super_net |\n        ansible.utils.ipsubnet((opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int)+1, 1) }}\n    nsx_edge_cluster: \"{{ nsxt.edge_clusters[0].edge_cluster_name}}\"\n    pod_cidrs: \"{{ opinionated.tanzu_vsphere.internal_pod_network }}\"\n    # This is used by the task which checks if the supervisor network is online\n    t0_uplink_ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"\n\nnsxt:  # (optional) - section can be removed to not create any nsxt objects\n  manager:\n    hostname: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(1) }}\"\n    ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(1) }}\"\n    netmask: \"{{ opinionated.hosting_network.base.cidr | ansible.utils.ipaddr('netmask') }}\"\n    gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n    username: admin  # this cannot be changed\n    password: \"{{ opinionated.master_password }}{{ opinionated.master_password }}\"\n    hosting_vcenter_ip: \"{{ hosting_vcenter.ip }}\"\n    hosting_vcenter_username: \"{{ hosting_vcenter.username }}\"\n    hosting_vcenter_password: \"{{ hosting_vcenter.password }}\"\n    hosting_datacenter: \"{{ hosting_vcenter.datacenter }}\"\n    hosting_datastore: \"{{ opinionated.hosting_datastore }}\"\n    hosting_network: \"{{ opinionated.hosting_network.base.port_group }}\"\n    hosting_cluster: \"{{ opinionated.hosting_cluster }}\"\n    license_key: \"{{ lookup('env', 'NSXT_LICENSE_KEY') }}\"\n\n  # If the section below is defined, the playbook will wait for the IP to become pingable\n  # For TKG service deployments this is the default gateway of the supervisor network\n  routing_test:\n    ip_to_ping: \"{{ opinionated.tanzu_vsphere.routeable_super_net | ansible.utils.ipmath(1) }}\"\n    # The playbook will present a message using the params below\n    # A static route must be made to the router uplink for nsxt_supernet\n    router_uplink: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"\n    nsxt_supernet: \"{{ opinionated.tanzu_vsphere.routeable_super_net }}\"\n\n  policy_ip_pools:\n    - display_name: tep-pool  # This is a non-routable range which is used for the overlay tunnels.\n      pool_static_subnets:\n        - id: tep-pool-1\n          state: present\n          allocation_ranges:\n            - start: \"{{ opinionated.hosting_network.nsxt_tep.cidr | ansible.utils.ipmath(1) }}\"\n              end: \"{{ opinionated.hosting_network.nsxt_tep.cidr | ansible.utils.ipaddr('-2') | ansible.utils.ipaddr('address') }}\"\n          cidr: \"{{ opinionated.hosting_network.nsxt_tep.cidr }}\"\n          do_wait_till_create: true\n\n  uplink_profiles:\n    - display_name: host-tep-profile\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: \"{{ opinionated.hosting_network.nsxt_tep.vlan_id }}\"\n    - display_name: edge-tep-profile\n      mtu: 9000\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: \"{{ opinionated.hosting_network.nsxt_tep.vlan_id }}\"\n    - display_name: edge-uplink-profile\n      mtu: 1500\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: 0\n\n  transport_zones:\n    - display_name: tz-overlay\n      transport_type: OVERLAY\n      # host_switch_name: \"{{ distributed_switches[0].vds_name }}\"\n      nested_nsx: true  # Set this to true if you use NSX-T for your physical host networking\n      description: \"Overlay Transport Zone\"\n      # - display_name: nsx-vlan-transportzone\n      #   transport_type: VLAN\n      #   # host_switch_name: sw_vlan\n      #   description: \"Uplink Transport Zone\"\n\n  transport_node_profiles:\n    - display_name: tnp1\n      host_switches:\n        - host_switch_profiles:\n            - name: host-tep-profile\n              type: UplinkHostSwitchProfile\n          host_switch_name: \"{{ distributed_switches[0].vds_name }}\"\n          host_switch_type: VDS\n          host_switch_mode: STANDARD\n          ip_assignment_spec:\n            resource_type: StaticIpPoolSpec\n            ip_pool_name: \"tep-pool\"\n          transport_zone_endpoints:\n            - transport_zone_name: \"tz-overlay\"\n            - transport_zone_name: \"nsx-vlan-transportzone\"\n          uplinks:\n            - uplink_name: \"uplink-1\"\n              vds_uplink_name: \"Uplink 1\"\n      description: \"Cluster node profile\"\n\n  cluster_attach:\n    - display_name: \"tnc1\"\n      description: \"Transport Node Collections 1\"\n      compute_manager_name: \"vCenter\"\n      cluster_name: \"compute\"\n      transport_node_profile_name: \"tnp1\"\n\n  edge_nodes:\n    - display_name: edge-node-1\n      size: MEDIUM\n      mgmt_ip_address: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(2) }}\"\n      mgmt_prefix_length: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n      mgmt_default_gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n      network_management_name: vm-network\n      network_uplink_name: vm-network\n      network_tep_name: edge-tep-seg\n      datastore_name: datastore-esx1\n      cluster_name: compute\n      host_switches:\n        tep:\n          uplink_profile_name: edge-tep-profile\n          ip_assignment_spec:\n            resource_type: StaticIpPoolSpec\n            ip_pool_name: tep-pool\n          transport_zone_endpoints:\n            - transport_zone_name: \"tz-overlay\"\n        uplink:\n          host_switch_name: \"sw_vlan\"\n          uplink_profile_name: edge-uplink-profile\n          transport_zone_endpoints:\n            - transport_zone_name: \"nsx-vlan-transportzone\"\n      transport_zone_endpoints:\n        - transport_zone_name: \"tz-overlay-2\"\n        - transport_zone_name: \"nsx-vlan-transportzone\"\n\n  edge_clusters:\n    - edge_cluster_name: edge-cluster-1\n      edge_cluster_members:\n        - transport_node_name: edge-node-1\n\n  vlan_segments:\n    - display_name: t0-uplink\n      vlan_ids: [0]\n      transport_zone_display_name: nsx-vlan-transportzone\n    - display_name: edge-tep-seg\n      vlan_ids: [0]\n      transport_zone_display_name: nsx-vlan-transportzone\n\n  # For full spec see - https://github.com/laidbackware/ansible-for-nsxt/blob/vmware-lab-builder/library/nsxt_policy_tier0.py\n  tier_0:\n    display_name: \"tkgs-t0\"\n    ha_mode: ACTIVE_STANDBY\n    uplink_ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"\n    disable_firewall: true\n    static_routes:\n      - state: present\n        display_name: default-route\n        network: \"0.0.0.0/0\"\n        next_hops:\n          - ip_address: \"{{ opinionated.hosting_network.base.gateway }}\"\n    locale_services:\n      - state: present\n        display_name: \"tkgs-t0-ls\"\n        edge_cluster_info:\n          edge_cluster_display_name: edge-cluster-1\n        interfaces:\n          - display_name: \"test-t0-t0ls-iface\"\n            state: present\n            subnets:\n              - ip_addresses: [\"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"]\n                prefix_len: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] | int }}\"\n            segment_id: t0-uplink\n            edge_node_info:\n              edge_cluster_display_name: edge-cluster-1\n              edge_node_display_name: edge-node-1\n            mtu: 1500\n\n  overlay_segments:\n    - display_name: supervisor-seg\n      transport_zone_display_name: tz-overlay\n      tier1_display_name: supervisor-t1\n      subnets:\n        - gateway_address: &gt;-\n            {{ opinionated.tanzu_vsphere.routeable_super_net |\n            ansible.utils.ipmath(1) }}/{{ opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int +3 }}\n\n  tier_1_gateways:\n    - display_name: supervisor-t1\n      route_advertisement_types:\n        - \"TIER1_CONNECTED\"\n      tier0_display_name: tkgs-t0\n</code></pre>"},{"location":"homelab/airgapped-tkgs/#restrict-internet-access-in-vyos-interfaces","title":"Restrict internet access in VyOS interfaces","text":"<p>We will create an environment that doesn't have outbound internet connection but ingress connection from the internet is allowed and also all traffic within the network is allowed. We don't restrict internal communications on a per-port level.</p> <ol> <li> <p>Create a <code>Firewall address-group</code> called <code>ALLOWED-IPS</code> that will collect all IP ranges to and from which communication is allowed using the command</p> <pre><code>set firewall group address-group ALLOWED-IPS address 172.20.16.1-172.20.16.255\n</code></pre> <p>Do this for every IP range you are using (don't forget the internal Kubernetes pod and service range). We get the following result:</p> <pre><code>vyos@vyos# show firewall group\naddress-group ALLOWED-IPS {\n    address 192.168.178.1-192.168.178.255\n    address 172.20.16.1-172.20.16.255\n    address 172.20.17.1-172.20.17.255\n    address 172.20.18.1-172.20.18.255\n    address 172.20.19.1-172.20.19.255\n    address 172.20.20.1-172.20.20.255\n    address 172.20.21.1-172.20.21.255\n    address 172.20.22.1-172.20.22.255\n    address 172.20.23.1-172.20.23.255\n    address 172.30.4.1-172.30.4.255\n    address 172.32.0.1-172.32.0.255\n    address 172.32.1.1-172.32.1.255\n    address 172.32.2.1-172.32.2.255\n    address 172.32.3.1-172.32.3.255\n    address 172.32.4.1-172.32.4.255\n    address 172.32.5.1-172.32.5.255\n    address 172.32.6.1-172.32.6.255\n    address 172.32.7.1-172.32.7.255\n}\n</code></pre> </li> <li> <p>Create a firewall rule to allow inbound traffic from everywhere</p> <pre><code>set firewall name INBOUND-ALL default-action drop\nset firewall name INBOUND-ALL description \"Allow all incoming connections\"\nset firewall name INBOUND-ALL enable-default-log\nset firewall name INBOUND-ALL rule 20 action accept\nset firewall name INBOUND-ALL rule 20 log enable\nset firewall name INBOUND-ALL rule 20 source address 0.0.0.0/0\ncommit \nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show firewall name INBOUND-ALL\ndefault-action drop\ndescription \"Allow all incoming connections\"\nenable-default-log\nrule 20 {\n    action accept\n    log enable\n    source {\n        address 0.0.0.0/0\n    }\n}\n</code></pre> </li> <li> <p>Create a firewall rule for outbound communication</p> <pre><code>set firewall name OUTBOUND-RESTRICT default-action 'drop'\nset firewall name OUTBOUND-RESTRICT description 'Restrict outbound connections'\nset firewall name OUTBOUND-RESTRICT enable-default-log\nset firewall name OUTBOUND-RESTRICT rule 10 action 'accept'\nset firewall name OUTBOUND-RESTRICT rule 10 description 'Allow DNS'\nset firewall name OUTBOUND-RESTRICT rule 10 destination port '53'\nset firewall name OUTBOUND-RESTRICT rule 10 protocol 'tcp_udp'\nset firewall name OUTBOUND-RESTRICT rule 20 action 'accept'\nset firewall name OUTBOUND-RESTRICT rule 20 description 'Allow SSH'\nset firewall name OUTBOUND-RESTRICT rule 20 destination port '22'\nset firewall name OUTBOUND-RESTRICT rule 20 log 'enable'\nset firewall name OUTBOUND-RESTRICT rule 20 protocol 'tcp'\nset firewall name OUTBOUND-RESTRICT rule 30 action 'accept'\nset firewall name OUTBOUND-RESTRICT rule 30 description 'Allow specific outbound traffic'\nset firewall name OUTBOUND-RESTRICT rule 30 log 'enable'\nset firewall name OUTBOUND-RESTRICT rule 30 protocol 'all'\nset firewall name OUTBOUND-RESTRICT rule 30 source group address-group 'ALLOWED-IPS'\ncommit \nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show firewall name OUTBOUND-RESTRICT\ndefault-action drop\ndescription \"Restrict outbound connections\"\nenable-default-log\nrule 10 {\n    action accept\n    description \"Allow DNS\"\n    destination {\n        port 53\n    }\n    protocol tcp_udp\n}\nrule 20 {\n    action accept\n    description \"Allow SSH\"\n    destination {\n        port 22\n    }\n    log enable\n    protocol tcp\n}\nrule 30 {\n    action accept\n    description \"Allow specific outbound traffic\"\n    log enable\n    protocol all\n    source {\n        group {\n            address-group ALLOWED-IPS\n        }\n    }\n}\n</code></pre> </li> <li> <p>Finally, apply the firewall rules on the respective interfaces:</p> <pre><code>set interfaces ethernet eth1 vif 20 firewall in name INBOUND-ALL\nset interfaces ethernet eth1 vif 20 firewall out name OUTBOUND-RESTRICT\nset interfaces ethernet eth1 vif 16 firewall in name INBOUND-ALL\nset interfaces ethernet eth1 vif 16 firewall out name OUTBOUND-RESTRICT\ncommit\nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show interfaces ethernet eth1\nhw-id 00:0c:29:85:a5:45\nmtu 9000\nvif 16 {\n    address 172.20.16.1/22\n    description tanzu-without-dhcp\n    firewall {\n        in {\n            name INBOUND-ALL\n        }\n        out {\n            name OUTBOUND-RESTRICT\n        }\n    }\n    mtu 9000\n}\nvif 20 {\n    address 172.20.20.1/22\n    description nsxt-tep\n    firewall {\n        in {\n            name INBOUND-ALL\n        }\n        out {\n            name OUTBOUND-RESTRICT\n        }\n    }\n    mtu 9000\n}\n</code></pre> </li> </ol>"},{"location":"kubernetes/architecture-concepts/","title":"Kubernetes Architecture Concepts","text":"<p>This page explains the basic concepts of the Kubernetes technical architecture which I find very important to better understand Kubernetes as a whole.</p>"},{"location":"kubernetes/architecture-concepts/#goal-of-this-page","title":"Goal of this page","text":"<p>Kubernetes has evolved from just being a container scheduling and management system. It can be used as a generic \"platform API\" - a standardized API for an entire platform consisting of not only containers, but also virtual machines, databases and services. The reason for this success is due to the well architected architecture in my opinion.</p> <p>This is the reason why I think it is very valuable to understand the basic technical concepts as it will help you better understand literally anything in Kubernetes.</p> <p>I try to go into technical details without going into technical details </p> <p>We will cover:</p> <ul> <li>what actually happens when I create a Kubernetes deployment?</li> <li>Kubernetes Reconciliation</li> <li>Kubernetes Admission Webhooks - Mutating and Validating</li> <li>Custom Resource Definitions (CRDs)</li> </ul>"},{"location":"kubernetes/architecture-concepts/#what-happens-when-i-create-a-kubernetes-deployment","title":"What happens when I create a Kubernetes deployment?","text":"<p>In order to deploy a simple <code>nginx</code> deployment with 3 replicas, we create a file <code>nginx-deployment.yaml</code>:</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre> <p>and apply it on a Kubernetes cluster with <code>kubectl apply -f nginx-deployment.yaml</code>, which will ultimately run 3 pods of nginx. Actually, a deployment does not create 3 pods but the deployment creates a replicaset, and the replicaset will run the 3 pods.</p> <p>But how does that work?</p> <p>Let's look into the simplified Kubernetes API request:</p> <p></p> <p>When executing <code>kubectl apply -f nginx-deployment.yaml</code>, multiple things happen which we will divide into 3 steps:</p>"},{"location":"kubernetes/architecture-concepts/#store-deployment-manifest-in-etcd","title":"Store Deployment Manifest in etcd","text":"<ol> <li>we hit the Kubernetes API Server, often running as a pod <code>kube-apiserver</code> itself on the Kubernetes cluster</li> <li>the <code>API HTTP Handler</code> takes the incoming request and forwards it to <code>Authentication</code> &amp; <code>Authorization</code> - Kubernetes Role Based Access Control (RBAC). If Kubernetes RBAC denies the request, the API server responds with a <code>permission denied</code> error and stops the request from being continued to the subsequent steps.</li> <li> <p>If RBAC approves, the request will be handled by further steps as explained below and end up in <code>Object Schema Validation</code>. This step validates if the request is a valid yaml/json and also validates if all fields are correct. For instance, if you have a typo in your Deployment's spec, e.g. in <code>replicas</code>:</p> <pre><code>[...]\nspec:\n  replica: 3\n  selector:\n    matchLabels:\n      app: nginx\n[...]\n</code></pre> <p>where we missed the \"s\" in <code>replicas</code>, this step will respond with </p> <pre><code>Error from server (BadRequest): error when creating \"nginx-deployment.yaml\": Deployment in version \"v1\" cannot be handled as a Deployment: strict decoding error: unknown field \"spec.replica\"\n</code></pre> </li> <li> <p>If we have a valid yaml and the syntax is correct, our <code>nginx-deployment.yaml</code> will be persisted in etcd, the distributed key-value store in Kubernetes.</p> </li> </ol> <p>Note</p> <p>The Kubernetes deployment manifest is now stored in etcd. There is no running pod yet though! At this point, <code>kubectl apply</code> completed its job and returns with <code>deployment.apps/nginx created</code>, essentially a <code>200 OK</code>. All subsequent steps necessary to ultimately run a pod are handled asynchronously.</p>"},{"location":"kubernetes/architecture-concepts/#etcd-watch-api-and-controllers","title":"etcd Watch API and Controllers","text":"<p>As soon as the deployment manifest is stored in etcd, a key feature of etcd kicks in, the etcd Watch API which provides an event-based interface for asynchronously monitoring changes to keys. An etcd watch waits for changes to keys by continuously watching from a given revision, either current or historical, and streams key updates back to the client. This API is heavily used by Kubernetes.</p> <p>Let's look at our example:</p> <p></p> <p>There is a <code>Deployment controller</code> watching for changes of <code>kind: Deployment</code> in etcd and then determining whether it's a <code>Create</code>, <code>Update</code> or <code>Delete</code> event. In our example, it's a <code>Create</code> event and thus, the Deployment Controller will create a Replicaset which will look similar to this</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  [...]\n  labels:\n    app: nginx\n  name: nginx-bf5d5cf98\n  namespace: default\n  [...]\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        [...]\n</code></pre> <p>The Deployment Controller applies this manifest to the API Server the same way as the user applied the deployment manifest. Hence, it will first hit the <code>API HTTP Handler</code>, <code>Authentication / Authorization (RBAC)</code>, <code>Object Schema Validation</code> and after some other steps be persisted in etcd.</p> <p>This update in etcd triggers another controller, the <code>Replicaset Controller</code> and the entire process starts over again:</p> <p></p> <p>The Replicaset Controller ultimately persists the pod manifest in etcd.</p>"},{"location":"kubernetes/architecture-concepts/#start-the-containers","title":"Start the containers","text":"<p>Once a pod manifest is stored in etcd, the same mechanisms apply:</p> <ol> <li>the kube-scheduler watches etcd for pod events and based on the <code>Create</code> event, it assigns the pod to a suitable node in the cluster based on resource availability and other constraints. It also updates the pod's status in etcd to reflect its node assignment.</li> <li>The kubelet on the assigned node also watches etcd for pod events and if there is a node assignment in the pod's spec, it will pull the necessary container images, start the containers and set up networking.</li> <li>The container runtime which is installed on the cluster will ultimately run the containers.</li> </ol>"},{"location":"kubernetes/architecture-concepts/#reconciliation","title":"Reconciliation","text":"<p>We've seen the <code>Deployment Controller</code> and <code>Replicaset Controller</code> how they watch etcd and react on specific changes in etcd. We've also seen that other processes, like <code>kube-scheduler</code> and <code>kubelet</code> work in a similar way in that they watch etcd for changes. All these so-called controllers do not only create/update/delete other resources but also report back the current status.</p> <p>(Almost) Every resource in Kubernetes has the following similar structure:</p> <pre><code>---\napiVersion: [APIGroup]/[APIGroupVersion]\nkind: [KIND]\nmetadata:\n  [...]\nspec:\n  [...]\nstatus:\n  [...]\n</code></pre> <p>where</p> <ul> <li><code>spec</code> is what get's created/updated from one controller</li> <li><code>status</code> is where another controller reports back the current status</li> </ul> <p>We call the <code>spec</code> the desired state, and the <code>status</code> the actual state.</p> <p>Let's look at our example:</p> <ol> <li>when the end user (e.g. a developer) creates a deployment manifest and applies it to the API Server to store it in etcd, then the user applies the desired state as described in the <code>spec</code></li> <li>when a deployment controller gets triggered on this event, it</li> <li>creates a replicaset manifest by applying the desired state in the spec and applies it to the API Server to store it in etcd</li> <li>updates the deployment manifest, which has been initially created by the end user, by updating the <code>status</code> block with information gathered from the created replicaset</li> <li>when the replicaset controller gets triggered on the replicaset event in etcd, it</li> <li>creates one or more pod manifests by applying the desired state in the spec and applies it to the API Server to store it in etcd</li> <li>updates the replicaset manifest, which has been initially created by the Deployment Controller, by updating the <code>status</code> block with information gathered from the pods</li> <li>this process goes on in the same way for all other controllers</li> </ol> <p>The Deployment Controller and Replicaset Controller are built-in upstream Kubernetes controllers which are, amongst several other controllers, bundled in kube-controller-manager.</p> <p>The previously described process can be summarized in this picture:</p> <p></p> <p>This process of constantly watching the desired state and syncing with the actual state is called Reconciliation - controllers reconcile the desired state with the actual state.</p> <p>It is important to know who is the owner of a resource in order to determine the desired state and what should be reconciled: In our example, the replicaset manifest stored in etcd is the desired state for the Replicaset Controller. But updating the replicaset manifest, e.g. with <code>kubectl edit replicaset</code>, does not update the pods - although this is the replicaset controller's job. The replicaset manifest is owned by the Deployment Controller, whose desired state is stored in the deployment manifest. Hence, the manual changes in the replicaset manifest will be reverted back to what's desired in the deployment manifest. Resource owners are referenced in the corresponding resource:</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  [...]\n  name: nginx-bf5d5cf98\n  namespace: default\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: Deployment\n    name: nginx\n    uid: 7063121f-1e39-4b03-96c1-d14edf24713d\n  [...]\n</code></pre>"},{"location":"kubernetes/architecture-concepts/#admission-controllers","title":"Admission Controllers","text":"<p>We've looked into a simplified Kubernetes API request flow when we explored what happens when we create a Kubernetes deployment where we haven't covered two steps - <code>Mutating Admission</code> and <code>Validating Admission</code>:</p> <p></p> <p>From the Kubernetes docs:</p> <p>An admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.</p> <p>Admission controllers may be validating, mutating, or both. Mutating controllers may modify objects related to the requests they admit; validating controllers may not.</p> <p>Admission controllers limit requests to create, delete, modify objects. Admission controllers can also block custom verbs, such as a request connect to a Pod via an API server proxy. Admission controllers do not (and cannot) block requests to read (get, watch or list) objects.</p> <p>Simply put, the mutating admission step will alter your manifest and the validating admission will allow or deny your request.</p>"},{"location":"kubernetes/architecture-concepts/#admission-controller-example","title":"Admission Controller Example","text":"<p>Let's look into our example and let's assume there is a AddLabel mutating admission controller implemented that injects a label <code>team: &lt;TeamName&gt;</code> to every request's manifest and there is a RequiredLabel validating admission controller implemented that expects a <code>cost-center: &lt;CostCenterID&gt;</code> label on every manifest.</p> <p>When we create a new deployment using</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre> <p>then the mutating admission controller <code>AddLabel</code> will inject the label <code>team: AwesomeTeam</code>, so the request becomes</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n    team: AwesomeTeam\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre> <p>The request then passes the <code>Object Schema Validation</code> step as there is no syntax error, but the request is denied on the <code>Validating Admission</code> step because the label <code>cost-center</code> is missing.</p> <p>When using</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n    cost-center: 12345\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre> <p>the request passes the API workflow and gets stored in etcd as</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n    cost-center: 12345\n    team: AwesomeTeam\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre>"},{"location":"kubernetes/architecture-concepts/#built-in-admission-controllers","title":"Built-In Admission Controllers","text":"<p>There are various admission controllers compiled into the <code>kube-apiserver</code> binary which Kubernetes Administrators can turn on and off with some default ones being turned on.</p>"},{"location":"kubernetes/architecture-concepts/#dynamic-admission-controllers","title":"Dynamic Admission Controllers","text":"<p>Kubernetes docs</p> <p>In addition to compiled-in admission plugins, admission plugins can be developed as extensions and run as webhooks configured at runtime. [...] Admission webhooks are HTTP callbacks that receive admission requests and do something with them.</p> <p>Simply put, the Kubernetes API exposes the Mutating Admission and Validating Admission interfaces so that you can write external custom software and extend those two api workflow steps.</p> <p>The example from above explains two possible custom implementations of a mutating and validating admission webhook.</p> <p>The most famous open source projects that implement both webhooks are Kyverno and Open Policy Agent Gatekeeper.</p> <p>You can read more about admission controllers on this blog post: A Guide to Kubernetes Admission Controllers.</p>"},{"location":"kubernetes/architecture-concepts/#extending-kubernetes","title":"Extending Kubernetes","text":""},{"location":"kubernetes/architecture-concepts/#custom-resources-and-custom-controllers","title":"Custom Resources and Custom Controllers","text":"<p>We have seen that Dynamic Admission Controllers allows to hook into the Kubernetes API and extend it with custom software.</p> <p>With the introduction of Custom Resources you can further extend Kubernetes by writing custom controllers and hook into the etcd Watch API the same way as it's done with the Deployment Controller or Replicaset Controller as explained above.</p> <p>A simple custom controller is kubewatch which basically looks for events like pod/deployment/confimap creation/update/deletion and send a notification to selected channels like slack, hipchat, mattermost or webhook:</p> <p></p>"},{"location":"kubernetes/architecture-concepts/#custom-resource-definitions-crds-and-operators","title":"Custom Resource Definitions (CRDs) and Operators","text":"<p>Custom Controllers can be implemented to use the etcd Watch API and watch for built-in Kubernetes resources, such as deployments, services or pods, as described in the previous section. This approach can be further extended by implementing own resources, and not only relying on built-in resources.</p> <p>Let's look at a simple example: In order to deploy a web application we probably need a <code>deployment</code> to deploy the application in pods, a <code>service</code> to make the application available to end users, a <code>configmap</code> to store application configuration and a <code>secret</code> to store application secrets. Let's assume, we work for <code>mycompany</code> and we have implemented an app called <code>shopping-cart</code> which uses an external Postgres database and uses S3 for storing files. For this, we could introduce a custom resource called <code>WebApp</code> which would look like:</p> <pre><code>---\napiVersion: apps.com.mycompany/v1\nkind: WebApp\nmetadata:\n  labels:\n    app: shopping-cart\n  name: shopping-cart\nspec:\n  replicas: 3\n  image: registry.mycompany.com/shopping-cart/shopping-cart:v1.0.0\n  config:\n    env: prod\n    postgresURL: postgres.mycompany.com:5432\n    s3URL: \n    s3Bucket: shopping-cart\n  secret:\n    postgresUser: pg\n    postGresPasswort: sup\u20acrs3cure!\n    s3AccessKeyID: JWQWDBWM2\n    s3SecretAccessKey: nTqfIa4AvynIEWG7cTmY\n</code></pre> <p>Tip</p> <p>Never store secrets in plaintext! This is just an example, so please forgive me \ud83d\ude09</p> <p>We can then apply this <code>WebApp</code> onto our cluster</p> <pre><code>kubectl apply -f shopping-cart-webapp.yaml\n</code></pre> <p>which will ultimately create a <code>deployment</code>, <code>service</code>, <code>configmap</code> and a <code>secret</code>.</p> <p>Note</p> <p>This is just a simple example. This can be further extended to abstract away required logic from developers. So we can think of the <code>WebApp</code> being a custom resource owned by platform admins who can implement all required details to standardize web application deployments  within the company. This could include implementing security requirements and other best practices whilst developers can focus on their application code.</p> <p>How can this be implemented?</p> <p>The <code>WebApp</code> is a Custom Resource Definition (CRD) - a custom API registered in the Kubernetes API. To make that work technically, you have to describe and register the <code>WebApp</code> API by creating a CRD:</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: webapps.apps.com.mycompany\nspec:\n  group: apps.com.mycompany\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                replicas:\n                  type: string\n                image:\n                  type: string\n                config:\n                  type: object\n                  properties:\n                    env:\n                      type: string\n                    postgresURL:\n                      type: string\n                    s3URL:\n                      type: string\n                    s3Bucket:\n                      type: string\n                secret:\n                  type: object\n                  properties:\n                    postgresURL:\n                      type: string\n                    postGresPasswort:\n                      type: string\n                    s3AccessKeyID:\n                      type: string\n                    s3SecretAccessKey:\n                      type: string\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;\n    plural: webapps\n    # singular name to be used as an alias on the CLI and for display\n    singular: webapp\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: WebApp\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - wa\n</code></pre> <p>which we can simply register in Kubernetes with</p> <pre><code>kubectl apply -f webapp-crd.yaml\n</code></pre> <p>Afterwards we can already execute</p> <pre><code>kubectl get webapp\n# or using the short name\nkubectl get wa\n# and because it's namespaced\nkubectl get wa --all-namespaces\n</code></pre> <p>we can actually also already apply our manifest from above. But this will solely store the manifest in etcd (after it passes all API workflow steps as described in Store Deployment Manifest in etcd). Until now, there is no controller that watches etcd for resources of <code>kind: WebApp</code>. Therefore, the next step is to implement such a custom controller - custom controllers that watch Custom Resource Definitions are called Operators.</p>"},{"location":"kubernetes/architecture-concepts/#summary","title":"Summary","text":"<p>The Kubernetes project started in 6th June 2014 to become a Production-Grade Container Scheduling and Management system and has since evolved to be way more than that - with the possibility of extending the Kubernetes API with admission controllers and Kubernetes itself with Custom Controller and Operator, Kubernetes can be used as a standardized Platform API. All these implementation patterns build on top of the paradigm of an asynchronous event based architecture with etcd and the controller pattern at the heart of it. This is the main reason for success of the Kubernetes project in my opinion.</p>"},{"location":"kubernetes/architecture-concepts/#further-interesting-resources","title":"Further interesting resources","text":"<ul> <li>What happens when ... Kubernetes edition!</li> <li>OperatorHub.io</li> <li>How does the Kubernetes scheduler work?</li> </ul>"},{"location":"tanzu/tanzu-packages/","title":"Tanzu Packages","text":"<p>See official docs here.</p>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/","title":"Deploy TKG packages in airgapped environments","text":""},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#prerequisites","title":"Prerequisites","text":"<ul> <li>TKGS Supervisor cluster running</li> <li>embedded Harbor running</li> <li>shared services cluster running</li> </ul>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#environment-info","title":"Environment info","text":"<ul> <li>vCenter 7u3p (Build 22837322)</li> <li>Supervisor cluster version v1.25.6+vmware.wcp.2</li> <li>Guest Cluster version: v1.23.8---vmware.3-tkg.1</li> </ul>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-packages","title":"Install Packages","text":"<p>We are installing the following packages:</p> Package Version cert-manager 1.7.2+vmware.1-tkg.1 contour 1.20.2+vmware.2-tkg.1 harbor 2.3.3+vmware.1-tkg.1 <p>We deploy those packages on a Kubernetes cluster with version 1.23.8. This cluster has PodSecurityPolicies enabled.  Because we don't care, we allow all pods with</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: psp:privileged\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - vmware-system-privileged\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: all:psp:privileged\nroleRef:\n  kind: ClusterRole\n  name: psp:privileged\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: Group\n  name: system:serviceaccounts\n  apiGroup: rbac.authorization.k8s.io\nEOF\n</code></pre>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#deploy-tkg-package-repository","title":"Deploy TKG Package Repository","text":""},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-kapp-controller","title":"Install kapp-controller","text":"<p>We are following the official docs here.</p> <p>Run the following from a machine with access to the VMware public registry:</p> <ol> <li> <p>list available versions:</p> <pre><code>imgpkg tag list -i projects.registry.vmware.com/tkg/kapp-controller\n</code></pre> </li> <li> <p>Copy the version of choice to your local registry</p> <pre><code>imgpkg copy \\\n  -i projects.registry.vmware.com/tkg/kapp-controller:v0.30.0_vmware.1 \\\n  --to-repo 172.30.4.131/shared-services/kapp-controller \\\n  --registry-ca-cert-path ./ca.crt\n</code></pre> <p>Alternatively, you can download the tar file to your filesystem</p> <pre><code>imgpkg copy \\\n  -i projects.registry.vmware.com/tkg/kapp-controller:v0.41.7_vmware.1 \\\n  --to-tar  ./kapp-controller_v0.41.7_vmware.1\n</code></pre> </li> <li> <p>Create the <code>tanzu-package-repo-global</code> namespace:</p> <pre><code>kubectl create ns tanzu-package-repo-global\n</code></pre> </li> <li> <p>create a secret to be able to pull images from the local registry with authentication</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n tanzu-package-repo-global\n</code></pre> </li> <li> <p>Copy the content of the kapp-controller manifest from here and make some changes:</p> </li> <li>update the <code>image:</code> accordingly to point to your image stored in your local container registry</li> <li> <p>add the following to <code>Deployment.spec.template.spec</code></p> <pre><code>```shell\nimagePullSecrets:\n- name: embedded-harbor\n```\n</code></pre> </li> <li> <p>Switch kubectl context to your shared services cluster and apply the manifest</p> <pre><code>kubectl apply -f kapp-controller.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#add-package-repository-to-cluster","title":"Add Package Repository to Cluster","text":"<p>We are following the official docs here.</p> <p>Run the following from a machine with access to the VMware public registry:</p> <ol> <li> <p>list available Package Repository versions:</p> <pre><code>imgpkg tag list -i projects.registry.vmware.com/tkg/packages/standard/repo\n</code></pre> </li> <li> <p>Copy your version of choice to your registry</p> <pre><code>imgpkg copy \\\n  -b projects.registry.vmware.com/tkg/packages/standard/repo:v1.6.1 \\\n    --to-repo 172.30.4.131/shared-services/packages/standard/repo \\\n    --registry-ca-cert-path ./ca.crt\n</code></pre> </li> <li> <p>Create a <code>PackageRepository</code> manifest and call it <code>packagerepo-v1.6.1.yaml</code>:</p> <pre><code>apiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\n  name: tanzu-standard\n  namespace: tanzu-package-repo-global\nspec:\n  fetch:\n    imgpkgBundle:\n      image: 172.30.4.131/shared-services/packages/standard/repo:v1.6.1\n      secretRef:\n        name: embedded-harbor\n</code></pre> </li> <li> <p>Switch kubectl context to your shared services cluster and apply the manifest</p> <pre><code>kubectl apply -f packagerepo-v1.6.1.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#prepare-user-managed-tanzu-packages","title":"Prepare user managed Tanzu Packages","text":"<ol> <li> <p>Create a common namespace used for all user managed Tanzu packages: </p> <pre><code>kubectl create ns tanzu-packages-user-managed\n</code></pre> </li> <li> <p>Replicate the <code>embedded-harbor</code> secret from the <code>tanzu-package-repo-global</code> namespace to the <code>tanzu-packages-user-managed</code> namespace:</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> <li> <p>To use the <code>embedded-harbor</code> in all cert-manager deployment's <code>spec.template.spec.imagePullSecrets</code> we have to create a ytt overlay and use that overlay in the <code>PackageInstall</code>. Create the overlay <code>image-pull-secrets-overlay-deployment.yaml</code></p> <pre><code>#@ load(\"@ytt:overlay\", \"overlay\")\n#@overlay/match by=overlay.subset({\"kind\": \"Deployment\"}), expects=\"1+\"\n---\nspec:\n  template:\n    spec:\n      #@overlay/match missing_ok=True\n      imagePullSecrets:\n      - name: embedded-harbor\n</code></pre> <p>and then create a Kubernetes secret:</p> <pre><code>kubectl create secret generic image-pull-secret-overlay-deployment \\\n  --from-file=image-pull-secrets-overlay-deployment.yaml \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> <li> <p>We do the same for DaemonSets. Create the overlay <code>image-pull-secrets-overlay-daemonset.yaml</code></p> <pre><code>#@ load(\"@ytt:overlay\", \"overlay\")\n#@overlay/match by=overlay.subset({\"kind\": \"DaemonSet\"}), expects=\"1+\"\n---\nspec:\n  template:\n    spec:\n      #@overlay/match missing_ok=True\n      imagePullSecrets:\n      - name: embedded-harbor\n</code></pre> <p>and then create a Kubernetes secret:</p> <pre><code>kubectl create secret generic image-pull-secret-overlay-daemonset \\\n  --from-file=image-pull-secrets-overlay-daemonset.yaml \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> <li> <p>We do the same for StatefulSets. Create the overlay <code>image-pull-secrets-overlay-statefulsets.yaml</code></p> <pre><code>#@ load(\"@ytt:overlay\", \"overlay\")\n#@overlay/match by=overlay.subset({\"kind\": \"StatefulSet\"}), expects=\"1+\"\n---\nspec:\n  template:\n    spec:\n      #@overlay/match missing_ok=True\n      imagePullSecrets:\n      - name: embedded-harbor\n</code></pre> <p>and then create a Kubernetes secret:</p> <pre><code>kubectl create secret generic image-pull-secret-overlay-statefulsets \\\n  --from-file=image-pull-secrets-overlay-statefulsets.yaml \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-cert-manager","title":"Install cert-manager","text":"<p>We are following the official docs here.</p> <ol> <li> <p>Create the <code>cert-manager</code> namespace:</p> <pre><code>kubectl create ns cert-manager\n</code></pre> </li> <li> <p>Create the <code>embedded-harbor</code> secrets used in <code>imagePullSecrets</code> in each pod:</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n cert-manager\n</code></pre> </li> <li> <p>List available version for cert-manager:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep cert-manager\n</code></pre> </li> <li> <p>Create the manifest <code>cert-manager.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: cert-manager-sa\n  namespace: tanzu-packages-user-managed\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: cert-manager-sa\n    namespace: tanzu-packages-user-managed\n---\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: cert-manager\n  namespace: tanzu-packages-user-managed\n  annotations:\n    ext.packaging.carvel.dev/fetch-0-secret-name: embedded-harbor\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.0: image-pull-secret-overlay-deployment\nspec:\n  serviceAccountName: cert-manager-sa\n  packageRef:\n    refName: cert-manager.tanzu.vmware.com\n    versionSelection:\n      constraints: 1.7.2+vmware.1-tkg.1\n  values:\n  - secretRef:\n      name: cert-manager-data-values\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: cert-manager-data-values\n  namespace: tanzu-packages-user-managed\nstringData:\n  values.yml: |\n    ---\n    namespace: cert-manager\n</code></pre> <p>and apply it to the cluster:</p> <pre><code>kubectl apply -f cert-manager.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-contour","title":"Install Contour","text":"<p>The process is very simlar to installing <code>cert-manager</code>. We are following the official docs here using <code>kubectl</code>.</p> <ol> <li> <p>Create the <code>tanzu-system-ingress</code> namespace:</p> <pre><code>kubectl create ns tanzu-system-ingress\n</code></pre> </li> <li> <p>Create the <code>embedded-harbor</code> secrets used in <code>imagePullSecrets</code> in each pod:</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n tanzu-system-ingress\n</code></pre> </li> <li> <p>List available version for contour:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep contour\n</code></pre> </li> <li> <p>Create the manifest <code>contour.yaml</code></p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: contour-sa\n  namespace: tanzu-packages-user-managed\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: contour-role-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: contour-sa\n    namespace: tanzu-packages-user-managed\n---\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: contour\n  namespace: tanzu-packages-user-managed\n  annotations:\n    ext.packaging.carvel.dev/fetch-0-secret-name: embedded-harbor\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.0: image-pull-secret-overlay-deployment\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.1: image-pull-secret-overlay-daemonset\nspec:\n  serviceAccountName: contour-sa\n  packageRef:\n    refName: contour.tanzu.vmware.com\n    versionSelection:\n      constraints: 1.20.2+vmware.2-tkg.1\n  values:\n  - secretRef:\n      name: contour-data-values\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: contour-data-values\n  namespace: tanzu-packages-user-managed\nstringData:\n  values.yml: |\n    ---\n    infrastructure_provider: vsphere\n    namespace: tanzu-system-ingress\n    contour:\n      configFileContents: {}\n      useProxyProtocol: false\n      replicas: 2\n      pspNames: \"vmware-system-restricted\"\n      logLevel: info\n    envoy:\n      service:\n        type: LoadBalancer\n        annotations: {}\n        nodePorts:\n          http: null\n          https: null\n        externalTrafficPolicy: Cluster\n        disableWait: false\n      hostPorts:\n        enable: false\n        http: 80\n        https: 443\n      hostNetwork: false\n      terminationGracePeriodSeconds: 300\n      logLevel: info\n      pspNames: null\n    certificates:\n      duration: 8760h\n      renewBefore: 360h\n</code></pre> <p>and apply it to the cluster:</p> <pre><code>kubectl apply -f contour.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-harbor","title":"Install Harbor","text":"<ol> <li> <p>Create the <code>tanzu-system-registry</code> namespace:</p> <pre><code>kubectl create ns tanzu-system-registry\n</code></pre> </li> <li> <p>Create the <code>embedded-harbor</code> secrets used in <code>imagePullSecrets</code> in each pod:</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n tanzu-system-registry\n</code></pre> </li> <li> <p>List available version for harbor:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep harbor\n</code></pre> </li> <li> <p>Create the manifest <code>harbor.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: harbor-sa\n  namespace: tanzu-packages-user-managed\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: habor-role-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: harbor-sa\n    namespace: tanzu-packages-user-managed\n---\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: harbor\n  namespace: tanzu-packages-user-managed\n  annotations:\n    ext.packaging.carvel.dev/fetch-0-secret-name: embedded-harbor\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.0: image-pull-secret-overlay-deployment\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.1: image-pull-secret-overlay-statefulsets\nspec:\n  serviceAccountName: harbor-sa\n  packageRef:\n    refName: harbor.tanzu.vmware.com\n    versionSelection:\n      constraints: 2.3.3+vmware.1-tkg.1\n  values:\n  - secretRef:\n      name: harbor-data-values\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: harbor-data-values\n  namespace: tanzu-packages-user-managed\nstringData:\n  values.yml: |\n    namespace: tanzu-system-registry\n    hostname: harbor.internal\n    port:\n      https: 443\n    logLevel: info\n    tlsCertificate:\n      tls.crt: \"\"\n      tls.key: \"\"\n      ca.crt:\n    tlsCertificateSecretName:\n    enableContourHttpProxy: true\n    harborAdminPassword: 'VMware1!'\n    secretKey: 'aiGhooghu8uaS7zo'\n    database:\n      password: 'VMware1!'\n      shmSizeLimit:\n      maxIdleConns:\n      maxOpenConns:\n    exporter:\n      cacheDuration:\n    core:\n      replicas: 1\n      secret: 'VMware1!'\n      xsrfKey: oopoo7iecae8wai5eejeethaingeip4W\n    jobservice:\n      replicas: 1\n      secret: 'VMware1!'\n    registry:\n      replicas: 1\n      secret: 'VMware1!'\n    notary:\n      enabled: true\n    trivy:\n      enabled: true\n      replicas: 1\n      gitHubToken: \"\"\n      skipUpdate: false\n    persistence:\n      persistentVolumeClaim:\n        registry:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 50Gi\n        jobservice:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 10Gi\n        database:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 10Gi\n        redis:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 10Gi\n        trivy:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 10Gi\n    proxy:\n      httpProxy:\n      httpsProxy:\n      noProxy: 127.0.0.1,localhost,.local,.internal\n    pspNames: vmware-system-restricted,vmware-system-privileged\n    network:\n      ipFamilies: [\"IPv4\", \"IPv6\"]\n</code></pre> <p>and apply it to the cluster:</p> <pre><code>kubectl apply -f harbor.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-packages","title":"Upgrade Packages","text":"<p>We are updating the following packages:</p> Package Current Version Target Version cert-manager 1.7.2+vmware.1-tkg.1 1.10.2+vmware.1-tkg.1 contour 1.20.2+vmware.2-tkg.1 1.23.5+vmware.1-tkg.1 harbor 2.3.3+vmware.1-tkg.1 2.5.3+vmware.1-tkg.1"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-tanzu-package-repository","title":"Upgrade Tanzu Package Repository","text":"<p>In order to have newer Package versions available, we first have to upgrade the Tanzu Package Repository.</p> <ol> <li> <p>list available Package Repository versions:</p> <pre><code>imgpkg tag list -i projects.registry.vmware.com/tkg/packages/standard/repo\n</code></pre> </li> <li> <p>Copy a new version to your local registry</p> <pre><code>imgpkg copy \\\n  -b projects.registry.vmware.com/tkg/packages/standard/repo:v2.2.0 \\\n    --to-repo 172.30.4.131/shared-services/packages/standard/repo \\\n    --registry-ca-cert-path ./ca.crt\n</code></pre> </li> <li> <p>Update the existing Package repository accordingly</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\n  name: tanzu-standard\n  namespace: tanzu-package-repo-global\nspec:\n  fetch:\n    imgpkgBundle:\n      image: 172.30.4.131/shared-services/packages/standard/repo:v2.2.0\n      secretRef:\n        name: embedded-harbor\nEOF\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-cert-manager","title":"Upgrade cert-manager","text":"<ol> <li> <p>List available version for cert-manager:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep cert-manager\n</code></pre> </li> <li> <p>Find a compatible cert-manager version with your Kubernetes version  and update <code>cert-manager</code>:</p> <pre><code>tanzu package installed update cert-manager \\\n  --version 1.10.2+vmware.1-tkg.1 \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-contour","title":"Upgrade contour","text":"<ol> <li> <p>List available version for contour:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep contour\n</code></pre> </li> <li> <p>Find a compatible contour version with your Kubernetes version and update <code>contour</code>:</p> <pre><code>tanzu package installed update contour \\\n  --version 1.23.5+vmware.1-tkg.1 \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-harbor","title":"Upgrade harbor","text":"<ol> <li> <p>List available version for harbor:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep harbor\n</code></pre> </li> <li> <p>Update <code>harbor</code>:</p> <pre><code>tanzu package installed update harbor \\\n  --version 2.5.3+vmware.1-tkg.1 \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#things-to-note","title":"Things to note","text":""},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#using-local-content-library","title":"Using local content library","text":"<p>when using local content library, the name when importing in vCenter has to be the same name as in <code>item.json</code>. Otherwise you get the following error when using the TKR:</p> <pre><code>unable to resolve that TKR due to could not resolve TKR/OSImage for controlPlane\n</code></pre>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/","title":"Tanzu Platform Technical Details","text":""},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#questions","title":"Questions","text":"<ul> <li>What happens when I don't enable <code>Tanzu Application Engine</code> when creating a cluster Group? Does it mean, the clusters in it are not managed by UCP?</li> <li>there is a dedicated ManagedNamespace for each AvailabilityTarget of a ManagedNamespaceSet =&gt; test this</li> <li>when interacting via kubectl with UCP (using kubeconfig <code>~/.config/tanzu/kube/config</code>), then you see different things depending which context you use (<code>project</code>, <code>clustergroup</code> or <code>space</code> context). Which objects do you see using which context?</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#concepts","title":"Concepts","text":"<p>There is a very good page explaining the concepts here. You have to be familiar with</p> <ul> <li>Spaces</li> <li>Capabilities</li> <li>Cluster Groups</li> <li>Availability Targets</li> <li>Profiles</li> <li>Traits</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#notes","title":"Notes","text":"<ul> <li>groupings (cluster groups to group clusters, availability targets to also group clusters, spaces to group namespaces) is all based on labels</li> <li>because of label selectors, a cluster might belong to one or many Availability Targets</li> <li>because of label selectors, a cluster might belong to one or many Cluster Groups</li> <li>capabilities are provided by Tanzu Packages (Carvel Packages) installed to every cluster belonging to the cluster group that provides this capability</li> <li>if a capability is provided to a cluster group, the corresponding Tanzu Package installed on all clusters in that cluster group have the <code>capability.tanzu.vmware.com/provides: &lt;capability-name&gt;</code> annotation</li> <li> <p>Capabilities vs Traits</p> <ul> <li>Capabilities provides the set of CRDs (with controllers)</li> <li>Traits creates an instance of the CRD (CR - Custom Resource)</li> <li>example:<ul> <li>the <code>cert-manager</code> capability deploys the <code>cert-manager</code> pods and installs the cert-manager CRDs, like <code>ClusterIssuer</code>, <code>Certificate</code> etc.</li> <li>there is a <code>multicloud-certmanager</code> trait, which actually creates certificate issuers, so custom resources of <code>kind: ClusterIssuer</code> etc. and actually makes use of the <code>cert-manager</code> issuer</li> </ul> </li> </ul> </li> <li> <p>Profiles is just a configuration but you don't really deploy anything by creating a profile. A space is ultimately combining everything to provision something with the information from a profile. This means, that all available capabilities are provided on the cluster group, but you can only use capabilities in your spaces if you have provided them in your space via the profile</p> </li> <li>when creating a space you can select an Availability Target and with it how many replicas of the AT - when you configure 3 replicas, the UCP will create 3 managed namespaces across 3 clusters</li> <li>namespaces/pods added by Hub (UCP)<ul> <li>tanzu-cluster-group-system: used to deploy <code>PackageInstalls</code> of all capabilities installed on the cluster group</li> <li>tanzu system<ul> <li><code>aria-k8s-collector</code></li> <li><code>syncer</code></li> <li><code>tanzu-capabilities-controller-manager</code></li> </ul> </li> <li>vmware-system-tmc</li> </ul> </li> <li>Hub adds 4 Package Repositories that bundle all capabilities</li> <li>when you create a space, it will<ul> <li>create a <code>ManagedNamespaceSet</code> on UCP</li> <li>which will create a <code>ManagedNamespace</code> on UCP</li> <li>which will create two <code>Namespace</code>'s on the target cluster, called <code>&lt;space-name&gt;-&lt;hash&gt;</code> and <code>&lt;space-name&gt;-&lt;hash&gt;-internal</code><ul> <li>the internal namespace is used to deploy <code>PackageInstalls</code> of the traits, referenced in the associated profiles</li> <li>the \"normal\" namespace is used to deploy end applications</li> </ul> </li> </ul> </li> <li>a <code>Space</code>and a <code>ManagedNamespace</code> (and the final <code>Namespace</code> on the target cluster(s) - which is essentially a 1:n mirror from a <code>ManagedNamespace</code>) can be compared to a <code>Deployment</code> and a <code>Pod</code><ul> <li>each time you update the Space, it will create a new <code>ManagedNamespaceSet</code></li> </ul> </li> <li><code>Spaces</code> are reconciled by a <code>Space Controller</code></li> <li>there is a dedicated ManagedNamespace for each AvailabilityTarget of a ManagedNamespaceSet</li> <li>once a ManagedNamespace has been created successfully on UCP, there is the <code>Space Scheduler</code> that provisions a namespace on a target cluster and installs the traits into that namespace</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#faq","title":"FAQ","text":""},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#when-i-have-1-availability-target-with-3-clusters-and-i-choose-to","title":"When I have 1 availability target with 3 clusters and I choose to","text":"<ul> <li>deploy a space with 1 replica: will the namespace get deployed on one cluster only? =&gt; yes</li> <li>deploy a space with 2 replicas: will 2 namespaces get deployed on one cluster or 1 namespace on each cluster? =&gt; 2 namespaces on one cluster is possible</li> <li>deploy a space with 3 replicas: will the namespace get deployed evenly, 1 namespace on each cluster? =&gt; no. 3 namespaces on one cluster is possible</li> <li>deploy a space with more replicas than available clusters =&gt; explained by above behaviour</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#who-are-the-personas-and-who-is-responsible-for-what","title":"Who are the personas and who is responsible for what?","text":"<p>Where are the boundaries? What's the motivation behind the technical architecture - who is supposed to see what and what is intended to be abstracted away? =&gt; https://docs.google.com/document/d/1Rmt-eskKBo2mNDsSdHeVaIWuZU3zhKznEDcRVN9BRRs/edit#heading=h.ixfosuy76boz</p>"},{"location":"tanzu/tkgs/supervisor-cluster-upgrade-failed-in-vsphere-8/","title":"Supervisor Cluster upgrade failed in vSphere 8u2b","text":""},{"location":"tanzu/tkgs/supervisor-cluster-upgrade-failed-in-vsphere-8/#issue-description","title":"Issue Description","text":"<p>When I upgraded the Supervisor cluster from 1.25.6 to 1.26.8 on vCenter version 8u2b which has been recently upgraded from 7u3p, I faced an issue that the upgrade process was stuck at 50% in vCenter and the <code>vsphere-csi-controller</code> pods in namespace <code>vmware-system-csi</code> were in <code>CrashLoopBackOff</code> resulting in the error</p> <pre><code>failed to init controller. Error: could not find any AvailabilityZone\n</code></pre> <p>and <code>kubectl get az</code> on the Supervisor Clusters doesn't  show any output.</p> <p>When ssh'ing into a Supervisor Cluster Control Plane node you can execute the following script to find more details of the upgrade process with</p> <pre><code>/usr/lib/vmware-wcp/upgrade/upgrade-ctl.py get-status \\\n  | jq '.progress | to_entries | .[] | \"\\(.value.status) - \\(.key)\"' \\\n  | sort\n</code></pre> <p>This gave me the following result:</p> <pre><code>\"failed - ImageRegistryUpgrade\"\n\"pending - CapvUpgrade\"\n\"pending - CertManagerAdditionalUpgrade\"\n\"pending - LicenseOperatorControllerUpgrade\"\n\"pending - NamespaceOperatorControllerUpgrade\"\n\"pending - PinnipedUpgrade\"\n\"pending - PspOperatorUpgrade\"\n\"pending - TkgUpgrade\"\n\"pending - TMCUpgrade\"\n\"pending - VmOperatorUpgrade\"\n\"processing - UtkgControllersUpgrade\"\n\"skipped - HarborUpgrade\"\n\"skipped - LoadBalancerApiUpgrade\"\n\"upgraded - AKOUpgrade\"\n\"upgraded - AppPlatformOperatorUpgrade\"\n\"upgraded - CapwUpgrade\"\n\"upgraded - CertManagerUpgrade\"\n\"upgraded - CsiControllerUpgrade\"\n\"upgraded - ExternalSnapshotterUpgrade\"\n\"upgraded - ImageControllerUpgrade\"\n\"upgraded - KappControllerUpgrade\"\n\"upgraded - NetOperatorUpgrade\"\n\"upgraded - NSXNCPUpgrade\"\n\"upgraded - RegistryAgentUpgrade\"\n\"upgraded - SchedextComponentUpgrade\"\n\"upgraded - SphereletComponentUpgrade\"\n\"upgraded - TelegrafUpgrade\"\n\"upgraded - UCSUpgrade\"\n\"upgraded - UtkgClusterMigration\"\n\"upgraded - VMwareSystemLoggingUpgrade\"\n\"upgraded - WCPClusterCapabilities\"\n</code></pre> <p>where we can see that the <code>ImageRegistryUpgrade</code> failed. Looking further into <code>var/log/VMware/upgrade-ctl-compupgrade.log</code> we could see the following error:</p> <pre><code>2024-06-18T05:33:08.437Z DEBUG comphelper: ret=0 out={\n    \"apiVersion\": \"v1\",\n    \"items\": [\n        {\n            \"apiVersion\": \"imageregistry.vmware.com/v1alpha1\",\n            \"kind\": \"ContentLibrary\",\n            \"metadata\": {\n                \"annotations\": {\n                    \"kubectl.kubernetes.io/last-applied-configuration\": \"{\\\"apiVersion\\\":\\\"imageregistry.vmware.com/v1alpha1\\\",\\\"kind\\\":\\\"ContentLibrary\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"cl-fe4e8c74d59491be7\\\",\\\"namespace\\\":\\\"test-vsphere-namespace\\\"},\\\"spec\\\":{\\\"uuid\\\":\\\"5f773a1c-5aa3-4268-871a-359401c55950\\\",\\\"writable\\\":false}}\\n\"\n2024-06-18T05:33:08.438Z ERROR compupgrade: {\"error\": \"TypeError\", \"message\": \"argument of type 'NoneType' is not iterable\", \"backtrace\": [\"  File \\\"/usr/lib/vmware-wcp/upgrade/compupgrade.py\\\", line 362, in do_upgrade_with_out_resume_failed_support\\n    comp.doUpgrade(upCtx)\\n\", \"  File \\\"/usr/lib/vmware-wcp/objects/image-registry-operator/imageregistry_upgrade.py\\\", line 323, in doUpgrade\\n    self.updateV1alpha1ImageRegistryResources()\\n\", \"  File \\\"/usr/lib/vmware-wcp/objects/image-registry-operator/imageregistry_upgrade.py\\\", line 171, in updateV1alpha1ImageRegistryResources\\n    self.updateV1alpha1Resource('contentlibraries', True)\\n\", \"  File \\\"/usr/lib/vmware-wcp/objects/image-registry-operator/imageregistry_upgrade.py\\\", line 193, in updateV1alpha1Resource\\n    patch_list = self.getResourcePatchBody(status, resource_kind)\\n\", \"  File \\\"/usr/lib/vmware-wcp/objects/image-registry-operator/imageregistry_upgrade.py\\\", line 218, in getResourcePatchBody\\n    if 'UTC' in creation_time:\\n\"]}\n</code></pre> <p>The error <code>argument of type 'NoneType' is not iterable</code> first indicated that the root cause was because of the missing Availability Zone as mentioned earlier. But digging into the logs of the <code>imageregistry-controller-manager</code> we could see the following errors:</p> <pre><code>\"msg\"=\"Reconciler error\" \"error\"=\"The underlying content library with ID 5f773a1c-5aa3-4268-871a-359401c55950 does not exist in vSphere\"\n</code></pre>"},{"location":"tanzu/tkgs/supervisor-cluster-upgrade-failed-in-vsphere-8/#root-cause","title":"Root Cause","text":"<p>There is an operator introduced in vCenter 8.x called <code>ImageRegistryOperator</code> that takes over some of the responsibilities of <code>VMoperator</code>. Part of the upgrade script is to migrate VMoperator's <code>ContentSource</code> and <code>ContentSourceBindings</code> to newer CRDs (<code>ContentLibrary</code>, <code>ClusterContentLibrary</code>, <code>ContentLibraryItems</code> etc.).</p> <p>There was a bug in the upgrade script (already fixed in newer versions of vCenter) that is exposed when a Supervisor namespace references a content library that does not exist, e.g. because it was removed from vCenter. Other component upgrades stalled out because they depend on <code>ImageRegistryUpgrade</code> to be finished - this also caused the <code>CrashLoopBackOff</code> of the <code>csi-controller</code> because <code>VmOperatorUpgrade</code> hasn't started yet and has not yet created the missing <code>AvailabilityZone</code> custom resource on the Supervisor Cluster.</p> <p>In this case, the content library <code>5f773a1c-5aa3-4268-871a-359401c55950</code> has been already deleted in vCenter, but it's still associated on some namespaces. This can be seen using the VMware Datacenter CLI (dcli):</p> <pre><code>dcli&gt; namespaces instances get --namespace &lt;vsphere-namespace-name&gt;\n</code></pre> <p>or when executing <code>kubectl get contentsourcebinding -A</code> on the Supervisor Cluster. Also, the orphaned <code>ContentLibrary</code> resource is still present on the Supervisor cluster when executing <code>kubectl get contentlibrary</code>.</p>"},{"location":"tanzu/tkgs/supervisor-cluster-upgrade-failed-in-vsphere-8/#resolution","title":"Resolution","text":"<ol> <li> <p>delete the content library reference on all vSphere namespaces using dcli:</p> <pre><code>dcli&gt; namespaces instances update --namespace &lt;vsphere-namespace-name&gt; --content-libraries '[]'\n</code></pre> </li> <li> <p>delete all <code>contentsourcbindings</code> related to the orphaned Content Library with</p> <pre><code>kubectl delete contentsourcebinding -n &lt;vsphere-namespace-name&gt; 5f773a1c-5aa3-4268-871a-359401c55950\n</code></pre> </li> <li> <p>delete the orphaned <code>contentlibrary</code> reference on the Supervisor cluster with</p> <pre><code>kubectl delete contentlibrary &lt;name&gt;\n</code></pre> </li> <li> <p>ensure the last step also deleted the corresponding <code>contentsource</code> with <code>kubectl get contentsource</code>.</p> </li> </ol> <p>The Supervisor Cluster upgrade process will be retried automatically. You can follow the progress again with</p> <pre><code>/usr/lib/vmware-wcp/upgrade/upgrade-ctl.py get-status \\\n  | jq '.progress | to_entries | .[] | \"\\(.value.status) - \\(.key)\"' \\\n  | sort\n</code></pre> <p>and you should see that <code>ImageRegistryUpgrade</code> process should complete successfully. After <code>VmOperatorUpgrade</code> has been completed you should also see the missing <code>AvailabilityZone</code> with <code>kubectl get az</code> and the <code>csi-controller</code> pods running successfully.</p>"},{"location":"tanzu/tkgs/tkgs-airgapped/","title":"Set up an airgapped TKGS homelab environment","text":"<p>See Set up an airgapped TKGS homelab environment.</p>"},{"location":"tanzu/tkgs/tkgs-nsxt-integration/","title":"vSphere with Tanzu integration with NSX-T","text":"<p>This page explains how Tanzu Kubernetes Clusters, sometimes referred to as guest clusters or workload clusters, integrate with NSX-T to provide L4 Load Balancing services via Kubernetes Services of type LoadBalancer.</p>"},{"location":"tanzu/tkgs/tkgs-nsxt-integration/#general-concept-of-load-balancer-integration","title":"General concept of Load Balancer integration","text":"<p>A Kubernetes Service of type LoadBalancer is an upstream Kubernetes concept that assigns a routable external IP address to a Kubernetes Service and, thus, make the service available from outside of the cluster. This IP assignment has to happen by the underlying infrastructure, the underlying network infrastructure to be more specific. Of course Kubernetes does not know and care about the underlying infrastructure, so it is the responsibility of cloud providers and infrastructure providers to implement this functionality.</p> <p>If you've read Kubernetes Architecture Concepts you know that everything in Kubernetes is implemented as controllers. In essence, an infrastructure provider has to implement a load balancer controller and run it on the cluster:</p> <p></p> <p>The load balancer controller watches for <code>Create</code>, <code>Update</code> and <code>Delete</code> events of Services of type <code>LoadBalancer</code>, interacts with the underlying network infrastructure provider and reports back status.</p> <p>As an example:</p> <ol> <li>a user creates a service of type <code>LoadBalancer</code></li> <li>the load balancer controller reacts on this event and determines it is a <code>Create</code> event</li> <li>load balancer controller communicates with the network infrastructure API and creates a Load Balancer, or Virtual Service or similar</li> <li>the network infrastructure API responds with an external IP address</li> <li>the load balancer controller updates the service's <code>status.loadBalancer</code> field and especially updates <code>status.loadBalancer.ingress.ip</code> with this external IP address.</li> </ol>"},{"location":"tanzu/tkgs/tkgs-nsxt-integration/#integration-in-aws","title":"Integration in AWS","text":"<p>When you use Amazon Elastic Kubernetes Service (EKS) there is the AWS Load Balancer Controller running on the EKS cluster responsible for provisioning Network Load Balancers and Application Load Balancers.</p>"},{"location":"tanzu/tkgs/tkgs-nsxt-integration/#integration-in-vsphere-with-tanzu-with-nsx-t","title":"Integration in vSphere with Tanzu with NSX-T","text":"<p>The Load Balancer integration on Tanzu Kubernetes Clusters is not as easy and straightforward as explained above. Indeed, there is a NSX-T load balancer controller, the so-called NSX-T Container Plugin (NCP), which you could deploy directly on a Tanzu Kubernetes Cluster. But, vSphere with Tanzu does not deploy it on the guest cluster because it would require to store NSX-T credentials on that cluster, too.</p> <p>One of the main philosophies in vSphere with Tanzu is having different personas with different responsibilities: a vSphere administrator, a DevOps engineer (or cluster admin) and an application developer. Tanzu Kubernetes Clusters are purposed for DevOps engineers, sometimes even application developers, giving them full cluster admin permissions on that cluster and consequently empowering full flexibility. The NSX-T infrastructure, though, is maintained by vSphere Administrators, or even another dedicated networking team, but is not maintained by those DevOps engineers or application developers. Therefore, credentials to NSX-T must not be exposed on Tanzu Kubernetes Clusters.</p> <p>The solution to still be able to integrate with NSX-T and to still be able to dynamically deploy services of type <code>LoadBalancer</code> and having the full flexibility as a DevOps engineer working with Kubernetes, vSphere with Tanzu realizes it using the following approach:</p> <p></p> <p>Let's see what happens when a user creates a <code>Service of type LoadBalancer</code> on a Tanzu Kubernetes Cluster (the same principle applies for <code>update</code> and <code>delete</code>):</p> <ol> <li>on the guest cluster, there is a controller called <code>guest-cluster-cloud-provider</code> that watches for Kubernetes Service's <code>Create</code>, <code>Update</code> and <code>Delete</code> events. On the <code>Create</code> event it creates a <code>VirtualMachineService</code> Custom Resource on the Supervisor cluster by calling the Supervisor Cluster Kubernetes API. This <code>VirtualMachineService</code> can have different types, in this example it is of type <code>LoadBalancer</code>.</li> <li>On the Supervisor cluster there is a controller called <code>vmop-controller-manager</code> that has multiple responsibilities with watching <code>Create</code>, <code>Update</code> and <code>Delete</code> events of the <code>VirtualMachineService</code> being one of them. On the <code>Create</code> event it creates a Service of type <code>LoadBalancer</code> on the Supervisor cluster which is essentially a mirror of the same service deployed on the guest cluster.</li> <li>From this step onwards, the common techniques applies as described above: the <code>NSX-T Container Plugin (NCP)</code>, the \"NSX-T Load Balancer Controller\" running on the Supervisor cluster, watches for Services and interacts with the NSX-T API. In NSX-T, it creates a Load Balancer and a Virtual Service.</li> <li>NSX-T built-in IP address management system assigns an external IP address to this Virtual Service and responds it back to the API Call initiated by NCP.</li> <li>NCP writes the external IP address to the Service's <code>status.loadBalancer.ingress.ip</code> on the Supervisor cluster.</li> <li>the <code>vmop-controller-manager</code> reacts on this <code>Update</code> event and reports back that status to the <code>VirtualMachineService</code>.</li> <li>the <code>guest-cluster-cloud-provider</code>, which regularly polls the <code>VirtualMachineService</code> (it can't subscribe to <code>Create</code>, <code>Update</code> and <code>Delete</code> as it is running on a different Kubernetes Cluster), finally reports back the status, and especially the external IP address, to the original service of type <code>LoadBalancer</code> which has been created by the end user.</li> </ol> <p>Info</p> <p>As you can see in the image, the actual network traffic from the intranet/internet to the target Kubernetes Service running on the Tanzu Kubernetes Cluster goes directly from NSX-T to the target guest cluster, and not going the route through the Supervisor Cluster. This \"hop\" architecture is only used for creating, updating and deleting services.</p>"}]}