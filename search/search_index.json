{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my Technical Scratchpad","text":"<p>I use this page to document all kinds of stuff because I am getting old and rusty and forget things. So this page is mainly for me but I'm happy if others can benefit from it as well .</p> <p>If you have any questions, feedback or other comments to certain pages, please let me know by either creating a github issue or email me at andy.knapp.ak@gmail.com.</p>"},{"location":"#upcoming","title":"Upcoming","text":"<ul> <li>TKGS integration with vSphere CSI controller</li> </ul>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>Every content on this page represents my personal experiences and personal view only.</p>"},{"location":"homelab/","title":"Homelab","text":"<p>When I joined VMware in February 2021 I have built a homelab to be able to quickly spin up test environments using VMware products with a primary focus on Tanzu Labs product portfolio.</p>"},{"location":"homelab/#bom","title":"BOM","text":"Component Item CPU HP/AMD EPYC 7551 PS7551BDVIHAF 2GHz 64MB 32 Core Processor Motherboard Supermicro H11SSL-i Socket SP3 ATX RAM Samsung 4x 64GB 256GB DDR4 ECC RAM 2933 Mhz RDIMM SSD Crucial MX500 2TB NVMe Samsung MZ-V7S2T0BW SSD 970 EVO Plus 2 TB M.2 Internal NVMe SSD (up to 3.500 MB/s) Cooler Noctua NH U12s TR4 SP3 GPU Asus GeForce GT 710 1GB Case Fractal Design Core 2500 PSU Kolink Enclave 500W"},{"location":"homelab/#setup","title":"Setup","text":"<p>There is a \"Management vCenter\" (<code>vcenter-mgmt</code>) VM deployed on the physical host that manages the physical host. As a result, the Management vCenter looks like this:</p> <p></p> <p>Here you can see:</p> <ul> <li><code>192.168.178.100</code>: the physical ESXi host</li> <li><code>jumpbox01</code>: an Ubuntu jumpbox for testing purposes</li> <li>VMs prefixed with <code>tkgs-</code>: a nested lab environment (more details see Nested Lab Setup)</li> <li><code>vcenter-mgmt</code>: Management vCenter</li> <li><code>vrli</code>: vRealize Log Insight</li> <li><code>vrops</code>: vRealize Operations Manager</li> <li><code>vyos</code>: the VyOS Router, responsible for the entire homelab network</li> <li><code>windows</code>: a Windows VM</li> </ul>"},{"location":"homelab/#networking-routing","title":"Networking &amp; Routing","text":"<p>The physical ESXi host has a virtual Switch <code>vSwitch0</code> with 2 port groups <code>Management Network</code> (the default port group) and <code>Home Network</code>:</p> <p></p> <p>Apart from the physical host there is only <code>vyos</code> running in the same network. The VyOS router acts as the entry point and default gateway to the homelab. My home router has a static IP route configured to forward requests to my internal homelab IP ranges <code>172.20.0.0/16</code> and <code>172.30.0.0/16</code> to the VyOS router.</p> <p>The VyOS router has another NIC in a trunk port group deployed on a virtual distributed switch (<code>vds-internal</code>) via the management vCenter:</p> <p></p> <p>Looking into the vyos configuration we thus have two network interfaces - <code>eth0</code> is the <code>Home Network</code> and <code>eth1</code> is the trunk port group:</p> <pre><code>vyos@vyos# show interfaces\n ethernet eth0 {\n     address 192.168.178.101/24\n     hw-id 00:0c:29:85:a5:3b\n     offload {\n         gro\n         gso\n         sg\n         tso\n     }\n }\n ethernet eth1 {\n     hw-id 00:0c:29:85:a5:45\n     mtu 9000\n }\n loopback lo {\n }\n</code></pre> <p>Refer to the VyOS Quick Start guide for more information how to configure interfaces, protocols, firewalls and more.</p>"},{"location":"homelab/#enable-mac-learning","title":"Enable Mac Learning","text":"<p>Read Native MAC Learning in vSphere 6.7 removes the need for Promiscuous mode for Nested ESXi - I am not the best person to explain this .</p> <p>TL;DR: It's best to enable mac learning on all port groups deployed on the vDS <code>vds-internal</code> used for nested labs.</p> <p>Steps:</p> <ol> <li>Install Powershell</li> <li>open it in a terminal emulator: <code>pwsh</code></li> <li> <p>Install PowerCLI:</p> <pre><code>Install-Module VMware.PowerCLI -Scope CurrentUser\n</code></pre> </li> <li> <p>Connect to vCenter:</p> <pre><code>Connect-VIServer -Server 192.168.178.102 -Protocol https -User administrator@vsphere.local -Password VMware1!\n</code></pre> </li> <li> <p>download the powershell functions <code>Get-MacLearn</code> and <code>Set-MacLearn</code> from here</p> </li> <li> <p>Set Mac learning on a port group:</p> <pre><code>Set-MacLearn -DVPortgroupName @(\"Nested-01-DVPG\") -EnableMacLearn $true -EnablePromiscuous $false -EnableForgedTransmit $true -EnableMacChange $false\n</code></pre> </li> <li> <p>Get Mac learning details:</p> <pre><code>Get-MacLearn -DVPortgroupName @(\"Nested-01-DVPG\")\n</code></pre> </li> </ol>"},{"location":"homelab/#create-a-network-for-a-nested-lab-environment","title":"Create a network for a nested lab environment","text":"<p>In order to create different networks for different nested lab environments I have to:</p> <ol> <li>create distributed port group on the vDS <code>vds-internal</code> with a specific VLAN ID</li> <li>create an interface with the same VLAN ID number as <code>vif</code> on <code>eth1</code> in VyOS</li> </ol>"},{"location":"homelab/#nested-lab-network-example","title":"Nested Lab network example","text":"<p>We create a distributed port group called <code>TKGM</code> with VLAN ID <code>12</code>:</p> <p></p> <p>As a consequence we create a virtual interface in VyOS with ID <code>12</code> and an IP range of my choice:</p> <pre><code>set interfaces ethernet eth1 vif 12 address 172.20.12.1/22\nset interfaces ethernet eth1 vif 12 description TKGM\nset interfaces ethernet eth1 vif 12 mtu 9000\ncommit \nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show interfaces\n ethernet eth0 {\n     address 192.168.178.101/24\n     hw-id 00:0c:29:85:a5:3b\n     offload {\n         gro\n         gso\n         sg\n         tso\n     }\n }\n ethernet eth1 {\n     hw-id 00:0c:29:85:a5:45\n     mtu 9000\n     vif 12 {\n         address 172.20.12.1/22\n         description TKGM\n         mtu 9000\n     }\n }\n loopback lo {\n }\n</code></pre> <p>We can then specify this port group in the nested lab setup config file - see details below.</p>"},{"location":"homelab/#use-a-nsx-segment-and-make-it-routable","title":"Use a NSX segment and make it routable","text":"<p>Some of my nested labs include NSX software-defined network as overlay networks for products such as vSphere IaaS Control Plane, Tanzu Application Service (TAS) for VMs or Tanzu Kubernetes Grid Integrated Edition (TKGI).</p> <p>When creating a NSX Segment I have to create a static route of the segment's CIDR with the NSX Tier-0 IP address being the next-hop. Assuming you want to use the cidr <code>172.30.2.0/24</code> for a NSX segment, and the Tier0's IP address is <code>172.20.17.13</code>, then you create a static route in VyOS with</p> <pre><code>set protocols static route 172.30.2.0/24 next-hop 172.20.17.13\n</code></pre> <p>Info</p> <p>When building your nested labs with vmware-lab-builder and the opinionated var-examples the T0's IP address is the 3rd IP after the <code>starting_addr</code>. So in this TKGS+NSX-T example with <code>starting_addr: \"192.168.0.160\"</code> the T0's IP address will be <code>192.168.0.163</code>.</p>"},{"location":"homelab/#nested-lab-setup","title":"Nested Lab Setup","text":"<p>To bootstrap nested lab environments I am using vmware-lab-builder, Kudos to Matt .</p> <p>An example config I am using to deploy a TKGm environment is</p> <pre><code>---\n# SOFTWARE_DIR must contain all required software\nvc_iso: \"{{ lookup('env', 'SOFTWARE_DIR') }}/VMware-VCSA-all-7.0.3-19234570.iso\"\nesxi_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/Nested_ESXi7.0u3c_Appliance_Template_v1.ova\"\nnsx_alb_controller_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/controller-20.1.6-9132.ova\"\ntkgm_os_kubernetes_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/photon-3-kube-v1.25.7+vmware.2-tkg.1-8795debf8031d8e671660af83b673daa.ova\"\n\nenvironment_tag: \"tanzu-multi-cloud-avi\"  # Used to prepend object names in hosting vCenter\ndns_server: \"192.168.178.1\"\ndns_domain: \"home.local\"\nntp_server_ip: \"192.168.178.1\"\ndisk_mode: thin  # How all disks should be deployed\n# This will be set everywhere!\nnested_host_password: \"{{ opinionated.master_password }}\"\n\nhosting_vcenter:  # This is the vCenter which will be the target for nested vCenters and ESXi hosts\n  ip: \"192.168.178.102\"\n  username: \"{{ lookup('env', 'PARENT_VCENTER_USERNAME') }}\"\n  password: \"{{ lookup('env', 'PARENT_VCENTER_PASSWORD') }}\"\n  datacenter: \"Home\"  # Target for all VM deployment\n\n# This section is only referenced by other variables in this file\nopinionated:\n  master_password: \"VMware1!\"\n  number_of_hosts: 1  # number of ESXi VMs to deploy\n  nested_hosts:\n    cpu_cores: 12  # CPU count per nested host\n    ram_in_gb: 96  # memory per nested host\n    local_disks:  # (optional) this section can be removed to not modify local disks\n      - size_gb: 500\n        datastore_prefix: \"datastore\"  # omit this to not have a datastore created\n  hosting_cluster: Physical\n  hosting_datastore: NVME\n  hosting_network:\n    base:\n      port_group: TKGM\n      cidr: \"172.20.12.0/22\"\n      gateway: \"172.20.12.1\"\n      # A TKGM deployment requires 5 contiguous IPs. vCenter, Avi Controller, Esxi, 2 x Avi Service Engines.\n      starting_addr: \"172.20.12.10\"\n    # If using your own network you must provide at least a /24.\n    # In the example below the same subnet is used for both the workload nodes and the VIPs\n    # Avi will use the IP range defined vip_ip_range for VIPs\n    # TKG needs DHCP to be configured for the first half of the subnet\n    workload:\n      port_group: trunk\n      vlan_id: 24\n      cidr: \"172.20.24.0/24\"\n      gateway: \"172.20.24.1\"\n      vip_ip_range: \"172.20.24.128-172.20.24.254\"\n  avi_control_plane_ha_provider: true\n  # This public key will be assigned to the created VMs, so you must have the private key to be able to use SSH\n  ssh_public_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDNre3RSDIQIbswU/AbFMrdGDTRNIxXs1L1aY9ozDm/TsTKBf85Kr/0Bi3Az1DgOifR3s7SblZFhqKtGueqyv4NKoNy8dgxUsFzGKaRBmwGfCn9rp0hAV/r6BdyhRGloltaZ4KuR3v3AQTTjpyWPsf55wUIMYtOtrQ1tnNspmZmEyh11e8Hbwsq6jaVDCpSWkLPgir4lDTFANRpA/MaU9XUG3PjYszaNFgIScUwQVl0otmCpFgZgf9jXwy4K5CpG4u/1CSIA6H+5XAJaAzDNGcAIGQKaIAj6Cvd8QyUs+UEV5n6rJSsp+gnfu0gEYx4QmeWwcVdGu1Re18qHVgAP56gF58uV7p/V60Tlf9IFqkz67lwLlOuq9dfNmPGIJ/lydcLgcvXmJObyntm1jFi5ChtIrBd7uShh9b6wwOLhekv3TwOn0nHPjXDabVDcmfXVhAgyJyknCFv1Hm3UFwTngoc4WFm38wgxzajOzxt83b8bXXhMNaU8L8VXtVpfIrDHbAKhho6Aaf8JahKa46UxFa4yjkVfN75N4++8CdCpGiruLUp1rW4zYkrjxBuIBcib/06QTsvdPIsYVsPic/mYoxBVTclvLi/ELVdwFFSavwNKI+XwH6ENI7vckkVs6HpM0GJu4qsvmhrlkGVmNM0BkaJN1i1CQgn01OTz+um1rjr+Q== andy.knapp.ak@gmail.com\n\n#####################################################################\n### No need to edit below this line for an opinionated deployment ###\n#####################################################################\n\nnested_vcenter:  # the vCenter appliance that will be deployed\n  ip: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # vCenter ip address\n  mask: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n  gw: \"{{ opinionated.hosting_network.base.gateway }}\"\n  host_name: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # FQDN if there is working DNS server, otherwise put the ip as a name\n  username: \"administrator@vsphere.local\"\n  password: \"{{ opinionated.master_password }}\"\n  datacenter: \"Lab\"  # DC to create after deployment\n  # Below are properties of parent cluster\n  hosting_network: \"{{ opinionated.hosting_network.base.port_group }}\"  # Parent port group where the vCenter VM will be deployed\n  hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # Parent cluster where the vCenter VM will be deployed\n  hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Parent datastore where the vCenter VM will be deployed\n\nnested_clusters:  # You can add clusters in this section by duplicating the existing cluster\n  compute:  # This will be the name of the cluster in the nested  vCenter. Below are the minimum settings.\n    enable_drs: true\n    # HA can only be enabled if there is are datastores accessible by all hosts.\n    enable_ha: true\n    ha_host_monitoring: disabled\n    # Below are properties of the hosting cluster\n    hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # The nested ESXi VMs will be deployed here\n    hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Datastore target for nested ESXi VMs\n    # Settings below are assigned to each host in the cluster\n    vswitch0_vm_port_group_name: vm-network\n    vswitch0_vm_port_group_vlan: \"0\"\n    cpu_cores: \"{{ opinionated.nested_hosts.cpu_cores }}\"  # CPU count\n    ram_in_gb: \"{{ opinionated.nested_hosts.ram_in_gb }}\"  # memory\n    # In order list of disks to assign to the nested host. All will be marked as SSD.\n    # Datastore names will be automatically be pre-pended with the hostname. E.g esx1\n    # If the datastore_prefix property is removed the disk will not be set as a datastore\n    # To leave the default OVA disks in place, delete this section.\n    nested_hosts_disks: \"{{ opinionated.nested_hosts.local_disks | default(omit) }}\"\n    # Added in vmnic order, these port groups must exist on the physical host\n    # Must specify at least 2 port groups, up to a maximum of 10\n    vmnic_physical_portgroup_assignment:\n      - name: \"{{ opinionated.hosting_network.base.port_group }}\"\n      - name: \"{{ opinionated.hosting_network.workload.port_group }}\"\n    resource_pools:  # List of resource pools, remove if not needed\n      - tkc\n\n# Below specifies how many IPs are reserved for other functions\nopinionated_host_ip_ofset: 4\n# You can add nested ESXi hosts below\nnested_hosts: &gt;-\n  [\n    {% for host_number in range(opinionated.number_of_hosts) %}\n    {\n      \"name\": \"esx{{ host_number + 1 }}\",\n      \"ip\": \"{{ opinionated.hosting_network.base.starting_addr | ipmath(opinionated_host_ip_ofset + host_number) }}\",\n      \"mask\": \"{{ opinionated.hosting_network.base.cidr | ansible.netcommon.ipaddr('netmask') }}\",\n      \"gw\": \"{{ opinionated.hosting_network.base.gateway }}\",\n      \"nested_cluster\": \"compute\"\n    },\n    {% endfor %}\n  ]\n\ndistributed_switches:  # To not create any distributed switches, comment this section.\n  - vds_name: vds\n    mtu: 1500\n    vds_version: 7.0.0\n    clusters:  # distributed switch will be attached to all hosts in the clusters defined\n      - compute\n    uplink_quantity: 1\n    vmnics:\n      - vmnic1\n    distributed_port_groups:\n      - port_group_name: workload-pg\n        vlan_id: \"{{ opinionated.hosting_network.workload.vlan_id |default(0) }}\"\n\nvm_templates:\n  - local_path: \"{{ tkgm_os_kubernetes_ova }}\"\n    vcenter_server: \"{{ nested_vcenter.ip }}\"\n    vcenter_username: \"{{ nested_vcenter.username }}\"\n    vcenter_password: \"{{ nested_vcenter.password }}\"\n    vcenter_datacenter: \"{{ nested_vcenter.datacenter }}\"\n    vcenter_datastore: datastore-esx1\n    vcenter_network: workload-pg\n    vsphere_clusters: \"{{ nested_clusters.keys() | list }}\"\n\ntanzu_multi_cloud:\n  generated_config_file_name: cluster-config.yml\n\n  avi_cloud_name: \"{{ nsx_alb.cloud_name | default('Default-Cloud') }}\"\n  avi_controller: \"{{ nsx_alb.controller_ip }}\"\n  avi_data_network: \"{{ nsx_alb.se_vip_port_group }}\"\n  avi_data_network_cidr: \"{{ nsx_alb.se_vip_network_cidr }}\"\n  avi_password: \"{{ nsx_alb.controller_password }}\"\n  avi_service_engine_group: \"{{ nsx_alb.se_group_name | default('Default-Group') }}\"\n  avi_username: \"{{ nsx_alb.controller_username }}\"\n  avi_control_plane_ha_provider: \"{{ opinionated.avi_control_plane_ha_provider }}\"\n\n  # used in TKGM cluster config yaml\n  vsphere_control_plane_endpoint: \"{{ opinionated.hosting_network.workload.cidr | ipmath(2) }}\"\n  vsphere_datacenter_path: \"/{{ nested_vcenter.datacenter }}\"\n  vsphere_datastore_path: \"/{{ nested_vcenter.datacenter }}/datastore/datastore-esx1\"\n  vsphere_folder_path: \"/{{ nested_vcenter.datacenter }}/vm\"\n  vsphere_network: workload-pg\n  vsphere_password: \"{{ nested_vcenter.password }}\"\n  vsphere_resource_pool_path: \"/{{ nested_vcenter.datacenter }}/host/compute/Resources\"\n  vsphere_server: \"{{ nested_vcenter.ip }}\"\n  vsphere_ssh_authorized_key: \"{{ opinionated.ssh_public_key }}\"\n  vsphere_username: \"{{ nested_vcenter.username }}\"\n\nnsx_alb:\n  api_version: \"20.1.6\"\n  controller_username: admin\n  controller_password: \"{{ opinionated.master_password }}\"\n  controller_ssh_public_key: \"{{ opinionated.ssh_public_key }}\"\n  controller_default_password: \"{{ lookup('env', 'AVI_DEFAULT_PASSWORD') }}\"\n\n  controller_ip: \"{{ opinionated.hosting_network.base.starting_addr | ipmath(1) }}\"\n  controller_netmask: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n  controller_gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n  dns_server: \"{{ dns_server }}\"\n  ntp_server: \"{{ ntp_server_ip }}\"\n\n  controller_vcenter_ip: \"{{ hosting_vcenter.ip }}\"\n  controller_vcenter_username: \"{{ hosting_vcenter.username }}\"\n  controller_vcenter_password: \"{{ hosting_vcenter.password }}\"\n  controller_vcenter_datacenter: \"{{ hosting_vcenter.datacenter }}\"\n  controller_vcenter_cluster: \"{{ opinionated.hosting_cluster }}\"\n  controller_vcenter_datastore: \"{{ opinionated.hosting_datastore }}\"\n  controller_port_group: \"{{ opinionated.hosting_network.base.port_group }}\"\n\n  cloud_vcenter_username: \"{{ nested_vcenter.username }}\"\n  cloud_vcenter_password: \"{{ nested_vcenter.password }}\"\n  cloud_vcenter_url: \"{{ nested_vcenter.ip }}\"\n  cloud_vcenter_datacenter: \"{{ nested_vcenter.datacenter }}\"\n\n  se_vcenter_cluster: compute\n  # The management network will host he service engine management interface\n  se_management_port_group: vm-network\n  se_management_network_cidr: \"{{ opinionated.hosting_network.base.cidr }}\"\n  se_management_network_range: &gt;-\n    {{ opinionated.hosting_network.base.starting_addr | ipmath(2) }}-{{ opinionated.hosting_network.base.starting_addr | ipmath(3) }}\n  se_management_network_gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n  # The vip network will contain the virtual servers created by Avi\n  se_vip_port_group: workload-pg\n  se_vip_network_cidr: \"{{ opinionated.hosting_network.workload.cidr }}\"\n  se_vip_network_range: \"{{ opinionated.hosting_network.workload.vip_ip_range }}\"\n  se_vip_network_gateway: \"{{ opinionated.hosting_network.workload.cidr | ipmath(1) }}\"\n</code></pre> <p>In this example you can see we refer to a distributed port group named <code>TKGM</code> with cidr <code>172.20.12.0/22</code>, as mentioned in Nested Lab network example.</p> <p>We can use this config file to deploy a TKGm nested lab environment by running the Ansible playbook as explained here.</p>"},{"location":"homelab/airgapped-tkgs/","title":"Set up an airgapped TKGS homelab environment","text":"<p>We will create a fully airgapped TKGS (aka vSphere with Tanzu) environment. This is a common setup of lots of our customers who run vSphere with Tanzu without any internet connection, not even via a proxy.</p> <p>As described in Nested Lab Setup we will use vmware-lab-builder to bootstrap nested lab environments. This Ansible playbook is not (yet) implemented to work in airgapped environments as it creates a subscribed content library. Hence, we will first run <code>vmware-lab-builder</code> with internet access to bootstrap a TKGS Supervisor Cluster and afterwards, we create a Local Content Library to provision guest clusters without internet access.</p>"},{"location":"homelab/airgapped-tkgs/#initial-setup-with-internet-access","title":"Initial Setup with internet access","text":"<p>As mentioned in the lab network setup we use a virtualized VyOS router to route all traffic in our homelab. For the TKGS with NSX-T environment we configure the following interface in VyOS</p> <pre><code>set interfaces ethernet eth1 vif 16 address 172.20.16.1/22\nset interfaces ethernet eth1 vif 16 description tanzu-without-dhcp\nset interfaces ethernet eth1 vif 16 mtu 9000\nset interfaces ethernet eth1 vif 20 address 172.20.20.1/22\nset interfaces ethernet eth1 vif 20 description nsxt-tep\nset interfaces ethernet eth1 vif 20 mtu 9000\ncommit \nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show interfaces\n ethernet eth0 {\n     address 192.168.178.101/24\n     hw-id 00:0c:29:85:a5:3b\n     offload {\n         gro\n         gso\n         sg\n         tso\n     }\n }\n ethernet eth1 {\n     hw-id 00:0c:29:85:a5:45\n     mtu 9000\n     vif 16 {\n         address 172.20.16.1/22\n         description tanzu-without-dhcp\n         mtu 9000\n     }\n     vif 20 {\n         address 172.20.20.1/22\n         description nsxt-tep\n         mtu 9000\n     }\n }\n loopback lo {\n }\n</code></pre> <p>where <code>vif 16</code> is used for all Virtual Machines and <code>vif 20</code> for the NSX-T TEP network infrastructure.</p> <p>For the Supervisor Management Network, for the Egress and Ingress range, we create a VyOS protocol (static route)</p> <pre><code>set protocols static route 172.30.4.0/24 next-hop 172.20.16.103\ncommit\nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show protocols\n static {\n     route 172.30.4.0/24 {\n         next-hop 172.20.16.103 {\n         }\n     }\n }\n</code></pre> <p>here, <code>172.20.16.103</code> will be the Tier-0 Gateway sitting on <code>vif 16</code>.</p> <p>My <code>vmware-lab-builder</code> config looks like this:</p> <pre><code>---\n# SOFTWARE_DIR must contain all required software\nvc_iso: \"{{ lookup('env', 'SOFTWARE_DIR') }}/VMware-VCSA-all-7.0.3-19717403.iso\"\nesxi_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/Nested_ESXi7.0u3c_Appliance_Template_v1.ova\"\nnsxt_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/nsx-unified-appliance-4.1.2.1.0.22667794.ova\"\n\nenvironment_tag: \"tkgs-nsxt\"  # Used to prepend object names in hosting vCenter\ndns_server: \"192.168.178.1\"\ndns_domain: \"home.local\"\nntp_server_ip: \"192.168.178.1\"  # Must be set to an IP address!\ndisk_mode: thin  # How all disks should be deployed\nnested_host_password: \"{{ opinionated.master_password }}\"\n\nhosting_vcenter:  # This is the vCenter which will be the target for nested vCenters and ESXi hosts\n  ip: \"192.168.178.102\"\n  username: \"{{ lookup('env', 'PARENT_VCENTER_USERNAME') }}\"\n  password: \"{{ lookup('env', 'PARENT_VCENTER_PASSWORD') }}\"\n  datacenter: \"Home\"  # Target for all VM deployment\n\n# This section describes what will be created\nopinionated:\n  master_password: \"VMware1!\"\n  nested_hosts:\n    cpu_cores: 12  # CPU count per nested host\n    ram_in_gb: 128  # memory per nested host\n    local_disks:\n      - size_gb: 500\n        datastore_prefix: \"datastore\"\n  hosting_cluster: Physical\n  hosting_datastore: NVME\n  hosting_network:\n    base:\n      port_group: tanzu-without-dhcp\n      cidr: \"172.20.16.0/22\"\n      gateway: \"172.20.16.1\"\n      # A NSX-T deployment requires 4 IPs, plus 1 per esxi host. They MUST be contiguous.\n      starting_addr: \"172.20.16.100\"\n    # nsxt tep pool will not be routed, but should not clash with routeable ranges\n    nsxt_tep:\n      port_group: nsxt-tep\n      vlan_id: 0\n      cidr: \"172.20.20.0/22\"  # Should be at least a 29 which supports up to 5 hosts and 1 edge\n  tanzu_vsphere:\n    # This network must be minimum of a /25\n    # 1/8 of the network will be used for the supervisor\n    # 3/8 of the network will be used for egress IP range\n    # 1/2 of the network will be used for ingress IP range\n    routeable_super_net: 172.30.4.0/24\n    # This network is private and should not overlap with any routable networks\n    internal_pod_network: 172.32.0.0/22\n    # This network is private and should not overlap with any routable networks\n    internal_kubernetes_services_network: 172.32.4.0/22\n\n#####################################################################\n### No need to edit below this line for an opinionated deployment ###\n#####################################################################\n\nnested_vcenter:  # the vCenter appliance that will be deployed\n  ip: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # vCenter ip address\n  mask: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n  gw: \"{{ opinionated.hosting_network.base.gateway }}\"\n  host_name: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # FQDN if there is working DNS server, otherwise put the ip as a name\n  username: \"administrator@vsphere.local\"\n  password: \"{{ opinionated.master_password }}\"\n  datacenter: \"Lab\"  # DC to create after deployment\n  # Below are properties of parent cluster\n  hosting_network: \"{{ opinionated.hosting_network.base.port_group }}\"  # Parent port group where the vCenter VM will be deployed\n  hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # Parent cluster where the vCenter VM will be deployed\n  hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Parent datastore where the vCenter VM will be deployed\n\nnested_clusters:  # You can add clusters in this section by duplicating the existing cluster\n  compute:  # This will be the name of the cluster in the nested  vCenter. Below are the minimum settings.\n    enable_drs: true\n    enable_ha: true\n    # Below are properties of the hosting cluster\n    hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # The nested ESXi VMs will be deployed here\n    hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Datastore target for nested ESXi VMs\n    # Settings below are assigned to each host in the cluster\n    vswitch0_vm_port_group_name: vm-network\n    vswitch0_vm_port_group_vlan: \"0\"\n    cpu_cores: \"{{ opinionated.nested_hosts.cpu_cores }}\"  # CPU count\n    ram_in_gb: \"{{ opinionated.nested_hosts.ram_in_gb }}\"  # memory\n    # In order list of disks to assign to the nested host. All will be marked as SSD.\n    # Datastore names will be automatically be pre-pended with the hostname. E.g esx1\n    # If the datastore_prefix property is removed the disk will not be set as a datastore\n    # To leave the default OVA disks in place, delete this section.\n    nested_hosts_disks: \"{{ opinionated.nested_hosts.local_disks | default(omit) }}\"\n    # Added in vmnic order, these port groups must exist on the physical host\n    # Must specify at least 2 port groups, up to a maximum of 10\n    vmnic_physical_portgroup_assignment:\n      - name: \"{{ opinionated.hosting_network.base.port_group }}\"\n      - name: \"{{ opinionated.hosting_network.nsxt_tep.port_group }}\"\n\nnested_hosts:\n  - name: \"esx1\"\n    ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(4) }}\"\n    mask: \"{{ opinionated.hosting_network.base.cidr | ansible.utils.ipaddr('netmask') }}\"\n    gw: \"{{ opinionated.hosting_network.base.gateway }}\"\n    nested_cluster: compute\n\ndistributed_switches:  # (optional) - section can be removed to not create any distributed switches\n  - vds_name: nsxt-vds\n    mtu: 1600\n    vds_version: 7.0.0  # Should be 7.0.0, 6.7.0\n    clusters:  # distributed switch will be attached to all hosts in the clusters listed\n      - compute\n    uplink_quantity: 1\n    vmnics:\n      - vmnic1\n\n\ntspbm:  # Tag-based Storage Policy Based Management\n  tag_categories:\n    - category_name: tkgs-storage-category\n      description: \"TKGS tag category\"\n      tags:\n        - tag_name: tkgs-storage-tag\n          description: \"Tag for datastores used by TKGS\"\n  datastore_tags:\n    - datastore_name: \"{{ opinionated.nested_hosts.local_disks[0].datastore_prefix }}-esx1\"\n      tag_names:\n        - tkgs-storage-tag\n  vm_storage_policies:\n    - storage_policy_name: tkgs-storage-policy\n      description: \"TKGS storage performance policy\"\n      tag_name: tkgs-storage-tag\n      tag_category: tkgs-storage-category\n\ntanzu_vsphere:\n  services_cidr: \"{{ opinionated.tanzu_vsphere.internal_kubernetes_services_network }}\"  # This is private within each cluster\n  content_library_datastore: \"{{ opinionated.nested_hosts.local_disks[0].datastore_prefix }}-esx1\"\n  content_library_name: tkgs-library\n  content_library_url: \"http://wp-content.vmware.com/v2/latest/lib.json\"\n  default_content_library: tkgs-library\n  dns_server_list: [\"{{ dns_server }}\"]\n  ephemeral_storage_policy: \"{{ tspbm.vm_storage_policies[0].storage_policy_name }}\"\n  fluentbit_enabled: false\n  image_storage_policy: \"{{ tspbm.vm_storage_policies[0].storage_policy_name }}\"\n  ntp_server_list: [\"{{ ntp_server_ip }}\"]\n  management_dns_servers: [\"{{ dns_server }}\"]\n  management_port_group: supervisor-seg\n  management_gateway: \"{{ opinionated.tanzu_vsphere.routeable_super_net | ansible.utils.ipmath(1) }}\"\n  management_netmask: &gt;-\n    {{ opinionated.tanzu_vsphere.routeable_super_net |\n    ansible.utils.ipsubnet((opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int)+3, 0) |\n    ansible.utils.ipaddr('netmask') }}\n  management_starting_address: \"{{ opinionated.tanzu_vsphere.routeable_super_net | ansible.utils.ipmath(2) }}\"\n  master_storage_policy: \"{{ tspbm.vm_storage_policies[0].storage_policy_name }}\"\n  network_provider: NSXT_CONTAINER_PLUGIN\n  supervisor_size: tiny\n  vsphere_cluster: compute\n  workload_dns_servers: [\"{{ dns_server }}\"]\n\n  nsxt:\n    cluster_distributed_switch: \"{{ distributed_switches[0].vds_name }}\"\n    egress_cidrs:\n      - &gt;-\n        {{ opinionated.tanzu_vsphere.routeable_super_net |\n        ansible.utils.ipsubnet((opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int)+3, 1) }}\n      - &gt;-\n        {{ opinionated.tanzu_vsphere.routeable_super_net |\n        ansible.utils.ipsubnet((opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int)+2, 1) }}\n    ingress_cidrs:\n      - &gt;-\n        {{ opinionated.tanzu_vsphere.routeable_super_net |\n        ansible.utils.ipsubnet((opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int)+1, 1) }}\n    nsx_edge_cluster: \"{{ nsxt.edge_clusters[0].edge_cluster_name}}\"\n    pod_cidrs: \"{{ opinionated.tanzu_vsphere.internal_pod_network }}\"\n    # This is used by the task which checks if the supervisor network is online\n    t0_uplink_ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"\n\nnsxt:  # (optional) - section can be removed to not create any nsxt objects\n  manager:\n    hostname: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(1) }}\"\n    ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(1) }}\"\n    netmask: \"{{ opinionated.hosting_network.base.cidr | ansible.utils.ipaddr('netmask') }}\"\n    gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n    username: admin  # this cannot be changed\n    password: \"{{ opinionated.master_password }}{{ opinionated.master_password }}\"\n    hosting_vcenter_ip: \"{{ hosting_vcenter.ip }}\"\n    hosting_vcenter_username: \"{{ hosting_vcenter.username }}\"\n    hosting_vcenter_password: \"{{ hosting_vcenter.password }}\"\n    hosting_datacenter: \"{{ hosting_vcenter.datacenter }}\"\n    hosting_datastore: \"{{ opinionated.hosting_datastore }}\"\n    hosting_network: \"{{ opinionated.hosting_network.base.port_group }}\"\n    hosting_cluster: \"{{ opinionated.hosting_cluster }}\"\n    license_key: \"{{ lookup('env', 'NSXT_LICENSE_KEY') }}\"\n\n  # If the section below is defined, the playbook will wait for the IP to become pingable\n  # For TKG service deployments this is the default gateway of the supervisor network\n  routing_test:\n    ip_to_ping: \"{{ opinionated.tanzu_vsphere.routeable_super_net | ansible.utils.ipmath(1) }}\"\n    # The playbook will present a message using the params below\n    # A static route must be made to the router uplink for nsxt_supernet\n    router_uplink: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"\n    nsxt_supernet: \"{{ opinionated.tanzu_vsphere.routeable_super_net }}\"\n\n  policy_ip_pools:\n    - display_name: tep-pool  # This is a non-routable range which is used for the overlay tunnels.\n      pool_static_subnets:\n        - id: tep-pool-1\n          state: present\n          allocation_ranges:\n            - start: \"{{ opinionated.hosting_network.nsxt_tep.cidr | ansible.utils.ipmath(1) }}\"\n              end: \"{{ opinionated.hosting_network.nsxt_tep.cidr | ansible.utils.ipaddr('-2') | ansible.utils.ipaddr('address') }}\"\n          cidr: \"{{ opinionated.hosting_network.nsxt_tep.cidr }}\"\n          do_wait_till_create: true\n\n  uplink_profiles:\n    - display_name: host-tep-profile\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: \"{{ opinionated.hosting_network.nsxt_tep.vlan_id }}\"\n    - display_name: edge-tep-profile\n      mtu: 9000\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: \"{{ opinionated.hosting_network.nsxt_tep.vlan_id }}\"\n    - display_name: edge-uplink-profile\n      mtu: 1500\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: 0\n\n  transport_zones:\n    - display_name: tz-overlay\n      transport_type: OVERLAY\n      # host_switch_name: \"{{ distributed_switches[0].vds_name }}\"\n      nested_nsx: true  # Set this to true if you use NSX-T for your physical host networking\n      description: \"Overlay Transport Zone\"\n      # - display_name: nsx-vlan-transportzone\n      #   transport_type: VLAN\n      #   # host_switch_name: sw_vlan\n      #   description: \"Uplink Transport Zone\"\n\n  transport_node_profiles:\n    - display_name: tnp1\n      host_switches:\n        - host_switch_profiles:\n            - name: host-tep-profile\n              type: UplinkHostSwitchProfile\n          host_switch_name: \"{{ distributed_switches[0].vds_name }}\"\n          host_switch_type: VDS\n          host_switch_mode: STANDARD\n          ip_assignment_spec:\n            resource_type: StaticIpPoolSpec\n            ip_pool_name: \"tep-pool\"\n          transport_zone_endpoints:\n            - transport_zone_name: \"tz-overlay\"\n            - transport_zone_name: \"nsx-vlan-transportzone\"\n          uplinks:\n            - uplink_name: \"uplink-1\"\n              vds_uplink_name: \"Uplink 1\"\n      description: \"Cluster node profile\"\n\n  cluster_attach:\n    - display_name: \"tnc1\"\n      description: \"Transport Node Collections 1\"\n      compute_manager_name: \"vCenter\"\n      cluster_name: \"compute\"\n      transport_node_profile_name: \"tnp1\"\n\n  edge_nodes:\n    - display_name: edge-node-1\n      size: MEDIUM\n      mgmt_ip_address: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(2) }}\"\n      mgmt_prefix_length: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n      mgmt_default_gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n      network_management_name: vm-network\n      network_uplink_name: vm-network\n      network_tep_name: edge-tep-seg\n      datastore_name: datastore-esx1\n      cluster_name: compute\n      host_switches:\n        tep:\n          uplink_profile_name: edge-tep-profile\n          ip_assignment_spec:\n            resource_type: StaticIpPoolSpec\n            ip_pool_name: tep-pool\n          transport_zone_endpoints:\n            - transport_zone_name: \"tz-overlay\"\n        uplink:\n          host_switch_name: \"sw_vlan\"\n          uplink_profile_name: edge-uplink-profile\n          transport_zone_endpoints:\n            - transport_zone_name: \"nsx-vlan-transportzone\"\n      transport_zone_endpoints:\n        - transport_zone_name: \"tz-overlay-2\"\n        - transport_zone_name: \"nsx-vlan-transportzone\"\n\n  edge_clusters:\n    - edge_cluster_name: edge-cluster-1\n      edge_cluster_members:\n        - transport_node_name: edge-node-1\n\n  vlan_segments:\n    - display_name: t0-uplink\n      vlan_ids: [0]\n      transport_zone_display_name: nsx-vlan-transportzone\n    - display_name: edge-tep-seg\n      vlan_ids: [0]\n      transport_zone_display_name: nsx-vlan-transportzone\n\n  # For full spec see - https://github.com/laidbackware/ansible-for-nsxt/blob/vmware-lab-builder/library/nsxt_policy_tier0.py\n  tier_0:\n    display_name: \"tkgs-t0\"\n    ha_mode: ACTIVE_STANDBY\n    uplink_ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"\n    disable_firewall: true\n    static_routes:\n      - state: present\n        display_name: default-route\n        network: \"0.0.0.0/0\"\n        next_hops:\n          - ip_address: \"{{ opinionated.hosting_network.base.gateway }}\"\n    locale_services:\n      - state: present\n        display_name: \"tkgs-t0-ls\"\n        edge_cluster_info:\n          edge_cluster_display_name: edge-cluster-1\n        interfaces:\n          - display_name: \"test-t0-t0ls-iface\"\n            state: present\n            subnets:\n              - ip_addresses: [\"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"]\n                prefix_len: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] | int }}\"\n            segment_id: t0-uplink\n            edge_node_info:\n              edge_cluster_display_name: edge-cluster-1\n              edge_node_display_name: edge-node-1\n            mtu: 1500\n\n  overlay_segments:\n    - display_name: supervisor-seg\n      transport_zone_display_name: tz-overlay\n      tier1_display_name: supervisor-t1\n      subnets:\n        - gateway_address: &gt;-\n            {{ opinionated.tanzu_vsphere.routeable_super_net |\n            ansible.utils.ipmath(1) }}/{{ opinionated.tanzu_vsphere.routeable_super_net.split('/')[1] |int +3 }}\n\n  tier_1_gateways:\n    - display_name: supervisor-t1\n      route_advertisement_types:\n        - \"TIER1_CONNECTED\"\n      tier0_display_name: tkgs-t0\n</code></pre>"},{"location":"homelab/airgapped-tkgs/#restrict-internet-access-in-vyos-interfaces","title":"Restrict internet access in VyOS interfaces","text":"<p>We will create an environment that doesn't have outbound internet connection but ingress connection from the internet is allowed and also all traffic within the network is allowed. We don't restrict internal communications on a per-port level.</p> <ol> <li> <p>Create a <code>Firewall address-group</code> called <code>ALLOWED-IPS</code> that will collect all IP ranges to and from which communication is allowed using the command</p> <pre><code>set firewall group address-group ALLOWED-IPS address 172.20.16.1-172.20.16.255\n</code></pre> <p>Do this for every IP range you are using (don't forget the internal Kubernetes pod and service range). We get the following result:</p> <pre><code>vyos@vyos# show firewall group\naddress-group ALLOWED-IPS {\n    address 192.168.178.1-192.168.178.255\n    address 172.20.16.1-172.20.16.255\n    address 172.20.17.1-172.20.17.255\n    address 172.20.18.1-172.20.18.255\n    address 172.20.19.1-172.20.19.255\n    address 172.20.20.1-172.20.20.255\n    address 172.20.21.1-172.20.21.255\n    address 172.20.22.1-172.20.22.255\n    address 172.20.23.1-172.20.23.255\n    address 172.30.4.1-172.30.4.255\n    address 172.32.0.1-172.32.0.255\n    address 172.32.1.1-172.32.1.255\n    address 172.32.2.1-172.32.2.255\n    address 172.32.3.1-172.32.3.255\n    address 172.32.4.1-172.32.4.255\n    address 172.32.5.1-172.32.5.255\n    address 172.32.6.1-172.32.6.255\n    address 172.32.7.1-172.32.7.255\n}\n</code></pre> </li> <li> <p>Create a firewall rule to allow inbound traffic from everywhere</p> <pre><code>set firewall name INBOUND-ALL default-action drop\nset firewall name INBOUND-ALL description \"Allow all incoming connections\"\nset firewall name INBOUND-ALL enable-default-log\nset firewall name INBOUND-ALL rule 20 action accept\nset firewall name INBOUND-ALL rule 20 log enable\nset firewall name INBOUND-ALL rule 20 source address 0.0.0.0/0\ncommit \nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show firewall name INBOUND-ALL\ndefault-action drop\ndescription \"Allow all incoming connections\"\nenable-default-log\nrule 20 {\n    action accept\n    log enable\n    source {\n        address 0.0.0.0/0\n    }\n}\n</code></pre> </li> <li> <p>Create a firewall rule for outbound communication</p> <pre><code>set firewall name OUTBOUND-RESTRICT default-action 'drop'\nset firewall name OUTBOUND-RESTRICT description 'Restrict outbound connections'\nset firewall name OUTBOUND-RESTRICT enable-default-log\nset firewall name OUTBOUND-RESTRICT rule 10 action 'accept'\nset firewall name OUTBOUND-RESTRICT rule 10 description 'Allow DNS'\nset firewall name OUTBOUND-RESTRICT rule 10 destination port '53'\nset firewall name OUTBOUND-RESTRICT rule 10 protocol 'tcp_udp'\nset firewall name OUTBOUND-RESTRICT rule 20 action 'accept'\nset firewall name OUTBOUND-RESTRICT rule 20 description 'Allow SSH'\nset firewall name OUTBOUND-RESTRICT rule 20 destination port '22'\nset firewall name OUTBOUND-RESTRICT rule 20 log 'enable'\nset firewall name OUTBOUND-RESTRICT rule 20 protocol 'tcp'\nset firewall name OUTBOUND-RESTRICT rule 30 action 'accept'\nset firewall name OUTBOUND-RESTRICT rule 30 description 'Allow specific outbound traffic'\nset firewall name OUTBOUND-RESTRICT rule 30 log 'enable'\nset firewall name OUTBOUND-RESTRICT rule 30 protocol 'all'\nset firewall name OUTBOUND-RESTRICT rule 30 source group address-group 'ALLOWED-IPS'\ncommit \nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show firewall name OUTBOUND-RESTRICT\ndefault-action drop\ndescription \"Restrict outbound connections\"\nenable-default-log\nrule 10 {\n    action accept\n    description \"Allow DNS\"\n    destination {\n        port 53\n    }\n    protocol tcp_udp\n}\nrule 20 {\n    action accept\n    description \"Allow SSH\"\n    destination {\n        port 22\n    }\n    log enable\n    protocol tcp\n}\nrule 30 {\n    action accept\n    description \"Allow specific outbound traffic\"\n    log enable\n    protocol all\n    source {\n        group {\n            address-group ALLOWED-IPS\n        }\n    }\n}\n</code></pre> </li> <li> <p>Finally, apply the firewall rules on the respective interfaces:</p> <pre><code>set interfaces ethernet eth1 vif 20 firewall in name INBOUND-ALL\nset interfaces ethernet eth1 vif 20 firewall out name OUTBOUND-RESTRICT\nset interfaces ethernet eth1 vif 16 firewall in name INBOUND-ALL\nset interfaces ethernet eth1 vif 16 firewall out name OUTBOUND-RESTRICT\ncommit\nsave\n</code></pre> <p>The result is:</p> <pre><code>vyos@vyos# show interfaces ethernet eth1\nhw-id 00:0c:29:85:a5:45\nmtu 9000\nvif 16 {\n    address 172.20.16.1/22\n    description tanzu-without-dhcp\n    firewall {\n        in {\n            name INBOUND-ALL\n        }\n        out {\n            name OUTBOUND-RESTRICT\n        }\n    }\n    mtu 9000\n}\nvif 20 {\n    address 172.20.20.1/22\n    description nsxt-tep\n    firewall {\n        in {\n            name INBOUND-ALL\n        }\n        out {\n            name OUTBOUND-RESTRICT\n        }\n    }\n    mtu 9000\n}\n</code></pre> </li> </ol>"},{"location":"kubernetes/architecture-concepts/","title":"Kubernetes Architecture Concepts","text":"<p>This page explains the basic concepts of the Kubernetes technical architecture which I find very important to better understand Kubernetes as a whole.</p>"},{"location":"kubernetes/architecture-concepts/#goal-of-this-page","title":"Goal of this page","text":"<p>Kubernetes has evolved from just being a container scheduling and management system. It can be used as a generic \"platform API\" - a standardized API for an entire platform consisting of not only containers, but also virtual machines, databases and services. The reason for this success is due to the well architected architecture in my opinion.</p> <p>This is the reason why I think it is very valuable to understand the basic technical concepts as it will help you better understand literally anything in Kubernetes.</p> <p>I try to go into technical details without going into technical details </p> <p>We will cover:</p> <ul> <li>what actually happens when I create a Kubernetes deployment?</li> <li>Kubernetes Reconciliation</li> <li>Kubernetes Admission Webhooks - Mutating and Validating</li> <li>Custom Resource Definitions (CRDs)</li> </ul>"},{"location":"kubernetes/architecture-concepts/#what-happens-when-i-create-a-kubernetes-deployment","title":"What happens when I create a Kubernetes deployment?","text":"<p>In order to deploy a simple <code>nginx</code> deployment with 3 replicas, we create a file <code>nginx-deployment.yaml</code>:</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre> <p>and apply it on a Kubernetes cluster with <code>kubectl apply -f nginx-deployment.yaml</code>, which will ultimately run 3 pods of nginx. Actually, a deployment does not create 3 pods but the deployment creates a replicaset, and the replicaset will run the 3 pods.</p> <p>But how does that work?</p> <p>Let's look into the simplified Kubernetes API request:</p> <p></p> <p>When executing <code>kubectl apply -f nginx-deployment.yaml</code>, multiple things happen which we will divide into 3 steps:</p>"},{"location":"kubernetes/architecture-concepts/#store-deployment-manifest-in-etcd","title":"Store Deployment Manifest in etcd","text":"<ol> <li>we hit the Kubernetes API Server, often running as a pod <code>kube-apiserver</code> itself on the Kubernetes cluster</li> <li>the <code>API HTTP Handler</code> takes the incoming request and forwards it to <code>Authentication</code> &amp; <code>Authorization</code> - Kubernetes Role Based Access Control (RBAC). If Kubernetes RBAC denies the request, the API server responds with a <code>permission denied</code> error and stops the request from being continued to the subsequent steps.</li> <li> <p>If RBAC approves, the request will be handled by further steps as explained below and end up in <code>Object Schema Validation</code>. This step validates if the request is a valid yaml/json and also validates if all fields are correct. For instance, if you have a typo in your Deployment's spec, e.g. in <code>replicas</code>:</p> <pre><code>[...]\nspec:\n  replica: 3\n  selector:\n    matchLabels:\n      app: nginx\n[...]\n</code></pre> <p>where we missed the \"s\" in <code>replicas</code>, this step will respond with </p> <pre><code>Error from server (BadRequest): error when creating \"nginx-deployment.yaml\": Deployment in version \"v1\" cannot be handled as a Deployment: strict decoding error: unknown field \"spec.replica\"\n</code></pre> </li> <li> <p>If we have a valid yaml and the syntax is correct, our <code>nginx-deployment.yaml</code> will be persisted in etcd, the distributed key-value store in Kubernetes.</p> </li> </ol> <p>Note</p> <p>The Kubernetes deployment manifest is now stored in etcd. There is no running pod yet though! At this point, <code>kubectl apply</code> completed its job and returns with <code>deployment.apps/nginx created</code>, essentially a <code>200 OK</code>. All subsequent steps necessary to ultimately run a pod are handled asynchronously.</p>"},{"location":"kubernetes/architecture-concepts/#etcd-watch-api-and-controllers","title":"etcd Watch API and Controllers","text":"<p>As soon as the deployment manifest is stored in etcd, a key feature of etcd kicks in, the etcd Watch API which provides an event-based interface for asynchronously monitoring changes to keys. An etcd watch waits for changes to keys by continuously watching from a given revision, either current or historical, and streams key updates back to the client. This API is heavily used by Kubernetes.</p> <p>Let's look at our example:</p> <p></p> <p>There is a <code>Deployment controller</code> watching for changes of <code>kind: Deployment</code> in etcd and then determining whether it's a <code>Create</code>, <code>Update</code> or <code>Delete</code> event. In our example, it's a <code>Create</code> event and thus, the Deployment Controller will create a Replicaset which will look similar to this</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  [...]\n  labels:\n    app: nginx\n  name: nginx-bf5d5cf98\n  namespace: default\n  [...]\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        [...]\n</code></pre> <p>The Deployment Controller applies this manifest to the API Server the same way as the user applied the deployment manifest. Hence, it will first hit the <code>API HTTP Handler</code>, <code>Authentication / Authorization (RBAC)</code>, <code>Object Schema Validation</code> and after some other steps be persisted in etcd.</p> <p>This update in etcd triggers another controller, the <code>Replicaset Controller</code> and the entire process starts over again:</p> <p></p> <p>The Replicaset Controller ultimately persists the pod manifest in etcd.</p>"},{"location":"kubernetes/architecture-concepts/#start-the-containers","title":"Start the containers","text":"<p>Once a pod manifest is stored in etcd, the same mechanisms apply:</p> <ol> <li>the kube-scheduler watches etcd for pod events and based on the <code>Create</code> event, it assigns the pod to a suitable node in the cluster based on resource availability and other constraints. It also updates the pod's status in etcd to reflect its node assignment.</li> <li>The kubelet on the assigned node also watches etcd for pod events and if there is a node assignment in the pod's spec, it will pull the necessary container images, start the containers and set up networking.</li> <li>The container runtime which is installed on the cluster will ultimately run the containers.</li> </ol>"},{"location":"kubernetes/architecture-concepts/#reconciliation","title":"Reconciliation","text":"<p>We've seen the <code>Deployment Controller</code> and <code>Replicaset Controller</code> how they watch etcd and react on specific changes in etcd. We've also seen that other processes, like <code>kube-scheduler</code> and <code>kubelet</code> work in a similar way in that they watch etcd for changes. All these so-called controllers do not only create/update/delete other resources but also report back the current status.</p> <p>(Almost) Every resource in Kubernetes has the following similar structure:</p> <pre><code>---\napiVersion: [APIGroup]/[APIGroupVersion]\nkind: [KIND]\nmetadata:\n  [...]\nspec:\n  [...]\nstatus:\n  [...]\n</code></pre> <p>where</p> <ul> <li><code>spec</code> is what get's created/updated from one controller</li> <li><code>status</code> is where another controller reports back the current status</li> </ul> <p>We call the <code>spec</code> the desired state, and the <code>status</code> the actual state.</p> <p>Let's look at our example:</p> <ol> <li>when the end user (e.g. a developer) creates a deployment manifest and applies it to the API Server to store it in etcd, then the user applies the desired state as described in the <code>spec</code></li> <li>when a deployment controller gets triggered on this event, it</li> <li>creates a replicaset manifest by applying the desired state in the spec and applies it to the API Server to store it in etcd</li> <li>updates the deployment manifest, which has been initially created by the end user, by updating the <code>status</code> block with information gathered from the created replicaset</li> <li>when the replicaset controller gets triggered on the replicaset event in etcd, it</li> <li>creates one or more pod manifests by applying the desired state in the spec and applies it to the API Server to store it in etcd</li> <li>updates the replicaset manifest, which has been initially created by the Deployment Controller, by updating the <code>status</code> block with information gathered from the pods</li> <li>this process goes on in the same way for all other controllers</li> </ol> <p>The Deployment Controller and Replicaset Controller are built-in upstream Kubernetes controllers which are, amongst several other controllers, bundled in kube-controller-manager.</p> <p>The previously described process can be summarized in this picture:</p> <p></p> <p>This process of constantly watching the desired state and syncing with the actual state is called Reconciliation - controllers reconcile the desired state with the actual state.</p> <p>It is important to know who is the owner of a resource in order to determine the desired state and what should be reconciled: In our example, the replicaset manifest stored in etcd is the desired state for the Replicaset Controller. But updating the replicaset manifest, e.g. with <code>kubectl edit replicaset</code>, does not update the pods - although this is the replicaset controller's job. The replicaset manifest is owned by the Deployment Controller, whose desired state is stored in the deployment manifest. Hence, the manual changes in the replicaset manifest will be reverted back to what's desired in the deployment manifest. Resource owners are referenced in the corresponding resource:</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  [...]\n  name: nginx-bf5d5cf98\n  namespace: default\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: true\n    controller: true\n    kind: Deployment\n    name: nginx\n    uid: 7063121f-1e39-4b03-96c1-d14edf24713d\n  [...]\n</code></pre>"},{"location":"kubernetes/architecture-concepts/#admission-controllers","title":"Admission Controllers","text":"<p>We've looked into a simplified Kubernetes API request flow when we explored what happens when we create a Kubernetes deployment where we haven't covered two steps - <code>Mutating Admission</code> and <code>Validating Admission</code>:</p> <p></p> <p>From the Kubernetes docs:</p> <p>An admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized.</p> <p>Admission controllers may be validating, mutating, or both. Mutating controllers may modify objects related to the requests they admit; validating controllers may not.</p> <p>Admission controllers limit requests to create, delete, modify objects. Admission controllers can also block custom verbs, such as a request connect to a Pod via an API server proxy. Admission controllers do not (and cannot) block requests to read (get, watch or list) objects.</p> <p>Simply put, the mutating admission step will alter your manifest and the validating admission will allow or deny your request.</p>"},{"location":"kubernetes/architecture-concepts/#admission-controller-example","title":"Admission Controller Example","text":"<p>Let's look into our example and let's assume there is a AddLabel mutating admission controller implemented that injects a label <code>team: &lt;TeamName&gt;</code> to every request's manifest and there is a RequiredLabel validating admission controller implemented that expects a <code>cost-center: &lt;CostCenterID&gt;</code> label on every manifest.</p> <p>When we create a new deployment using</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre> <p>then the mutating admission controller <code>AddLabel</code> will inject the label <code>team: AwesomeTeam</code>, so the request becomes</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n    team: AwesomeTeam\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre> <p>The request then passes the <code>Object Schema Validation</code> step as there is no syntax error, but the request is denied on the <code>Validating Admission</code> step because the label <code>cost-center</code> is missing.</p> <p>When using</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n    cost-center: 12345\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre> <p>the request passes the API workflow and gets stored in etcd as</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: nginx\n    cost-center: 12345\n    team: AwesomeTeam\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        resources: {}\n</code></pre>"},{"location":"kubernetes/architecture-concepts/#built-in-admission-controllers","title":"Built-In Admission Controllers","text":"<p>There are various admission controllers compiled into the <code>kube-apiserver</code> binary which Kubernetes Administrators can turn on and off with some default ones being turned on.</p>"},{"location":"kubernetes/architecture-concepts/#dynamic-admission-controllers","title":"Dynamic Admission Controllers","text":"<p>Kubernetes docs</p> <p>In addition to compiled-in admission plugins, admission plugins can be developed as extensions and run as webhooks configured at runtime. [...] Admission webhooks are HTTP callbacks that receive admission requests and do something with them.</p> <p>Simply put, the Kubernetes API exposes the Mutating Admission and Validating Admission interfaces so that you can write external custom software and extend those two api workflow steps.</p> <p>The example from above explains two possible custom implementations of a mutating and validating admission webhook.</p> <p>The most famous open source projects that implement both webhooks are Kyverno and Open Policy Agent Gatekeeper.</p> <p>You can read more about admission controllers on this blog post: A Guide to Kubernetes Admission Controllers.</p>"},{"location":"kubernetes/architecture-concepts/#extending-kubernetes","title":"Extending Kubernetes","text":""},{"location":"kubernetes/architecture-concepts/#custom-resources-and-custom-controllers","title":"Custom Resources and Custom Controllers","text":"<p>We have seen that Dynamic Admission Controllers allows to hook into the Kubernetes API and extend it with custom software.</p> <p>With the introduction of Custom Resources you can further extend Kubernetes by writing custom controllers and hook into the etcd Watch API the same way as it's done with the Deployment Controller or Replicaset Controller as explained above.</p> <p>A simple custom controller is kubewatch which basically looks for events like pod/deployment/confimap creation/update/deletion and send a notification to selected channels like slack, hipchat, mattermost or webhook:</p> <p></p>"},{"location":"kubernetes/architecture-concepts/#custom-resource-definitions-crds-and-operators","title":"Custom Resource Definitions (CRDs) and Operators","text":"<p>Custom Controllers can be implemented to use the etcd Watch API and watch for built-in Kubernetes resources, such as deployments, services or pods, as described in the previous section. This approach can be further extended by implementing own resources, and not only relying on built-in resources.</p> <p>Let's look at a simple example: In order to deploy a web application we probably need a <code>deployment</code> to deploy the application in pods, a <code>service</code> to make the application available to end users, a <code>configmap</code> to store application configuration and a <code>secret</code> to store application secrets. Let's assume, we work for <code>mycompany</code> and we have implemented an app called <code>shopping-cart</code> which uses an external Postgres database and uses S3 for storing files. For this, we could introduce a custom resource called <code>WebApp</code> which would look like:</p> <pre><code>---\napiVersion: apps.com.mycompany/v1\nkind: WebApp\nmetadata:\n  labels:\n    app: shopping-cart\n  name: shopping-cart\nspec:\n  replicas: 3\n  image: registry.mycompany.com/shopping-cart/shopping-cart:v1.0.0\n  config:\n    env: prod\n    postgresURL: postgres.mycompany.com:5432\n    s3URL: \n    s3Bucket: shopping-cart\n  secret:\n    postgresUser: pg\n    postGresPasswort: sup\u20acrs3cure!\n    s3AccessKeyID: JWQWDBWM2\n    s3SecretAccessKey: nTqfIa4AvynIEWG7cTmY\n</code></pre> <p>Tip</p> <p>Never store secrets in plaintext! This is just an example, so please forgive me \ud83d\ude09</p> <p>We can then apply this <code>WebApp</code> onto our cluster</p> <pre><code>kubectl apply -f shopping-cart-webapp.yaml\n</code></pre> <p>which will ultimately create a <code>deployment</code>, <code>service</code>, <code>configmap</code> and a <code>secret</code>.</p> <p>Note</p> <p>This is just a simple example. This can be further extended to abstract away required logic from developers. So we can think of the <code>WebApp</code> being a custom resource owned by platform admins who can implement all required details to standardize web application deployments  within the company. This could include implementing security requirements and other best practices whilst developers can focus on their application code.</p> <p>How can this be implemented?</p> <p>The <code>WebApp</code> is a Custom Resource Definition (CRD) - a custom API registered in the Kubernetes API. To make that work technically, you have to describe and register the <code>WebApp</code> API by creating a CRD:</p> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: webapps.apps.com.mycompany\nspec:\n  group: apps.com.mycompany\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                replicas:\n                  type: string\n                image:\n                  type: string\n                config:\n                  type: object\n                  properties:\n                    env:\n                      type: string\n                    postgresURL:\n                      type: string\n                    s3URL:\n                      type: string\n                    s3Bucket:\n                      type: string\n                secret:\n                  type: object\n                  properties:\n                    postgresURL:\n                      type: string\n                    postGresPasswort:\n                      type: string\n                    s3AccessKeyID:\n                      type: string\n                    s3SecretAccessKey:\n                      type: string\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;\n    plural: webapps\n    # singular name to be used as an alias on the CLI and for display\n    singular: webapp\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: WebApp\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n    - wa\n</code></pre> <p>which we can simply register in Kubernetes with</p> <pre><code>kubectl apply -f webapp-crd.yaml\n</code></pre> <p>Afterwards we can already execute</p> <pre><code>kubectl get webapp\n# or using the short name\nkubectl get wa\n# and because it's namespaced\nkubectl get wa --all-namespaces\n</code></pre> <p>we can actually also already apply our manifest from above. But this will solely store the manifest in etcd (after it passes all API workflow steps as described in Store Deployment Manifest in etcd). Until now, there is no controller that watches etcd for resources of <code>kind: WebApp</code>. Therefore, the next step is to implement such a custom controller - custom controllers that watch Custom Resource Definitions are called Operators.</p>"},{"location":"kubernetes/architecture-concepts/#summary","title":"Summary","text":"<p>The Kubernetes project started in 6th June 2014 to become a Production-Grade Container Scheduling and Management system and has since evolved to be way more than that - with the possibility of extending the Kubernetes API with admission controllers and Kubernetes itself with Custom Controller and Operator, Kubernetes can be used as a standardized Platform API. All these implementation patterns build on top of the paradigm of an asynchronous event based architecture with etcd and the controller pattern at the heart of it. This is the main reason for success of the Kubernetes project in my opinion.</p>"},{"location":"kubernetes/architecture-concepts/#further-interesting-resources","title":"Further interesting resources","text":"<ul> <li>What happens when ... Kubernetes edition!</li> <li>OperatorHub.io</li> <li>How does the Kubernetes scheduler work?</li> </ul>"},{"location":"misc/deploy-ubuntu-on-vcenter/","title":"Deploy Ubuntu on vCenter","text":"<p>See this gist how to deploy a Ubuntu VM on vCenter with a public SSH key distributed and a static IP address.</p>"},{"location":"misc/nsx-rest-api/","title":"NSX REST API","text":""},{"location":"misc/nsx-rest-api/#postman-collection","title":"Postman Collection","text":"<ul> <li>NSX-T Manager API Collection</li> </ul>"},{"location":"misc/nsx-rest-api/#delete-nsx-protected-objects","title":"Delete NSX Protected Objects","text":"<p>When deleting resources you might get an error similar to</p> <p>Error</p> <p>\"Principal 'admin' with role '[enterprise_admin]'attempts to delete or modify an object of type nsx$InternalLogicalPort it doesn't own.</p> <p>This can happen if objects have been created by another system using a Principal Identity. E.g. Tanzu Application Service (TAS) or Tanzu Kubernetes Grid Integrated Edition (TKGi) can integrate with NSX-T via the NSX Container Plugin (NCP) which creates NSX objects at runtime.</p> <p>If you still want to delete this object, use the header <code>X-Allow-Overwrite: true</code>.</p>"},{"location":"tanzu/cf/","title":"Introduction","text":"<p>Cloud Foundry is an open source platform for cloud-native application development which centers all around the so-called <code>cf push</code> experience.</p> <p>Commercial offerings of Cloud Foundry are provided by VMware with Tanzu Platform for Cloud Foundry (formerly known as Tanzu Application Service for VMs aka TAS).</p> <p>Info</p> <p>I am using the names Tanzu Platform for Cloud Foundry (TP4CF), Tanzu Application Service (TAS) and Cloud Foundry interchangeably on this page.</p>"},{"location":"tanzu/cf/using-cf/","title":"Using Cloud Foundry","text":"<p>I have built a lab environment for TAS using the Platform Automation Toolkit (Concourse) which is described here.</p>"},{"location":"tanzu/cf/using-cf/#managing-orgs-and-spaces-from-concourse","title":"Managing Orgs and Spaces from Concourse","text":"<p>To manage Cloud Foundry Orgs and Spaces you can use cf-mgmt that enables you to declaratively create orgs and spaces and manage owners, permissions, quotas and more using Concourse.</p> <p>You can find an example of this used in my lab setup mentioned above here.</p>"},{"location":"tanzu/platform-automation-toolkit/","title":"Automated TAS &amp; TKGI Foundation","text":"<p>In the next series of pages I will explain how to achieve an automated setup of the following target architecture:</p> <p></p> <p>This architecture is a common setup our customers have as well:</p> <ul> <li>a \"Management\" vCenter that host the Platform Automation Toolkit (Concourse) and all its dependencies. From there we automate all target platforms</li> <li>one or more environment-specific vCenters, e.g. a \"Sandbox\" environment. This vCenter hosts the infrastructure and platform used by the end applications. The entire platform setup should be automated from Management.</li> </ul> <p>I will provision this architecture in my homelab where I will deploy \"Management\" on my \"Management vCenter\", which is the vCenter also managing my physical host. The \"Sandbox\" environment will be a Nested Lab. This nested lab includes one nested ESXi host, a vCenter appliance and one NSX Manager.</p> <p>For automation, Ansible will be used to provision the Platform Automation Toolkit itself and the infrastructure of the Sandbox environment. Concourse will then be used to provision the Platform, i.e. the TAS &amp; TKGI foundation(s).</p> <p>To begin with, read the next page Install Concourse for Platform Automation.</p>"},{"location":"tanzu/platform-automation-toolkit/deploy-tas-tkgi-with-concourse/","title":"Deploy TAS and TKGI with Concourse","text":"<p>This page explains how to deploy TAS, TKGI and some related products on vSphere with NSX-T networking in an automated way using the Platform Automation Toolkit (Concourse).</p> <p>We will use Concourse Pipelines from my repo which is a fork of a public repo provided by Broadcom that is used in the official docs providing full pipeline and reference configurations.</p> <p>Info</p> <p>All steps below assume you have forked my Concourse Pipelines Repo to your own account.</p>"},{"location":"tanzu/platform-automation-toolkit/deploy-tas-tkgi-with-concourse/#prerequisites","title":"Prerequisites","text":"<ul> <li>Platform Automation Toolkit running: You can use this guide to install Concourse for Platform Automation on vSphere</li> <li>a vSphere + NSX-T environment. NSX-T must be preconfigured to meet the requirements to deploy TAS for VMs with NSX-T Networking and to deploy TKGI on vSphere with NSX. You can follow this guide to achieve this</li> <li>Concourse CLI (fly), Credhub CLI, OM CLI, BOSH CLI: best to install them with asdf</li> <li>the S3 buckets in MinIO have Versioning enabled: Buckets can be created using the automation of Installing Concourse for Platform Automation but you have to manually enable Versioning for the created buckets as the automation is not able to do that today.</li> </ul>"},{"location":"tanzu/platform-automation-toolkit/deploy-tas-tkgi-with-concourse/#download-products","title":"Download Products","text":"<p>The first Concourse pipeline will download all required products from Broadcom Support Portal and store it on the local MinIO S3 instance. The purpose is that you can then deploy all products without requiring internet access.</p> <ol> <li> <p>Login to Minio (http://&lt;MINIO-IP&gt;:9092) and create Access Keys - you'll need them later</p> </li> <li> <p>Retrieve the Tanzu API Token (aka Pivnet API Token) from Broadcom Support:</p> <ol> <li>Login to Broadcom Support Portal</li> <li>On the right navigation bar, click on Tanzu API Token, which will navigate you to the tanzu-token page</li> </ol> </li> <li> <p>If you haven't yet configured to git clone/pull/push from your Github via SSH, add a new SSH Key to your Github account</p> </li> <li>update the IP addresses in login_to_concourse script</li> <li> <p>login to Concourse (which will also log you in to Credhub)</p> <pre><code>source ./login_to_concourse\n</code></pre> <p>If the command tells you to sync fly, simply execute the provided command.</p> </li> <li> <p>Create all required entries in Credhub</p> <pre><code>credhub set -n /concourse/main/s3_secret_access_key -t password -w &lt;minio-secret-access-key&gt;\ncredhub set -n /concourse/main/s3_access_key_id -t password -w  &lt;minio-access-key&gt;\ncredhub set -n /concourse/main/s3_endpoint -t value -v http://&lt;MINIO-IP&gt;:9091 # port 9091 is the API port, which is different from the UI port 9092\ncredhub set -n /concourse/main/s3_pivnet_products_bucket -t value -v products\ncredhub set -n /concourse/main/pivnet_token -t password -w &lt;pivnet-api-token-from&gt;\ncredhub set -n /concourse/main/pipeline-git-repo-key -t ssh -p ~/.ssh/id_rsa # path to your private SSH key that you use to interact with Github\ncredhub set -n /concourse/main/pipeline-git-repo-uri -t value -v git@github.com:&lt;your Github Handle&gt;/concourse-pipelines.git\n</code></pre> </li> <li> <p>Set the Concourse Pipeline</p> <pre><code>./scripts/update-download-products-pipeline.sh\n</code></pre> </li> <li> <p>Unpause the pipeline</p> <pre><code>fly -t ci unpause-pipeline -p download-products\n</code></pre> </li> <li> <p>Navigate to the Concourse UI (https://) and follow the <code>download-products</code> pipeline.  <ol> <li>The <code>fetch-platform-automation</code> job starts automatically after a few seconds if the <code>platform-automation-pivnet</code> input resource has been checked successfully.</li> <li>all other jobs will not be triggered automatically: I have done this on purpose, because I don't want to stress my network and rather have control when the pipeline downloads and uploads lots of data. Hence, you need to trigger them manually. You can do this either on the UI or using the CLI.</li> </ol>"},{"location":"tanzu/platform-automation-toolkit/deploy-tas-tkgi-with-concourse/#deploy-foundation","title":"Deploy Foundation","text":"<p>The next Concourse pipeline <code>deploy-foundation</code> will be used to deploy TAS &amp; TKGI and other related products. This pipeline can ultimately be used for different environments by providing different variables files. We will deploy the <code>sandbox</code> environment. Adding other environments is self-explanatory.</p>"},{"location":"tanzu/platform-automation-toolkit/deploy-tas-tkgi-with-concourse/#adapt-variables-files","title":"Adapt Variables Files","text":"<p>Most of the required information can be retrieved from Deploy vSphere + NSX-T to be used for TAS and TKGI and Install Concourse for Platform Automation.</p> <p>All Variables files can be found here. </p> <p>Some notes to some variables that might be unclear:</p> <ul> <li> <p><code>nsx_ca_certificate</code> in <code>director.yml</code>: can be retrieved with </p> <pre><code>openssl s_client -connect &lt;nsx_address&gt;:443 -showcerts &lt;/dev/null\n</code></pre> </li> <li> <p><code>pks_ssl_certificate</code> &amp; <code>pks_ssl_private_key</code>: Assuming your TKGI API will be <code>api.tkgi.example.com</code>, self-signed SSL certificates can be generated with:</p> <pre><code>openssl req -x509 -nodes -days 730 -newkey rsa:2048 \\\n  -keyout api.tkgi.example.com.key \\\n  -out api.tkgi.example.com.crt \\\n  -subj \"/C=US/ST=California/L=CA/O=TKGi/CN=api.tkgi.example.com\" \\\n  -extensions SAN \\\n  -config &lt;(cat /etc/ssl/openssl.cnf \\\n    &lt;(printf \"\\n[SAN]\\nsubjectAltName=DNS:api.tkgi.example.com\"))\n</code></pre> </li> </ul>"},{"location":"tanzu/platform-automation-toolkit/deploy-tas-tkgi-with-concourse/#set-pipeline","title":"Set Pipeline","text":"<ol> <li> <p>Create required entries in Credhub</p> <pre><code>credhub set -n /concourse/main/s3_installation_bucket -t value -v installation\ncredhub set -n /concourse/main/s3_foundation_state_bucket -t value -v foundation-state\ncredhub set -n /concourse/main/nsx_credentials -t user -z admin -w 'VMware1!VMware1!'\ncredhub set -n /concourse/main/opsman_decryption_passphrase -t password -w 'VMware1!VMware1!'\ncredhub set -n /concourse/main/opsman_user -t user -z admin -w 'VMware1!'\ncredhub set -n /concourse/main/vcenter_credentials -t user -z administrator@vsphere.local -w 'VMware1!'\n</code></pre> </li> <li> <p>Set the Concourse Pipeline</p> <pre><code>./scripts/update-sandbox-foundation-pipeline.sh\n</code></pre> </li> <li> <p>Unpause the pipeline</p> <pre><code>fly -t ci unpause-pipeline -p deploy-sandbox-foundation\n</code></pre> </li> <li> <p>Install Opsman and configure BOSH</p> <pre><code>fly -t ci trigger-job -j deploy-sandbox-foundation/install-opsman\n</code></pre> </li> <li> <p>Trigger all other pipelines to deploy all products either on the UI or using the CLI</p> </li> </ol>"},{"location":"tanzu/platform-automation-toolkit/deploy-tas-tkgi-with-concourse/#use-tas-and-tkgi","title":"Use TAS and TKGI","text":"<ul> <li>see this Getting Started with Cloud Foundry to get started with TAS.</li> <li>See Managing TKGI and Creating and Managing Kubernetes Clusters and Workloads to get started with TKGI.</li> <li>See Using Cloud Foundry</li> </ul>"},{"location":"tanzu/platform-automation-toolkit/deploy-vsphere-with-nsxt-for-tas-tkgi/","title":"Deploy vSphere + NSX-T for TAS and TKGI","text":"<p>This page explains how to deploy a Nested Lab with a \"naked\" vCenter plus NSX Manager pre-configured that can be used to Deploy TAS and TKGI with Concourse.</p>"},{"location":"tanzu/platform-automation-toolkit/deploy-vsphere-with-nsxt-for-tas-tkgi/#configure-routing-for-nested-lab","title":"Configure Routing for Nested Lab","text":"<p>Read about my routing configuration in my homelab page. As it is a NSX backed environment where my Tier0 will have the IP <code>172.20.16.13</code>, I have the following static routes:</p> <ul> <li><code>route 172.30.5.0/24 next-hop 172.20.16.13</code></li> <li><code>route 172.30.6.0/26 next-hop 172.20.16.13</code></li> </ul> <p>Additionally I will deploy one TAS Isolation Segment for which I create another static route:</p> <ul> <li><code>route 172.30.7.0/27 next-hop 172.20.16.13</code></li> </ul> <p>They all point to the same Tier0 which is shared between TAS and TKGI. This example can be easily expanded to have multiple Tier0 if desired.</p>"},{"location":"tanzu/platform-automation-toolkit/deploy-vsphere-with-nsxt-for-tas-tkgi/#deploy-nested-esxi-lab","title":"Deploy Nested ESXi Lab","text":"<p>We will use vmware-lab-builder to bootstrap a nested lab environment with</p> <ul> <li>a Nested ESXi host</li> <li>vCenter</li> <li>NSX Manager</li> </ul> <p>We pre-configure some NSX resources required for TAS &amp; TKGi (like a T0 Router, T1 Routers, IP Pools etc.) using the following opinionated vars yaml (see more info in how to deploy labs):</p> <pre><code>---\n# SOFTWARE_DIR must contain all required software\nvc_iso: \"{{ lookup('env', 'SOFTWARE_DIR') }}/VMware-VCSA-all-8.0.2-23319993.iso\"\nesxi_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/Nested_ESXi7.0u3c_Appliance_Template_v1.ova\"\nnsxt_ova: \"{{ lookup('env', 'SOFTWARE_DIR') }}/nsx-unified-appliance-4.2.0.0.0.24105821.ova\"\n\nenvironment_tag: \"tas-tkgi-nsx\"  # Used to prepend object names in hosting vCenter\ndns_server: \"192.168.178.1\"\ndns_domain: \"lab.knappster.de\"\nntp_server_ip: \"192.168.178.1\"  # Must be set to an IP address!\ndisk_mode: thin  # How all disks should be deployed\nnested_host_password: \"{{ opinionated.master_password }}\"\n\nhosting_vcenter:  # This is the vCenter which will be the target for nested vCenters and ESXi hosts\n  ip: \"192.168.178.102\"\n  username: \"{{ lookup('env', 'PARENT_VCENTER_USERNAME') }}\"\n  password: \"{{ lookup('env', 'PARENT_VCENTER_PASSWORD') }}\"\n  datacenter: \"Home\"  # Target for all VM deployment\n\n# This section is only referenced by other variables in this file\nopinionated:\n  master_password: \"VMware1!\"\n  number_of_hosts: 1  # number of ESXi VMs to deploy\n  nested_hosts:\n    cpu_cores: 16  # CPU count per nested host\n    ram_in_gb: 128  # memory per nested host\n    local_disks:  # (optional) this section can be removed to not modify local disks\n      - size_gb: 1000\n        datastore_prefix: \"datastore\"  # omit this to not have a datastore created\n  hosting_cluster: Physical\n  hosting_datastore: NVME\n  hosting_network:\n    base:\n      port_group: tanzu-without-dhcp\n      cidr: \"172.20.16.0/22\"\n      gateway: \"172.20.16.1\"\n      # A NSX-T deployment requires 4 IPs, plus 1 per esxi host. They MUST be contiguous.\n      starting_addr: \"172.20.16.10\"\n    nsxt_tep:\n      port_group: nsxt-tep\n      vlan_id: 0\n      cidr: \"172.20.20.0/22\" # IP calculation depends on this being a /24\n  nsxt:\n    tier0_gateway_name: tas-tkgi-t0\n    tier1_gateways:\n      - display_name: tkgi-mgmt-t1\n        route_advertisement_types:\n          - \"TIER1_CONNECTED\"\n          - \"TIER1_NAT\"\n        segments:\n          - display_name: tkgi-mgmt-seg\n            default_gateway_cidr: \"172.30.6.1/26\"\n      - display_name: tas-t1\n        route_advertisement_types:\n          - \"TIER1_CONNECTED\"\n          - \"TIER1_NAT\"\n        segments:\n          - display_name: tas-seg\n            default_gateway_cidr: \"172.30.5.1/24\"\n          - display_name: tas-iso-seg-1\n            default_gateway_cidr: \"172.30.7.1/27\"\n\n  tas:\n    nsx_principal_identity:\n      public_key: |-\n        -----BEGIN CERTIFICATE-----\n        MIIDaDCCAlCgAwIBAgIUbz/tPuQrBt2uIAkFIP51c+TybecwDQYJKoZIhvcNAQEL\n        BQAwHzELMAkGA1UEBhMCVVMxEDAOBgNVBAoMB1Bpdm90YWwwHhcNMjQxMDAxMTE1\n        NjIxWhcNMjYxMDAyMTE1NjIxWjA3MQswCQYDVQQGEwJVUzEQMA4GA1UECgwHUGl2\n        b3RhbDEWMBQGA1UEAwwNMTcyLjIwLjE2LjEwMTCCASIwDQYJKoZIhvcNAQEBBQAD\n        ggEPADCCAQoCggEBAMcdwWmzvzKL14+4dzUSX7T/hBHbvKLdMBfqU7WOsm9wBpdd\n        rYNFtElSI56MTgyYA9U94PyX/+IkpFznTT56U77TuFgrRBZlmleqDa4MV3+W0s+J\n        UfurdRLzW2vyIJNlNK4urub3YQV6F002GUa7PqA8ILf55CKRhaDYq+9LLMUq7KFG\n        Myl/nuKTfuRrSiKL67A9NElcPnaZTeE8yv3Qs70mv22una13xYqjU9zOZbXMLUxf\n        nuFlsqjzm521uISEK5G9nSilsyMMTfc1WksjSG2qlbBwWCtKp94oaiFJNuk2Pwdz\n        8n4+jYDJpnOTC8JSA6QqCZZWkL6IIgqzYRG7ywcCAwEAAaOBgzCBgDAdBgNVHQ4E\n        FgQUqX1jSYfVxHz/LX96LaI3K8nZEEMwHwYDVR0jBBgwFoAUiXTpjmsD5sGj10LC\n        1V0oRXu2ERIwHQYDVR0lBBYwFAYIKwYBBQUHAwIGCCsGAQUFBwMBMA4GA1UdDwEB\n        /wQEAwIFoDAPBgNVHREECDAGhwSsFBBlMA0GCSqGSIb3DQEBCwUAA4IBAQCJIIik\n        nagwmntydYA/m3gHe5xPApysxjUDw6qSKHgNxzu9y7Pdnfh6XZf6puPFci4xH5QE\n        QvEI5t6GrzfuI0OLO89YRQyaK5w/KTRFNhQ0sqvXgETn5i9sxzrv7MPgMlrkPj7Y\n        KcO4Aav9Is2l62kCTpKw4z5L1Q7k8CaJhsjRXmYnrFLFynyXCaCa9oxlGacQltl1\n        VBn7PMM05rrR+s0fFgmKvgH9l0aM2A59qSeSYA1A3F05CZ7nuuKQTw2/mkNuLZKe\n        HUB6jbp935hbOcew6bTUUrJu6T1ek+DQnb4RlNaO5Rnm0xxGabS0Pc0LHw4DlJiH\n        rFAge4ATExlV0+iV\n        -----END CERTIFICATE-----\n      private_key: |-\n        -----BEGIN RSA PRIVATE KEY-----\n        MIIEpAIBAAKCAQEAxx3BabO/MovXj7h3NRJftP+EEdu8ot0wF+pTtY6yb3AGl12t\n        g0W0SVIjnoxODJgD1T3g/Jf/4iSkXOdNPnpTvtO4WCtEFmWaV6oNrgxXf5bSz4lR\n        +6t1EvNba/Igk2U0ri6u5vdhBXoXTTYZRrs+oDwgt/nkIpGFoNir70ssxSrsoUYz\n        KX+e4pN+5GtKIovrsD00SVw+dplN4TzK/dCzvSa/ba6drXfFiqNT3M5ltcwtTF+e\n        4WWyqPObnbW4hIQrkb2dKKWzIwxN9zVaSyNIbaqVsHBYK0qn3ihqIUk26TY/B3Py\n        fj6NgMmmc5MLwlIDpCoJllaQvogiCrNhEbvLBwIDAQABAoIBADwx4ilW5jvdK98u\n        iJc6RUW+I0qUz+u6i5IHTKQsDgSDbPKwpsZzOaQa2VrSlrvW7v211cD3IKvYoPnX\n        ETKMn6mmbun0toJA2A6dgcI2x/LyASwtmuPG+z8t49r32WJF682mnkiDy8hwlv/I\n        FY8dBztAwjFsMcxDiw7Lwfq3EsNOBImEpFCm5QWASYGADtW3su1mRy/nYGrRvfzL\n        nGDUVy7X2RHDd+h3JlXGXmkmTHpWEgjiHtbsiejWgjUP4UUtlERtsQ3hDGw8eo1W\n        o+YTglTZ5yJkVMGFs2wgMbYA+zrEYcqLoYqWegxqfIwnCf7m6//rXyLIFJ/QiU4r\n        3upPFW0CgYEA1qthNarOGlrHicC3Y8vsHw4hYYzH062bu+TX8uW32SkLgZ/rN1rZ\n        6PaAfDn7qd7035p+S9yhjumtVVUNWhVK8a/jAnE/lSLkI4Zb6WA5VF2FaSoSzIUV\n        fon6TSBkBENv2qhtgjQXUPy4FNjUwLpsPFm0k0yELHVizRcc/KAisE0CgYEA7XPK\n        DULnlX2kpsGShSerGnz/HH6dmZOUNRUcXFsLjSVxZOvKoHmjQj/k5WCeePogxlVo\n        dqyF2VenjqQp/vJPds4n+5khOxAimXl4xL9GIKLmg1g5d8CqC/fA9SibHjJXsinw\n        jjmgfI+WXcAKAKhVe6PAg4YqzkrFla1g0flQsqMCgYEAoU+RMcHTNGyo6rO9Wyme\n        mkuE/AfNFRytHQk+2RCUEYRNWC+ykhscCnpJXJA5s5GN0wUGCL2XTYv9K1VJPjsn\n        4Ou5m1k8XTYl1ygcowcirWnFWZw7GiKbX0YRp6lCXw3J3LaZ67B3IO126ntxjA3K\n        TaNfFRz3aW0gPFs09gTjbDUCgYEAhqfMJDsVs0u+DKbnXUWCnZHW5iTTYN01BelD\n        3QfwhAmAxZeFn/163L35Iy7oj3hhD7gtdmcdvIQdzCFCg4aME7aTK/XJx4G97UTa\n        fNBvh2B50nA8nrGOfRzxutVdKgGog6uO9EivvxN6VQ3rXjYXy/av3KZALh5u8BOT\n        PV/iKHsCgYBQ/nSZeh8DNOkUxpNmC5mh4ZEjJ1dX8NWzomA2K5tkMqC53vrJLJx/\n        T3GH0STE7JwsFJrwIepKV85BotO/no4OOUyWFcpmu7f+0msKbae8FroRXaN8a5YI\n        lwOMrTuT7Caol9by5h8CT0pZbvzEPCsWJE1npT4QyPDcDI+tya0nBA==\n        -----END RSA PRIVATE KEY-----\n  tkgi:\n    node_ip_block: 100.69.0.0/16\n    pod_ip_block: 100.70.0.0/16\n    floating_ip_pool: 172.30.6.64/26\n  # principal identity can be kept as default unless there is a security reason to change\n    nsx_principal_identity:\n      public_key: |-\n        -----BEGIN CERTIFICATE-----\n        MIIDADCCAeigAwIBAgIUJdyN7pMx9a5rFluqWIdBu05GGIcwDQYJKoZIhvcNAQEL\n        BQAwHjEcMBoGA1UEAwwTcGtzLW5zeC10LXN1cGVydXNlcjAeFw0yMzA0MDQxNDIy\n        NDlaFw0yNTA0MDMxNDIyNDlaMB4xHDAaBgNVBAMME3Brcy1uc3gtdC1zdXBlcnVz\n        ZXIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDXUJoCuRXNvfG24gCB\n        H367P3BjeCJ04utTCwgsj0fnk/MbUQagXidL/0azIoCvEJ6eA8n1SfD14kErnrOi\n        9TwOvy9E2MRoFYwpLcjv1oKm0SUsldFfEU0TWTIt2zJcintIWZkEzb63YK2gud0k\n        g/snCcggK1rH91SwUgi3qlXzIdsrxxaKyL5G/u2P9zUKIfYIq+S4K5hh6PNM/7Pd\n        BaSeu1Tv9AKTUrRMnporO1iopcZhcwX43YkSnSHcZqvOIOsi8cry3iaFgRbhxtEE\n        QCG0IMmtFWTFrtoNBhtjLCIQvje2pWCM5dRRHos+LN1IigtcoZAORAwtN+05z6+M\n        tZ7RAgMBAAGjNjA0MBMGA1UdJQQMMAoGCCsGAQUFBwMCMB0GA1UdDgQWBBTT5Fot\n        BYKTSO49UJHpIKZusO0ZezANBgkqhkiG9w0BAQsFAAOCAQEAw3afcTvhnbCwlzeY\n        cWOiRK+gJ3YWN+4pzY04y3A+mFhHDFZGy01eUuW1qPnV9HWyfB1GpnEtnu3aw89h\n        F0bBIJ9Tvx8bXCVnSIn/+Zea39/hOYqXHewVrennpMyCCiDu6lxadHKJVUCBDcqV\n        e9uUhEcPupNs6f8P8FflHf5jCQ95464d9sZ5caM19vfomg3bC7iLlo/Owo23nZlw\n        X2gpUu8hJFDbAeeW6Sx6ZbtVikTUFaiwTzMmSAt+xLeGmYqXNxUPDP8ORxAEcrKN\n        J9fWUWc/BhKzlXGqOf/EGjYs41OiAY/2ok1gODPZW/2nwPlFNrrq1Qv72jJ0GaWW\n        onRhDA==\n        -----END CERTIFICATE-----\n      private_key: |-\n        -----BEGIN PRIVATE KEY-----\n        MIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDXUJoCuRXNvfG2\n        4gCBH367P3BjeCJ04utTCwgsj0fnk/MbUQagXidL/0azIoCvEJ6eA8n1SfD14kEr\n        nrOi9TwOvy9E2MRoFYwpLcjv1oKm0SUsldFfEU0TWTIt2zJcintIWZkEzb63YK2g\n        ud0kg/snCcggK1rH91SwUgi3qlXzIdsrxxaKyL5G/u2P9zUKIfYIq+S4K5hh6PNM\n        /7PdBaSeu1Tv9AKTUrRMnporO1iopcZhcwX43YkSnSHcZqvOIOsi8cry3iaFgRbh\n        xtEEQCG0IMmtFWTFrtoNBhtjLCIQvje2pWCM5dRRHos+LN1IigtcoZAORAwtN+05\n        z6+MtZ7RAgMBAAECggEAAeLouS23mq9XfvODmfNVhWdyC8phfDusCx9gHsp8kJ3+\n        YGwOePjfiENqx6aoO6Bo0K1A0jTSdx0DLCd+Sbxd89UfTA+dcjn/bzGW/pRBsxtx\n        o26GleTNaOYm/I7ckJdSqtjExe2AMRQYQVMPicKG60R4gTZQBnYiGE9dAzBAri9x\n        d2CMUYv+u8MgIz4UcNlOGT2m9pvOC/CX5YbDKP7GX3xuJ3Zg/XHmjvuEyBfKEGx2\n        9D7KKI9d8ra+KMJVqAR8NzubVVz0VDALXpFumbL2jJQb1jccyDbSTx619n4kRnIO\n        GtJqLWjuSAuRFk+PV6Vb48gmaKWSo5uXpvqeat/cdwKBgQDxq5J8xlXz/rzPLUS4\n        fraFTlR+MnwmN65FQyqHPYaGI4o9/DJJO7DuuUbj7fNta2FDuzR+ZQZgZhiBNqiq\n        RYL0sL6AHSnduka5XSbXRCDiwl7nfO6i4+iy52VeKdacrhV3YVDydx6hfxGPHDPo\n        F8pMpYC2mizFYPvTu1httfISgwKBgQDkFPgGSvWZt5BpkofTJfNNtQcg2eiFhnXn\n        bVaKScVmvk7AWpZ/FkmQRc1gXGKszDp1Zx7EgUBhCiN8YgxK8bQT2aVBViIaqG7N\n        mkN1RUIisBIaOPFURt1T1XAaZJxLPFD30ufv1dKhoNm50/SBUp6DITnhpNjNcT/s\n        DvQSIm+5GwKBgGQUbUGG0SmOIJqbYI4Wy3dBDPSF66vX+y9rtTz0WbVLGoC45Ao3\n        0fnKeHUDoX96rHjkGcUOCSn6ncNE42xABQ9X8kwTx7au4YL59I/JAuVlIPA0aI7E\n        WyVbdjsckGeqH/GkN2VxtxmiCZ9+SnCfCYPcNgVoq4nBtAfm2aP1aR4JAoGAHZay\n        zm4vCnAL5gZCZJwJwkz3zcU3KwtUhF9k2K/VUgziPoYB/B6yEGtdx2B01KHx+4UT\n        Mr7p0Sz1iY9WtOpCSEj17VH1PqwXI8kdczs25zUcRBabCCnhUJzh3CqtM/1xK5VK\n        zYxZtOofFMJwd852DeDjl2hBT/WfK0qNU0TwZX0CgYAm+wjS++HJBv9f2oZM2PjF\n        GVM9Bp/uEiG1z/4O7Rpsv9qJHCgVaZ3q2Qebppq4D4o7ELLcV15djClmv7qkaR9B\n        ZauGoYXF2AwcdX8JO4bWHc985IwdAfZohGEqYSOIDkwgOoDXbrtK3LE/R8XtmJJk\n        MQTQCVfQMqNNIHcVkgWBNg==\n        -----END PRIVATE KEY-----\n\n#####################################################################\n### No need to edit below this line for an opinionated deployment ###\n#####################################################################\n\nnested_vcenter:  # the vCenter appliance that will be deployed\n  ip: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # vCenter ip address\n  mask: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n  gw: \"{{ opinionated.hosting_network.base.gateway }}\"\n  host_name: \"{{ opinionated.hosting_network.base.starting_addr }}\"  # FQDN if there is working DNS server, otherwise put the ip as a name\n  username: \"administrator@vsphere.local\"\n  password: \"{{ opinionated.master_password }}\"\n  datacenter: \"Lab\"  # DC to create after deployment\n  # Below are properties of parent cluster\n  hosting_network: \"{{ opinionated.hosting_network.base.port_group }}\"  # Parent port group where the vCenter VM will be deployed\n  hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # Parent cluster where the vCenter VM will be deployed\n  hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Parent datastore where the vCenter VM will be deployed\n\nnested_clusters:  # You can add clusters in this section by duplicating the existing cluster\n  compute:  # This will be the name of the cluster in the nested  vCenter. Below are the minimum settings.\n    enable_drs: true\n    # Below are properties of the hosting cluster\n    hosting_cluster: \"{{ opinionated.hosting_cluster }}\"  # The nested ESXi VMs will be deployed here\n    hosting_datastore: \"{{ opinionated.hosting_datastore }}\"  # Datastore target for nested ESXi VMs\n    # Settings below are assigned to each host in the cluster\n    vswitch0_vm_port_group_name: vm-network\n    vswitch0_vm_port_group_vlan: \"0\"\n    cpu_cores: \"{{ opinionated.nested_hosts.cpu_cores }}\"  # CPU count\n    ram_in_gb: \"{{ opinionated.nested_hosts.ram_in_gb }}\"  # memory\n    # In order list of disks to assign to the nested host. All will be marked as SSD.\n    # Datastore names will be automatically be pre-pended with the hostname. E.g esx1\n    # If the datastore_prefix property is removed the disk will not be set as a datastore\n    # To leave the default OVA disks in place, delete this section.\n    nested_hosts_disks: \"{{ opinionated.nested_hosts.local_disks | default(omit) }}\"\n    # Added in vmnic order, these port groups must exist on the physical host\n    # Must specify at least 2 port groups, up to a maximum of 10\n    vmnic_physical_portgroup_assignment:\n      - name: \"{{ opinionated.hosting_network.base.port_group }}\"\n      - name: \"{{ opinionated.hosting_network.nsxt_tep.port_group }}\"\n\nopinionated_host_ip_ofset: 4\n# See the custom example for host to build hosts out manually\nnested_hosts: &gt;-\n  [\n    {% for host_number in range(opinionated.number_of_hosts) %}\n    {\n      \"name\": \"esx{{ host_number + 1 }}\",\n      \"ip\": \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(opinionated_host_ip_ofset + host_number) }}\",\n      \"mask\": \"{{ opinionated.hosting_network.base.cidr | ansible.utils.ipaddr('netmask') }}\",\n      \"gw\": \"{{ opinionated.hosting_network.base.gateway }}\",\n      \"nested_cluster\": \"compute\"\n    },\n    {% endfor %}\n  ]\n\ndistributed_switches:  # (optional) - section can be removed to not create any distributed switches\n  - vds_name: nsxt-vds\n    mtu: 1600\n    vds_version: 7.0.0  # Should be 7.0.0, 6.7.0\n    clusters:  # distributed switch will be attached to all hosts in the clusters listed\n      - compute\n    uplink_quantity: 1\n    vmnics:\n      - vmnic1\n\nnsxt:  # (optional) - section can be removed to not create any nsxt objects\n  manager:\n    hostname: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(1) }}\"\n    ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(1) }}\"\n    netmask: \"{{ opinionated.hosting_network.base.cidr | ansible.utils.ipaddr('netmask') }}\"\n    gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n    username: admin  # this cannot be changed\n    password: \"{{ opinionated.master_password }}{{ opinionated.master_password }}\"\n    hosting_vcenter_ip: \"{{ hosting_vcenter.ip }}\"\n    hosting_vcenter_username: \"{{ hosting_vcenter.username }}\"\n    hosting_vcenter_password: \"{{ hosting_vcenter.password }}\"\n    hosting_datacenter: \"{{ hosting_vcenter.datacenter }}\"\n    hosting_datastore: \"{{ opinionated.hosting_datastore }}\"\n    hosting_network: \"{{ opinionated.hosting_network.base.port_group }}\"\n    hosting_cluster: \"{{ opinionated.hosting_cluster }}\"\n    license_key: \"{{ lookup('env', 'NSXT_LICENSE_KEY') }}\"\n\n  principal_identities:\n    - display_name: tkgi-super-user\n      role: \"enterprise_admin\"\n      public_key: |-\n        {{ opinionated.tkgi.nsx_principal_identity.public_key }}\n    - display_name: tas-super-user\n      role: \"enterprise_admin\"\n      public_key: |-\n        {{ opinionated.tas.nsx_principal_identity.public_key }}\n\n  policy_ip_pools:\n    - display_name: tep-pool  # This is a non-routable range which is used for the overlay tunnels.\n      pool_static_subnets:\n        - id: tep-pool-1\n          state: present\n          allocation_ranges:\n            - start: \"{{ opinionated.hosting_network.nsxt_tep.cidr | ansible.utils.ipmath(1) }}\"\n              end: \"{{ opinionated.hosting_network.nsxt_tep.cidr | ansible.utils.ipaddr('-2') |ansible.utils.ipaddr('address') }}\"\n          cidr: \"{{ opinionated.hosting_network.nsxt_tep.cidr }}\"\n          do_wait_till_create: true\n\n    - display_name: tkgi-floating-ips\n      pool_static_subnets:\n        - id: tkgi-floating-ips-1\n          state: present\n          allocation_ranges:\n            - start: \"{{ opinionated.tkgi.floating_ip_pool | ansible.utils.ipaddr('1') | ansible.utils.ipv4('address') }}\"\n              end: \"{{ opinionated.tkgi.floating_ip_pool | ansible.utils.ipaddr('-2') | ansible.utils.ipv4('address') }}\"\n          cidr: \"{{ opinionated.tkgi.floating_ip_pool }}\"\n\n  ip_blocks:\n    - display_name: tkgi-nodes\n      cidr: \"{{ opinionated.tkgi.node_ip_block }}\"\n    - display_name: tkgi-pods\n      cidr: \"{{ opinionated.tkgi.pod_ip_block }}\"\n\n  uplink_profiles:\n    - display_name: host-tep-profile\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: \"{{ opinionated.hosting_network.nsxt_tep.vlan_id }}\"\n    - display_name: edge-tep-profile\n      mtu: 9000\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: \"{{ opinionated.hosting_network.nsxt_tep.vlan_id }}\"\n    - display_name: edge-uplink-profile\n      mtu: 1500\n      teaming:\n        active_list:\n          - uplink_name: \"uplink-1\"\n            uplink_type: PNIC\n        policy: FAILOVER_ORDER\n      transport_vlan: 0\n\n  transport_zones:\n    - display_name: tz-overlay\n      # transport_type: OVERLAY_BACKED\n      transport_type: OVERLAY\n      # host_switch_name: \"{{ distributed_switches[0].vds_name }}\"\n      nested_nsx: true  # Set this to true if you use NSX-T for your physical host networking\n      description: \"Overlay Transport Zone\"\n    - display_name: tz-vlan\n      # transport_type: VLAN_BACKED\n      transport_type: VLAN\n      # host_switch_name: sw_vlan\n      description: \"Uplink Transport Zone\"\n\n  transport_node_profiles:\n    - display_name: tnp1\n      host_switches:\n        - host_switch_profiles:\n            - name: host-tep-profile\n              type: UplinkHostSwitchProfile\n          host_switch_name: \"{{ distributed_switches[0].vds_name }}\"\n          host_switch_type: VDS\n          host_switch_mode: STANDARD\n          ip_assignment_spec:\n            resource_type: StaticIpPoolSpec\n            ip_pool_name: \"tep-pool\"\n          transport_zone_endpoints:\n            - transport_zone_name: \"tz-overlay\"\n            - transport_zone_name: \"tz-vlan\"\n          uplinks:\n            - uplink_name: \"uplink-1\"\n              vds_uplink_name: \"Uplink 1\"\n      description: \"Cluster node profile\"\n\n  cluster_attach:\n    - display_name: \"tnc1\"\n      description: \"Transport Node Collections 1\"\n      compute_manager_name: \"vCenter\"\n      cluster_name: \"compute\"\n      transport_node_profile_name: \"tnp1\"\n\n  edge_nodes:\n    - display_name: edge-node-1\n      size: LARGE\n      mgmt_ip_address: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(2) }}\"\n      mgmt_prefix_length: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] }}\"\n      mgmt_default_gateway: \"{{ opinionated.hosting_network.base.gateway }}\"\n      network_management_name: vm-network\n      network_uplink_name: vm-network\n      network_tep_name: edge-tep-seg\n      datastore_name: \"{{ opinionated.nested_hosts.local_disks[0].datastore_prefix }}-esx1\"\n      cluster_name: compute\n      host_switches:\n        tep:\n          uplink_profile_name: edge-tep-profile\n          ip_assignment_spec:\n            resource_type: StaticIpPoolSpec\n            ip_pool_name: tep-pool\n          transport_zone_endpoints:\n            - transport_zone_name: \"tz-overlay\"\n        uplink:\n          host_switch_name: \"sw_vlan\"\n          uplink_profile_name: edge-uplink-profile\n          transport_zone_endpoints:\n            - transport_zone_name: \"tz-vlan\"\n      transport_zone_endpoints:\n        - transport_zone_name: \"tz-overlay\"\n        - transport_zone_name: \"tz-vlan\"\n\n  edge_clusters:\n    - edge_cluster_name: edge-cluster-1\n      edge_cluster_members:\n        - transport_node_name: edge-node-1\n\n  vlan_segments:\n    - display_name: t0-uplink\n      vlan_ids: [0]\n      transport_zone_display_name: tz-vlan\n    - display_name: edge-tep-seg\n      vlan_ids: [0]\n      transport_zone_display_name: tz-vlan\n\n  # For full spec see - https://github.com/laidbackware/ansible-for-nsxt/blob/vmware-lab-builder/library/nsxt_policy_tier0.py\n  tier_0:\n    display_name: \"{{ opinionated.nsxt.tier0_gateway_name }}\"\n    ha_mode: ACTIVE_STANDBY\n    uplink_ip: \"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"\n    disable_firewall: true\n    static_routes:\n      - state: present\n        display_name: default-route\n        network: \"0.0.0.0/0\"\n        next_hops:\n          - ip_address: \"{{ opinionated.hosting_network.base.gateway }}\"\n    locale_services:\n      - state: present\n        display_name: \"{{ opinionated.nsxt.tier0_gateway_name }}-locale\"\n        edge_cluster_info:\n          edge_cluster_display_name: edge-cluster-1\n        interfaces:\n          - display_name: \"{{ opinionated.nsxt.tier0_gateway_name }}-t0ls-iface\"\n            state: present\n            subnets:\n              - ip_addresses: [\"{{ opinionated.hosting_network.base.starting_addr | ansible.utils.ipmath(3) }}\"]\n                prefix_len: \"{{ opinionated.hosting_network.base.cidr.split('/')[1] | int }}\"\n            segment_id: t0-uplink\n            edge_node_info:\n              edge_cluster_display_name: edge-cluster-1\n              edge_node_display_name: edge-node-1\n            mtu: 1500\n\n  # build list of segments from opinionated section. See custom deployment for example of how to configurre\n  overlay_segments: &gt;-\n    [\n      {% if \"overlay_segment\" in  opinionated.nsxt %}\n      {% for overlay_segment in opinionated.nsxt.standalone_overlay_segments %}\n      {\n        \"display_name\": \"{{ overlay_segment.display_name }}\",\n        \"transport_zone_display_name\": \"tz-overlay\"\n      },\n      {% endfor %}\n      {% endif %}\n      {% for tier1_gateway in opinionated.nsxt.tier1_gateways %}\n      {% if \"segments\" in  tier1_gateway %}\n      {% for segment in tier1_gateway.segments %}\n      {\n        \"display_name\": \"{{ segment.display_name }}\",\n        \"transport_zone_display_name\": \"tz-overlay\",\n        \"tier1_display_name\": \"{{ tier1_gateway.display_name }}\",\n        \"subnets\": [\n          {\"gateway_address\": \"{{ segment.default_gateway_cidr }}\"}\n        ]\n      },\n      {% endfor %}\n      {% endif %}\n      {% endfor %}\n    ]\n\n  tier_1_gateways: &gt;-\n    [\n      {% for tier1_gateway in opinionated.nsxt.tier1_gateways %}\n      {\n        \"display_name\": \"{{ tier1_gateway.display_name }}\",\n        \"route_advertisement_types\": \"{{ tier1_gateway.route_advertisement_types }}\",\n        \"tier0_display_name\": \"{{ opinionated.nsxt.tier0_gateway_name }}\"\n      },\n      {% endfor %}\n    ]\n</code></pre> <p>Now we are ready to finally Deploy TAS and TKGI with Concourse.</p>"},{"location":"tanzu/platform-automation-toolkit/install-concourse-for-platform-automation/","title":"Install Concourse for Platform Automation","text":"<p>This guide follows the official documentation to Installing Concourse for Platform Automation but uses Ansible to automate the deployment on vCenter.</p> <p>The Ansible playbook will:</p> <ul> <li>provision Opsman (VMware Operations Manager) in a VM</li> <li>configure BOSH director</li> <li>deploy Concourse as a BOSH release</li> <li>deploy MinIO as a Single-Node Single-Drive in a Ubuntu VM</li> </ul> <p>The goal is to have a fully functioning Platform Automation Toolkit that can be used to install Tanzu Application Service or Tanzu Kubernetes Grid Integrated Edition. MinIO can be used to store the required artifacts.</p>"},{"location":"tanzu/platform-automation-toolkit/install-concourse-for-platform-automation/#prerequisites","title":"Prerequisites","text":"<ul> <li>a running vCenter (\"Management\"): tested on 8.0.3 but should work on version 7 as well</li> <li>all required products need to be pre-downloaded from Broadcom Support Portal:<ul> <li>Opsman</li> <li>all Concourse products: Concourse Bosh deployment, Concourse BOSH Release, BPM, Postgres, UAA, Credhub, Backup and Restore SDK</li> <li>Ubuntu Jammy Stemcell (you can also use any other stemcell)</li> </ul> </li> <li>Ubuntu OVA downloaded, e.g. noble-server-cloudimg</li> </ul>"},{"location":"tanzu/platform-automation-toolkit/install-concourse-for-platform-automation/#deployment","title":"Deployment","text":"<p>We will use vmware-lab-builder to bootstrap the infrastructure for the  Platform Automation Toolkit using this vars yaml. This will provision all resources mentioned above non-nested, which means it will deploy the VMs on your hosting vCenter. See further details how to run the Ansible playbook to deploy this lab in my Homelab Section.</p> <p>Once, the Platform Automation Toolkit is running, you can access Concourse UI and MinIO UI to verify the deployment. Before we can actually create our Concourse pipelines to deploy TAS or TKGI, we first have to Deploy vSphere + NSX-T for TAS and TKGI.</p>"},{"location":"tanzu/tanzu-packages/","title":"Tanzu Packages","text":"<p>See official docs here.</p>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/","title":"Deploy TKG packages in airgapped environments","text":""},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#prerequisites","title":"Prerequisites","text":"<ul> <li>TKGS Supervisor cluster running</li> <li>embedded Harbor running</li> <li>shared services cluster running</li> </ul>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#environment-info","title":"Environment info","text":"<ul> <li>vCenter 7u3p (Build 22837322)</li> <li>Supervisor cluster version v1.25.6+vmware.wcp.2</li> <li>Guest Cluster version: v1.23.8---vmware.3-tkg.1</li> </ul>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-packages","title":"Install Packages","text":"<p>We are installing the following packages:</p> Package Version cert-manager 1.7.2+vmware.1-tkg.1 contour 1.20.2+vmware.2-tkg.1 harbor 2.3.3+vmware.1-tkg.1 <p>We deploy those packages on a Kubernetes cluster with version 1.23.8. This cluster has PodSecurityPolicies enabled.  Because we don't care, we allow all pods with</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: psp:privileged\nrules:\n- apiGroups: ['policy']\n  resources: ['podsecuritypolicies']\n  verbs:     ['use']\n  resourceNames:\n  - vmware-system-privileged\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: all:psp:privileged\nroleRef:\n  kind: ClusterRole\n  name: psp:privileged\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: Group\n  name: system:serviceaccounts\n  apiGroup: rbac.authorization.k8s.io\nEOF\n</code></pre>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#deploy-tkg-package-repository","title":"Deploy TKG Package Repository","text":""},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-kapp-controller","title":"Install kapp-controller","text":"<p>We are following the official docs here.</p> <p>Run the following from a machine with access to the VMware public registry:</p> <ol> <li> <p>list available versions:</p> <pre><code>imgpkg tag list -i projects.registry.vmware.com/tkg/kapp-controller\n</code></pre> </li> <li> <p>Copy the version of choice to your local registry</p> <pre><code>imgpkg copy \\\n  -i projects.registry.vmware.com/tkg/kapp-controller:v0.30.0_vmware.1 \\\n  --to-repo 172.30.4.131/shared-services/kapp-controller \\\n  --registry-ca-cert-path ./ca.crt\n</code></pre> <p>Alternatively, you can download the tar file to your filesystem</p> <pre><code>imgpkg copy \\\n  -i projects.registry.vmware.com/tkg/kapp-controller:v0.41.7_vmware.1 \\\n  --to-tar  ./kapp-controller_v0.41.7_vmware.1\n</code></pre> </li> <li> <p>Create the <code>tanzu-package-repo-global</code> namespace:</p> <pre><code>kubectl create ns tanzu-package-repo-global\n</code></pre> </li> <li> <p>create a secret to be able to pull images from the local registry with authentication</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n tanzu-package-repo-global\n</code></pre> </li> <li> <p>Copy the content of the kapp-controller manifest from here and make some changes:</p> </li> <li>update the <code>image:</code> accordingly to point to your image stored in your local container registry</li> <li> <p>add the following to <code>Deployment.spec.template.spec</code></p> <pre><code>```shell\nimagePullSecrets:\n- name: embedded-harbor\n```\n</code></pre> </li> <li> <p>Switch kubectl context to your shared services cluster and apply the manifest</p> <pre><code>kubectl apply -f kapp-controller.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#add-package-repository-to-cluster","title":"Add Package Repository to Cluster","text":"<p>We are following the official docs here.</p> <p>Run the following from a machine with access to the VMware public registry:</p> <ol> <li> <p>list available Package Repository versions:</p> <pre><code>imgpkg tag list -i projects.registry.vmware.com/tkg/packages/standard/repo\n</code></pre> </li> <li> <p>Copy your version of choice to your registry</p> <pre><code>imgpkg copy \\\n  -b projects.registry.vmware.com/tkg/packages/standard/repo:v1.6.1 \\\n    --to-repo 172.30.4.131/shared-services/packages/standard/repo \\\n    --registry-ca-cert-path ./ca.crt\n</code></pre> </li> <li> <p>Create a <code>PackageRepository</code> manifest and call it <code>packagerepo-v1.6.1.yaml</code>:</p> <pre><code>apiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\n  name: tanzu-standard\n  namespace: tanzu-package-repo-global\nspec:\n  fetch:\n    imgpkgBundle:\n      image: 172.30.4.131/shared-services/packages/standard/repo:v1.6.1\n      secretRef:\n        name: embedded-harbor\n</code></pre> </li> <li> <p>Switch kubectl context to your shared services cluster and apply the manifest</p> <pre><code>kubectl apply -f packagerepo-v1.6.1.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#prepare-user-managed-tanzu-packages","title":"Prepare user managed Tanzu Packages","text":"<ol> <li> <p>Create a common namespace used for all user managed Tanzu packages: </p> <pre><code>kubectl create ns tanzu-packages-user-managed\n</code></pre> </li> <li> <p>Replicate the <code>embedded-harbor</code> secret from the <code>tanzu-package-repo-global</code> namespace to the <code>tanzu-packages-user-managed</code> namespace:</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> <li> <p>To use the <code>embedded-harbor</code> in all cert-manager deployment's <code>spec.template.spec.imagePullSecrets</code> we have to create a ytt overlay and use that overlay in the <code>PackageInstall</code>. Create the overlay <code>image-pull-secrets-overlay-deployment.yaml</code></p> <pre><code>#@ load(\"@ytt:overlay\", \"overlay\")\n#@overlay/match by=overlay.subset({\"kind\": \"Deployment\"}), expects=\"1+\"\n---\nspec:\n  template:\n    spec:\n      #@overlay/match missing_ok=True\n      imagePullSecrets:\n      - name: embedded-harbor\n</code></pre> <p>and then create a Kubernetes secret:</p> <pre><code>kubectl create secret generic image-pull-secret-overlay-deployment \\\n  --from-file=image-pull-secrets-overlay-deployment.yaml \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> <li> <p>We do the same for DaemonSets. Create the overlay <code>image-pull-secrets-overlay-daemonset.yaml</code></p> <pre><code>#@ load(\"@ytt:overlay\", \"overlay\")\n#@overlay/match by=overlay.subset({\"kind\": \"DaemonSet\"}), expects=\"1+\"\n---\nspec:\n  template:\n    spec:\n      #@overlay/match missing_ok=True\n      imagePullSecrets:\n      - name: embedded-harbor\n</code></pre> <p>and then create a Kubernetes secret:</p> <pre><code>kubectl create secret generic image-pull-secret-overlay-daemonset \\\n  --from-file=image-pull-secrets-overlay-daemonset.yaml \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> <li> <p>We do the same for StatefulSets. Create the overlay <code>image-pull-secrets-overlay-statefulsets.yaml</code></p> <pre><code>#@ load(\"@ytt:overlay\", \"overlay\")\n#@overlay/match by=overlay.subset({\"kind\": \"StatefulSet\"}), expects=\"1+\"\n---\nspec:\n  template:\n    spec:\n      #@overlay/match missing_ok=True\n      imagePullSecrets:\n      - name: embedded-harbor\n</code></pre> <p>and then create a Kubernetes secret:</p> <pre><code>kubectl create secret generic image-pull-secret-overlay-statefulsets \\\n  --from-file=image-pull-secrets-overlay-statefulsets.yaml \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-cert-manager","title":"Install cert-manager","text":"<p>We are following the official docs here.</p> <ol> <li> <p>Create the <code>cert-manager</code> namespace:</p> <pre><code>kubectl create ns cert-manager\n</code></pre> </li> <li> <p>Create the <code>embedded-harbor</code> secrets used in <code>imagePullSecrets</code> in each pod:</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n cert-manager\n</code></pre> </li> <li> <p>List available version for cert-manager:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep cert-manager\n</code></pre> </li> <li> <p>Create the manifest <code>cert-manager.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: cert-manager-sa\n  namespace: tanzu-packages-user-managed\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: cert-manager-sa\n    namespace: tanzu-packages-user-managed\n---\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: cert-manager\n  namespace: tanzu-packages-user-managed\n  annotations:\n    ext.packaging.carvel.dev/fetch-0-secret-name: embedded-harbor\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.0: image-pull-secret-overlay-deployment\nspec:\n  serviceAccountName: cert-manager-sa\n  packageRef:\n    refName: cert-manager.tanzu.vmware.com\n    versionSelection:\n      constraints: 1.7.2+vmware.1-tkg.1\n  values:\n  - secretRef:\n      name: cert-manager-data-values\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: cert-manager-data-values\n  namespace: tanzu-packages-user-managed\nstringData:\n  values.yml: |\n    ---\n    namespace: cert-manager\n</code></pre> <p>and apply it to the cluster:</p> <pre><code>kubectl apply -f cert-manager.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-contour","title":"Install Contour","text":"<p>The process is very simlar to installing <code>cert-manager</code>. We are following the official docs here using <code>kubectl</code>.</p> <ol> <li> <p>Create the <code>tanzu-system-ingress</code> namespace:</p> <pre><code>kubectl create ns tanzu-system-ingress\n</code></pre> </li> <li> <p>Create the <code>embedded-harbor</code> secrets used in <code>imagePullSecrets</code> in each pod:</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n tanzu-system-ingress\n</code></pre> </li> <li> <p>List available version for contour:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep contour\n</code></pre> </li> <li> <p>Create the manifest <code>contour.yaml</code></p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: contour-sa\n  namespace: tanzu-packages-user-managed\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: contour-role-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: contour-sa\n    namespace: tanzu-packages-user-managed\n---\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: contour\n  namespace: tanzu-packages-user-managed\n  annotations:\n    ext.packaging.carvel.dev/fetch-0-secret-name: embedded-harbor\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.0: image-pull-secret-overlay-deployment\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.1: image-pull-secret-overlay-daemonset\nspec:\n  serviceAccountName: contour-sa\n  packageRef:\n    refName: contour.tanzu.vmware.com\n    versionSelection:\n      constraints: 1.20.2+vmware.2-tkg.1\n  values:\n  - secretRef:\n      name: contour-data-values\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: contour-data-values\n  namespace: tanzu-packages-user-managed\nstringData:\n  values.yml: |\n    ---\n    infrastructure_provider: vsphere\n    namespace: tanzu-system-ingress\n    contour:\n      configFileContents: {}\n      useProxyProtocol: false\n      replicas: 2\n      pspNames: \"vmware-system-restricted\"\n      logLevel: info\n    envoy:\n      service:\n        type: LoadBalancer\n        annotations: {}\n        nodePorts:\n          http: null\n          https: null\n        externalTrafficPolicy: Cluster\n        disableWait: false\n      hostPorts:\n        enable: false\n        http: 80\n        https: 443\n      hostNetwork: false\n      terminationGracePeriodSeconds: 300\n      logLevel: info\n      pspNames: null\n    certificates:\n      duration: 8760h\n      renewBefore: 360h\n</code></pre> <p>and apply it to the cluster:</p> <pre><code>kubectl apply -f contour.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#install-harbor","title":"Install Harbor","text":"<ol> <li> <p>Create the <code>tanzu-system-registry</code> namespace:</p> <pre><code>kubectl create ns tanzu-system-registry\n</code></pre> </li> <li> <p>Create the <code>embedded-harbor</code> secrets used in <code>imagePullSecrets</code> in each pod:</p> <pre><code>kubectl create secret docker-registry embedded-harbor \\\n  --docker-server=172.30.4.131 \\\n  --docker-username=administrator@vsphere.local \\\n  --docker-password=VMware1! \\\n  -n tanzu-system-registry\n</code></pre> </li> <li> <p>List available version for harbor:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep harbor\n</code></pre> </li> <li> <p>Create the manifest <code>harbor.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: harbor-sa\n  namespace: tanzu-packages-user-managed\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: habor-role-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: harbor-sa\n    namespace: tanzu-packages-user-managed\n---\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageInstall\nmetadata:\n  name: harbor\n  namespace: tanzu-packages-user-managed\n  annotations:\n    ext.packaging.carvel.dev/fetch-0-secret-name: embedded-harbor\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.0: image-pull-secret-overlay-deployment\n    ext.packaging.carvel.dev/ytt-paths-from-secret-name.1: image-pull-secret-overlay-statefulsets\nspec:\n  serviceAccountName: harbor-sa\n  packageRef:\n    refName: harbor.tanzu.vmware.com\n    versionSelection:\n      constraints: 2.3.3+vmware.1-tkg.1\n  values:\n  - secretRef:\n      name: harbor-data-values\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: harbor-data-values\n  namespace: tanzu-packages-user-managed\nstringData:\n  values.yml: |\n    namespace: tanzu-system-registry\n    hostname: harbor.internal\n    port:\n      https: 443\n    logLevel: info\n    tlsCertificate:\n      tls.crt: \"\"\n      tls.key: \"\"\n      ca.crt:\n    tlsCertificateSecretName:\n    enableContourHttpProxy: true\n    harborAdminPassword: 'VMware1!'\n    secretKey: 'aiGhooghu8uaS7zo'\n    database:\n      password: 'VMware1!'\n      shmSizeLimit:\n      maxIdleConns:\n      maxOpenConns:\n    exporter:\n      cacheDuration:\n    core:\n      replicas: 1\n      secret: 'VMware1!'\n      xsrfKey: oopoo7iecae8wai5eejeethaingeip4W\n    jobservice:\n      replicas: 1\n      secret: 'VMware1!'\n    registry:\n      replicas: 1\n      secret: 'VMware1!'\n    notary:\n      enabled: true\n    trivy:\n      enabled: true\n      replicas: 1\n      gitHubToken: \"\"\n      skipUpdate: false\n    persistence:\n      persistentVolumeClaim:\n        registry:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 50Gi\n        jobservice:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 10Gi\n        database:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 10Gi\n        redis:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 10Gi\n        trivy:\n          existingClaim: \"\"\n          storageClass: \"tkgs-storage-policy\"\n          subPath: \"\"\n          accessMode: ReadWriteOnce\n          size: 10Gi\n    proxy:\n      httpProxy:\n      httpsProxy:\n      noProxy: 127.0.0.1,localhost,.local,.internal\n    pspNames: vmware-system-restricted,vmware-system-privileged\n    network:\n      ipFamilies: [\"IPv4\", \"IPv6\"]\n</code></pre> <p>and apply it to the cluster:</p> <pre><code>kubectl apply -f harbor.yaml\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-packages","title":"Upgrade Packages","text":"<p>We are updating the following packages:</p> Package Current Version Target Version cert-manager 1.7.2+vmware.1-tkg.1 1.10.2+vmware.1-tkg.1 contour 1.20.2+vmware.2-tkg.1 1.23.5+vmware.1-tkg.1 harbor 2.3.3+vmware.1-tkg.1 2.5.3+vmware.1-tkg.1"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-tanzu-package-repository","title":"Upgrade Tanzu Package Repository","text":"<p>In order to have newer Package versions available, we first have to upgrade the Tanzu Package Repository.</p> <ol> <li> <p>list available Package Repository versions:</p> <pre><code>imgpkg tag list -i projects.registry.vmware.com/tkg/packages/standard/repo\n</code></pre> </li> <li> <p>Copy a new version to your local registry</p> <pre><code>imgpkg copy \\\n  -b projects.registry.vmware.com/tkg/packages/standard/repo:v2.2.0 \\\n    --to-repo 172.30.4.131/shared-services/packages/standard/repo \\\n    --registry-ca-cert-path ./ca.crt\n</code></pre> </li> <li> <p>Update the existing Package repository accordingly</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: packaging.carvel.dev/v1alpha1\nkind: PackageRepository\nmetadata:\n  name: tanzu-standard\n  namespace: tanzu-package-repo-global\nspec:\n  fetch:\n    imgpkgBundle:\n      image: 172.30.4.131/shared-services/packages/standard/repo:v2.2.0\n      secretRef:\n        name: embedded-harbor\nEOF\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-cert-manager","title":"Upgrade cert-manager","text":"<ol> <li> <p>List available version for cert-manager:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep cert-manager\n</code></pre> </li> <li> <p>Find a compatible cert-manager version with your Kubernetes version  and update <code>cert-manager</code>:</p> <pre><code>tanzu package installed update cert-manager \\\n  --version 1.10.2+vmware.1-tkg.1 \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-contour","title":"Upgrade contour","text":"<ol> <li> <p>List available version for contour:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep contour\n</code></pre> </li> <li> <p>Find a compatible contour version with your Kubernetes version and update <code>contour</code>:</p> <pre><code>tanzu package installed update contour \\\n  --version 1.23.5+vmware.1-tkg.1 \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#upgrade-harbor","title":"Upgrade harbor","text":"<ol> <li> <p>List available version for harbor:</p> <pre><code>kubectl get packages -n tanzu-packages-user-managed | grep harbor\n</code></pre> </li> <li> <p>Update <code>harbor</code>:</p> <pre><code>tanzu package installed update harbor \\\n  --version 2.5.3+vmware.1-tkg.1 \\\n  -n tanzu-packages-user-managed\n</code></pre> </li> </ol>"},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#things-to-note","title":"Things to note","text":""},{"location":"tanzu/tanzu-packages/deploy-packages-airgapped/#using-local-content-library","title":"Using local content library","text":"<p>when using local content library, the name when importing in vCenter has to be the same name as in <code>item.json</code>. Otherwise you get the following error when using the TKR:</p> <pre><code>unable to resolve that TKR due to could not resolve TKR/OSImage for controlPlane\n</code></pre>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/","title":"Tanzu Platform Technical Details","text":""},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#goal-of-this-page","title":"Goal of this page","text":"<p>The goal of this page is to add additional information around Tanzu Platform for Kubernetes, what you can't easily find in the official documentation, but what you would learn when working hands-on with it. This page tries to go into all necessary technical details required to better understand the concepts. By providing this, it should be easier to manage, own and troubleshoot.</p> <p>Warning</p> <p>This page is in progress. Currently it just contains notes, there is no clear format or structure. It's mainly used by myself while studying TP4K8s and I will bring it into a good shape once I understand it better.</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#questions","title":"Questions","text":"<ul> <li>how to specify to use <code>default-istio-gateway</code> or <code>spring-cloud-gateway</code> for ingress</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#concepts","title":"Concepts","text":"<p>There is a very good page explaining the concepts here. You have to be familiar with</p> <ul> <li>Spaces</li> <li>Capabilities</li> <li>Cluster Groups</li> <li>Availability Targets</li> <li>Profiles</li> <li>Traits</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#technical-architecture","title":"Technical Architecture","text":"<p>The Platform Engineer Hands-On Lab workshop provides a very high-level picture how Tanzu Platform for Kubernetes works technically, described in this picture:</p> <p></p> <p><code>UCP</code> stands for Unified Control Plane (which you don't find in the official docs as of today). It is technically based on KCP which essentially gives you a Kubernetes API without having a Kubernetes cluster with all its components like pods, deployments etc. In the end, you can interact with it using <code>kubectl</code> and enrich it with custom APIs (CRDs) with only having the APIs in place you wish to have in place.</p> <p>Let's illustrate that with a simple example without using the Tanzu Platform terminology, but the terminology that we already familiar with when using TKGS or TKGm: Let's say we want to install the cert-manager Tanzu Package (we will see later that this is called <code>Capability</code> in Tanzu Platform terminology) you would install it with</p> <pre><code>tanzu package install cert-manager.tanzu.vmware.com -p cert-manager.tanzu.vmware.com -v 1.11.1+vmware.1-tkg.1\n</code></pre> <p>This will actually not deploy the <code>PackageInstall</code> on your target cluster, but on UCP, and UCP will ensure the PackageInstall will be reconciled down to your Kubernetes Cluster.</p> <p>UCP in Tanzu Platform for Kubernetes is used to reconcile anything that you create via Hub (the UI) or via the API (using the tanzu cli) down to your Kubernetes clusters.</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#you-may-ask-what-is-the-motivation-behind-this-approach","title":"You may ask: what is the motivation behind this approach?","text":"<p>I can think of the following:</p> <p>TP4K8s has a concept called <code>Spaces</code>, which in the end create Kubernetes namespaces in the target K8s clusters. You can specify a space with 3 replicas, which will create 3 K8s namespaces, spread across multiple clusters (that could live in different regions potentially). When you install cert-manager in your space, with this approach of proxying via UCP, UCP will make sure to install cert-manager in all of those 3 namespaces across clusters and also make sure that they are all similarly configured.</p> <p>In the end, it all drills down to be able to manage a fleet of clusters from a single Control Plane (API), and this is UCP. It allows to bring a fleet of clusters into a consistent and manageable state, heavily using the power of Kubernetes Reconciliation and ensure the desired state is always equal to the actual state.</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#notes","title":"Notes","text":"<ul> <li>groupings (cluster groups to group clusters, availability targets to also group clusters, spaces to group namespaces) is all based on labels</li> <li>because of label selectors, a cluster might belong to one or many Availability Targets</li> <li>because of label selectors, a cluster might belong to one or many Cluster Groups</li> <li>capabilities are provided by Tanzu Packages (Carvel Packages) installed to every cluster belonging to the cluster group that provides this capability</li> <li>if a capability is provided to a cluster group, the corresponding Tanzu Package installed on all clusters in that cluster group have the <code>capability.tanzu.vmware.com/provides: &lt;capability-name&gt;</code> annotation</li> <li> <p>Capabilities vs Traits</p> <ul> <li>Capabilities provides the set of CRDs (with controllers)</li> <li>Traits creates an instance of the CRD (CR - Custom Resource)</li> <li>example:<ul> <li>the <code>cert-manager</code> capability deploys the <code>cert-manager</code> pods and installs the cert-manager CRDs, like <code>ClusterIssuer</code>, <code>Certificate</code> etc.</li> <li>there is a <code>multicloud-certmanager</code> trait, which actually creates certificate issuers, so custom resources of <code>kind: ClusterIssuer</code> etc. and actually makes use of the <code>cert-manager</code> issuer</li> </ul> </li> </ul> </li> <li> <p>Profiles is just a configuration but you don't really deploy anything by creating a profile. A space is ultimately combining everything to provision something with the information from a profile. This means, that all available capabilities are provided on the cluster group, but you can only use capabilities in your spaces if you have provided them in your space via the profile</p> </li> <li>when creating a space you can select an Availability Target and with it how many replicas of the AT - when you configure 3 replicas, the UCP will create 3 managed namespaces across 3 clusters</li> <li>namespaces/pods added by Hub (UCP)<ul> <li>tanzu-cluster-group-system: used to deploy <code>PackageInstalls</code> of all capabilities installed on the cluster group</li> <li>tanzu system<ul> <li><code>aria-k8s-collector</code></li> <li><code>syncer</code></li> <li><code>tanzu-capabilities-controller-manager</code></li> </ul> </li> <li>vmware-system-tmc</li> </ul> </li> <li>Hub adds 4 Package Repositories that bundle all capabilities</li> <li>when you create a space, it will<ul> <li>create a <code>ManagedNamespaceSet</code> on UCP</li> <li>which will create a <code>ManagedNamespace</code> on UCP</li> <li>which will create two <code>Namespace</code>'s on the target cluster, called <code>&lt;space-name&gt;-&lt;hash&gt;</code> and <code>&lt;space-name&gt;-&lt;hash&gt;-internal</code><ul> <li>the internal namespace is used to deploy <code>PackageInstalls</code> of the traits, referenced in the associated profiles</li> <li>the \"normal\" namespace is used to deploy end applications</li> </ul> </li> </ul> </li> <li>a <code>Space</code>and a <code>ManagedNamespace</code> (and the final <code>Namespace</code> on the target cluster(s) - which is essentially a 1:n mirror from a <code>ManagedNamespace</code>) can be compared to a <code>Deployment</code> and a <code>Pod</code><ul> <li>each time you update the Space, it will create a new <code>ManagedNamespaceSet</code></li> </ul> </li> <li><code>Spaces</code> are reconciled by a <code>Space Controller</code></li> <li>there is a dedicated ManagedNamespace for each AvailabilityTarget of a ManagedNamespaceSet</li> <li>once a ManagedNamespace has been created successfully on UCP, there is the <code>Space Scheduler</code> that provisions a namespace on a target cluster and installs the traits into that namespace</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#faq","title":"FAQ","text":""},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#ucp-stands-for-unified-control-plane-correct-do-we-explain-this-on-a-high-level-anywhere-in-the-docs-or-is-this-not-planned-to-do-as-the-end-users-dont-need-to-be-aware-of-it","title":"UCP stands for Unified Control Plane, correct? Do we explain this on a high-level anywhere in the docs? Or is this not planned to do as the end users don't need to be aware of it?","text":"<p>See Google Chat.</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#can-i-interact-with-ucp-using-kubectl","title":"Can I interact with UCP using kubectl?","text":"<p>Yes.</p> <ol> <li>login with <code>tanzu login</code></li> <li>use a specific context (see below for further information on contexts)<ol> <li><code>project</code> context: <code>tanzu project use</code></li> <li><code>clustergroup</code> context: <code>tanzu ops clustergroup use</code></li> <li><code>space</code> context: <code>tanzu space use</code></li> </ol> </li> <li>use the kuebconfig at <code>~/.config/tanzu/kube/config</code></li> </ol> <p>A handy thing to do is to create an alias <code>alias tk=\"KUBECONFIG=~/.config/tanzu/kube/config kubectl\"</code> and then interact with UCP using <code>tk</code> instead of <code>kubectl</code>.</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#when-i-have-1-availability-target-with-3-clusters-and-i-choose-to","title":"When I have 1 availability target with 3 clusters and I choose to","text":"<ul> <li>deploy a space with 1 replica: will the namespace get deployed on one cluster only? =&gt; yes</li> <li>deploy a space with 2 replicas: will 2 namespaces get deployed on one cluster or 1 namespace on each cluster? =&gt; 2 namespaces on one cluster is possible</li> <li>deploy a space with 3 replicas: will the namespace get deployed evenly, 1 namespace on each cluster? =&gt; no. 3 namespaces on one cluster is possible</li> <li>deploy a space with more replicas than available clusters =&gt; explained by above behaviour</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#who-are-the-personas-and-who-is-responsible-for-what","title":"Who are the personas and who is responsible for what?","text":"<p>Where are the boundaries? What's the motivation behind the technical architecture - who is supposed to see what and what is intended to be abstracted away? =&gt; https://docs.google.com/document/d/1Rmt-eskKBo2mNDsSdHeVaIWuZU3zhKznEDcRVN9BRRs/edit#heading=h.ixfosuy76boz</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#is-there-is-a-dedicated-managednamespace-for-each-availabilitytarget-of-a-managednamespaceset","title":"Is there is a dedicated ManagedNamespace for each AvailabilityTarget of a ManagedNamespaceSet","text":"<p>Yes. If you deploy a space to two Availability Targets with one replica each, there is one <code>ManagedNamespaceSet</code> on UCP, and two <code>ManagedNamespace</code>'s, one for each cluster.</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#what-happens-when-i-dont-enable-tanzu-application-engine-when-creating-a-cluster-group-does-it-mean-the-clusters-in-it-are-not-managed-by-ucp","title":"What happens when I don't enable <code>Tanzu Application Engine</code> when creating a cluster Group? Does it mean, the clusters in it are not managed by UCP?","text":"<p>Yes, it doesn't have the <code>PackageInstall</code>'s installed in the <code>tanzu-cluster-group-system</code> namespace, because you are not able to install capabilities into this clustergroup. Only the <code>vss-k8s-collector.tanzu.vmware.com</code> Package will be installed.</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#when-interacting-via-kubectl-with-ucp-using-kubeconfig-configtanzukubeconfig-then-you-see-different-things-depending-which-context-you-use-project-clustergroup-or-space-context-which-objects-do-you-see-using-which-context","title":"when interacting via kubectl with UCP (using kubeconfig <code>~/.config/tanzu/kube/config</code>), then you see different things depending which context you use (<code>project</code>, <code>clustergroup</code> or <code>space</code> context). Which objects do you see using which context?","text":"<ul> <li><code>managednamespacesets</code> - <code>project</code> context</li> <li><code>managednamespaces</code> - <code>project</code> context</li> <li><code>profiles</code> - <code>project</code> context</li> <li><code>spaces</code> - <code>project</code> context</li> <li><code>kubernetesclusters</code> - <code>clustergroup</code> context</li> <li><code>packagerepositories</code> - <code>project</code> &amp; <code>clustergroup</code> contexts, which give you different outputs but not clear what it provides</li> <li><code>packageinstalls</code>:<ul> <li><code>clustergroup</code> context: you will see all capabilities installed in a clustergroup</li> <li><code>space</code> context: you will see custom PackageInstalls installed in this space</li> </ul> </li> <li><code>availabilitytarget</code> <ul> <li><code>project</code> context: gives you all Availability Targets of the project</li> <li><code>clustergroup</code> context: gives you only the AVTs that have clusters selected in that cluster group</li> </ul> </li> <li><code>traits</code> - <code>project</code> context, this is because traits are provided out of the box, and at the time of writing it is not possible to create custom traits or capabilities.</li> </ul>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#is-it-possible-to-create-custom-capabilities-and-traits","title":"Is it possible to create custom capabilities and traits","text":"<p>No, not at time of this writing.</p>"},{"location":"tanzu/tanzu-platform/tanzu-platform-technical-details/#when-deploying-this-sample-app-in-the-space-it-deploys-the-app-itself-the-istio-gateway-pod-a-httproute-and-a-gateway-when-i-have-created-a-space-it-already-deployed-the-spring-cloud-gateway-pod-and-multicloud-ingress-pod-how-does-ingress-to-the-smoketest-app-actually-work","title":"When deploying this sample app in the space, it deploys the app itself, the istio gateway pod, a httproute and a gateway. When I have created a space, it already deployed the Spring cloud gateway pod and multicloud-ingress pod. How does ingress to the smoketest app actually work?","text":"<p>Pods that will be deployed when creating a space: <code>spring-cloud-gateway</code>, <code>multicloud-ingress-operator</code></p> <p>Pods that will be deployed when deploying the app: <code>spring-smoketest</code>, <code>default-gateway-istio</code></p> <p><code>spring-cloud-gateway</code> along with a a <code>kind: SpringCloudGateway</code> has been deployed because the <code>spring-cloud-gateway.tanzu.vmware.com</code> capability has been installed for the cluster group.</p> <p>The <code>multicloud-ingress-operator</code> has been deployed because the <code>tcs.tanzu.vmware.com</code> capability, which includes <code>multicloud-ingress.tanzu.vmware.com</code> capability, has been installed for the cluster group. It is responsible to spin up the istio gateway pod and the Gateway CR when deploying an app into the space.</p> <p>Ingress to an application in general goies like this: </p> <pre><code>Internet\n    ==&gt; External Load Balancer \n    ==&gt; platform-scoped istio ingress gateway (istio-ingress namespace)\n    ==&gt; space-scoped istio ingress gateway\n    ==&gt; app\n</code></pre> <p>When using Spring Cloud Gateway for ingress, it is an additional hop in the ingress chain: internet ==&gt; platform istio ingress gateway ==&gt; space-scoped istio ingress gateway == via HTTPRoute ==&gt; SCG ==&gt; app.</p> <p>When not using Spring Cloud Gateway, the ingress chain is: internet ==&gt; platform istio ingress gateway ==&gt; space-scoped istio ingress gateway == via HTTPRoute ==&gt; app.</p>"},{"location":"tanzu/tkgs/disaster-scenarios/","title":"Disaster Scenarios","text":"<p>This page explores certain disaster scenarios, how Tanzu behaves and how to repair the environment.</p> <p>Info</p> <p>This page is based on vSphere IaaS Control Plane, formerly known as vSphere with Tanzu aka TKGS. You should first know about the basic concepts before reading this page.</p>"},{"location":"tanzu/tkgs/disaster-scenarios/#crash-of-a-worker-node-on-a-guest-cluster","title":"Crash of a Worker Node on a guest cluster","text":""},{"location":"tanzu/tkgs/disaster-scenarios/#power-off-worker-node-in-vcenter","title":"Power off Worker node in vCenter","text":"<p>What happens if we manually power off a Virtual Machine in vCenter which is a worker node of a guest cluster?</p> <ul> <li>the corresponding Kubernetes node enters the state <code>NotReady</code></li> <li>the Supervisor Cluster still shows the corresponding <code>Machine</code> (see Cluster API Machine) to be <code>Running</code> and <code>VirtualMachine</code> (a CAPV object) to be <code>poweredOn</code></li> <li>pods that should be scheduled on that node are in <code>Pending</code> state</li> <li> <p>logs in <code>capi-controller-manager</code> (from MachineSet Controller):</p> <pre><code>Waiting for the Kubernetes node on the machine to report ready state\n</code></pre> </li> <li> <p>after 5 minutes, the Supervisor Cluster (actually the Cluster API controllers) deletes the Virtual Machine in vCenter and the Kubernetes nodes gets deleted</p> </li> <li>finally a new Virtual Machine gets created and the node joins the cluster</li> </ul>"},{"location":"tanzu/tkgs/disaster-scenarios/#delete-worker-vm-in-vcenter-delete-from-disk","title":"Delete Worker VM in vCenter (Delete from Disk)","text":"<p>The behavior is quite similar to powering off the VM:</p> <ul> <li>corresponding Kubernetes node marked as <code>NotReady</code></li> <li>after a few minutes the nodes enter <code>SchedulingDisabled</code>, but the Supervisor Cluster still shows the <code>Machine</code> to be running</li> <li> <p>after some time, logs in capi-controller-manager:</p> <pre><code>reason=\"UnhealthyNode\" message=\"Condition Ready on node is reporting status Unknown for more than 5m0s\"\n</code></pre> </li> <li> <p>after some minutes the <code>Machine</code> enters <code>Deleting</code> state</p> </li> <li>the Supervisor Cluster provisions a new node with the same name</li> <li>node persists in state <code>NoteReady,SchedulingDisabled</code> because it is not reachable (as kubelet is not running)</li> <li>the <code>Machine</code> is still in <code>Deleting</code> state</li> <li>after some time the VM gets successfully deleted by CAPI</li> <li>a new VM with new name gets provisioned and node joins the cluster and operating successfully</li> <li>we have a functioning cluster again 13 minutes after we have deleted the VM</li> </ul>"},{"location":"tanzu/tkgs/disaster-scenarios/#crash-of-a-control-plane-node-on-a-guest-cluster","title":"Crash of a Control Plane Node on a guest cluster","text":""},{"location":"tanzu/tkgs/disaster-scenarios/#power-off-control-plane-node-of-guest-cluster","title":"Power off Control Plane node of guest cluster","text":"<ul> <li>Kubernetes API Server not available for a few seconds. This is because we were connected to that node that we powered off, if we would have powered off another CP node, we wouldn't even have recognized it</li> <li>VM powered on immediately after a few seconds</li> <li>It is that quick that no pods get terminated</li> </ul>"},{"location":"tanzu/tkgs/disaster-scenarios/#delete-cp-vm-in-vcenter-delete-from-disk","title":"Delete CP VM in vCenter (Delete from Disk)","text":"<ul> <li>in less than a minute a new VM with the same name gets deployed in vCenter and powered on</li> <li> <p>the node is still marked <code>NotReady</code> because kubelet is not running and printing logs:</p> <pre><code>\"command failed\" err=\"failed to load kubelet config file, error: failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file \\\"/var/lib/kubelet/config.yaml\\\", error: open /var/lib/kubelet/config.yaml: no such file or directory, path: /var/lib/kubelet/config.yaml\"\n</code></pre> </li> <li> <p>this indicates that kubeadm does not join the control plane successfully to the cluster. Indeed, the <code>cloud-init-output.log</code> file prints</p> <pre><code>-info ConfigMap does not yet contain a JWS signature for token ID \"5j3q2p\", will try again\n[2024-09-12 14:52:39] I0912 14:52:39.589511    1244 token.go:223] [discovery] The cluster-info ConfigMap does not yet contain a JWS signature for token ID \"5j3q2p\", will try again\n[2024-09-12 14:52:43] error execution phase preflight: couldn't validate the identity of the API Server: could not find a JWS signature in the cluster-info ConfigMap for token ID \"5j3q2p\"\n[2024-09-12 14:52:43] To see the stack trace of this error execute with --v=5 or higher\n[2024-09-12 14:52:43] !!! [2024-09-12T14:52:43+00:00] kubeadm reported failed action(s) for 'kubeadm join phase preflight --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests'\n[2024-09-12 14:52:58] +++ [2024-09-12T14:52:58+00:00] running 'kubeadm join phase preflight --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests'\n</code></pre> </li> <li> <p>deploying pods is still working, although the control plane node is in <code>NotReady</code> state and other etcd pods are not able to connect to this etcd instance. The other two etcd instances are logging</p> <pre><code>\"dial tcp 10.244.0.34:2380: connect: connection refused\"\n</code></pre> </li> <li> <p>after a few minutes the node enters <code>SchedulingDisabled</code></p> </li> <li>after some more minutes, the <code>Machine</code> enters <code>Deleting</code> state, but  thge VirtualMachine is still <code>poweredOn</code></li> <li>after some time the VM gets deleted and a new one with a new name gets provisioned and joined to the cluster successfully</li> <li>we have a functioning cluster again ~15 minutes after we have deleted the VM</li> </ul>"},{"location":"tanzu/tkgs/disaster-scenarios/#kubelet-crashing","title":"Kubelet Crashing","text":""},{"location":"tanzu/tkgs/disaster-scenarios/#kubelet-not-running-on-a-worker-node","title":"Kubelet not running on a worker node","text":""},{"location":"tanzu/tkgs/disaster-scenarios/#stop-kubelet-on-a-worker-node","title":"Stop kubelet on a worker node","text":"<p>In this scenario, we simply stopped kubelet service on a node.</p> <p>The behavior is same as if you Power off a Worker node in vCenter.</p>"},{"location":"tanzu/tkgs/disaster-scenarios/#let-kubelet-crash-on-a-worker-node","title":"Let kubelet crash on a worker node","text":"<p>In this scenario, we removed the kubelet config file.</p> <p>The behavior is same as if you Power off a Worker node in vCenter.</p>"},{"location":"tanzu/tkgs/disaster-scenarios/#kubelet-not-running-on-a-control-plane-node","title":"Kubelet not running on a control plane node","text":"<p>We would guess the behaviour is similar to worker nodes, but it's not surprisingly.</p>"},{"location":"tanzu/tkgs/disaster-scenarios/#stop-kubelet-on-a-control-plane-node","title":"Stop kubelet on a control plane node","text":"<p>In this scenario, we simply stopped kubelet service on a node.</p> <p>The node enters <code>NotReady</code> state and stays there until we fix and start kubelet again. Cluster API does not recreate the node.</p>"},{"location":"tanzu/tkgs/disaster-scenarios/#let-kubelet-crash-on-a-control-plane-node","title":"Let kubelet crash on a control plane node","text":"<p>In this scenario, we removed the kubelet config file.</p> <p>The behaviour is same as if you Stop kubelet on a control plane node.</p>"},{"location":"tanzu/tkgs/disaster-scenarios/#etcd-not-working-properly","title":"etcd not working properly","text":""},{"location":"tanzu/tkgs/disaster-scenarios/#only-1-out-of-3-etcd-instances-running","title":"only 1 out of 3 etcd instances running","text":"<ul> <li>if you have only 2 etcd instances and you kill one of it, etcd tries to do a new leader election, also if you explicitly don't kill the leader</li> <li>because etcd doesn't reach a quorum, as there is only 1 out of 3 instances running, the remaining instance will never become a leader</li> <li>as a result, <code>kube-apiserver</code> is failing with the error <code>watch chan error: etcdserver: no leader</code> - apparently when apiserver receives this error, it terminates all watchers according to this description</li> <li>the apiserver is not able to recreate the watchers because there is no etcd leader</li> <li>Tanzu (Cluster API) does not recreate a node with a crashing etcd</li> </ul> <p>As a result:</p> <ul> <li>you are not able to communicate with the cluster using <code>kubectl</code> (or else to the API Server)</li> <li>because etcd is in readonly mode, the self-healing mechanism in Kubernetes doesn't work anymore</li> <li>running apps on the cluster are still working</li> <li>ingress to web apps is still functioning</li> </ul>"},{"location":"tanzu/tkgs/disaster-scenarios/#kubernetes-api-server-crashing","title":"Kubernetes API-Server Crashing","text":""},{"location":"tanzu/tkgs/disaster-scenarios/#kube-apiserver-not-running","title":"kube-apiserver not running","text":"<ul> <li>we removed the <code>kube-apiserver</code> static pod on one control plane, which didn't have any effect on the cluster and it was still operational</li> <li>we removed the <code>kube-apiserver</code> static pod on the second control plane, which also didn't have any effect on the cluster and it was still operational</li> <li>after removing <code>kube-apiserver</code> from the third control plane, then the API Server was not reachable anymore of course, but apps were still running fine.</li> </ul>"},{"location":"tanzu/tkgs/supervisor-cluster-upgrade-failed-in-vsphere-8/","title":"Supervisor Cluster upgrade failed in vSphere 8u2b","text":""},{"location":"tanzu/tkgs/supervisor-cluster-upgrade-failed-in-vsphere-8/#issue-description","title":"Issue Description","text":"<p>When I upgraded the Supervisor cluster from 1.25.6 to 1.26.8 on vCenter version 8u2b which has been recently upgraded from 7u3p, I faced an issue that the upgrade process was stuck at 50% in vCenter and the <code>vsphere-csi-controller</code> pods in namespace <code>vmware-system-csi</code> were in <code>CrashLoopBackOff</code> resulting in the error</p> <pre><code>failed to init controller. Error: could not find any AvailabilityZone\n</code></pre> <p>and <code>kubectl get az</code> on the Supervisor Clusters doesn't  show any output.</p> <p>When ssh'ing into a Supervisor Cluster Control Plane node you can execute the following script to find more details of the upgrade process with</p> <pre><code>/usr/lib/vmware-wcp/upgrade/upgrade-ctl.py get-status \\\n  | jq '.progress | to_entries | .[] | \"\\(.value.status) - \\(.key)\"' \\\n  | sort\n</code></pre> <p>This gave me the following result:</p> <pre><code>\"failed - ImageRegistryUpgrade\"\n\"pending - CapvUpgrade\"\n\"pending - CertManagerAdditionalUpgrade\"\n\"pending - LicenseOperatorControllerUpgrade\"\n\"pending - NamespaceOperatorControllerUpgrade\"\n\"pending - PinnipedUpgrade\"\n\"pending - PspOperatorUpgrade\"\n\"pending - TkgUpgrade\"\n\"pending - TMCUpgrade\"\n\"pending - VmOperatorUpgrade\"\n\"processing - UtkgControllersUpgrade\"\n\"skipped - HarborUpgrade\"\n\"skipped - LoadBalancerApiUpgrade\"\n\"upgraded - AKOUpgrade\"\n\"upgraded - AppPlatformOperatorUpgrade\"\n\"upgraded - CapwUpgrade\"\n\"upgraded - CertManagerUpgrade\"\n\"upgraded - CsiControllerUpgrade\"\n\"upgraded - ExternalSnapshotterUpgrade\"\n\"upgraded - ImageControllerUpgrade\"\n\"upgraded - KappControllerUpgrade\"\n\"upgraded - NetOperatorUpgrade\"\n\"upgraded - NSXNCPUpgrade\"\n\"upgraded - RegistryAgentUpgrade\"\n\"upgraded - SchedextComponentUpgrade\"\n\"upgraded - SphereletComponentUpgrade\"\n\"upgraded - TelegrafUpgrade\"\n\"upgraded - UCSUpgrade\"\n\"upgraded - UtkgClusterMigration\"\n\"upgraded - VMwareSystemLoggingUpgrade\"\n\"upgraded - WCPClusterCapabilities\"\n</code></pre> <p>where we can see that the <code>ImageRegistryUpgrade</code> failed. Looking further into <code>var/log/VMware/upgrade-ctl-compupgrade.log</code> we could see the following error:</p> <pre><code>2024-06-18T05:33:08.437Z DEBUG comphelper: ret=0 out={\n    \"apiVersion\": \"v1\",\n    \"items\": [\n        {\n            \"apiVersion\": \"imageregistry.vmware.com/v1alpha1\",\n            \"kind\": \"ContentLibrary\",\n            \"metadata\": {\n                \"annotations\": {\n                    \"kubectl.kubernetes.io/last-applied-configuration\": \"{\\\"apiVersion\\\":\\\"imageregistry.vmware.com/v1alpha1\\\",\\\"kind\\\":\\\"ContentLibrary\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"cl-fe4e8c74d59491be7\\\",\\\"namespace\\\":\\\"test-vsphere-namespace\\\"},\\\"spec\\\":{\\\"uuid\\\":\\\"5f773a1c-5aa3-4268-871a-359401c55950\\\",\\\"writable\\\":false}}\\n\"\n2024-06-18T05:33:08.438Z ERROR compupgrade: {\"error\": \"TypeError\", \"message\": \"argument of type 'NoneType' is not iterable\", \"backtrace\": [\"  File \\\"/usr/lib/vmware-wcp/upgrade/compupgrade.py\\\", line 362, in do_upgrade_with_out_resume_failed_support\\n    comp.doUpgrade(upCtx)\\n\", \"  File \\\"/usr/lib/vmware-wcp/objects/image-registry-operator/imageregistry_upgrade.py\\\", line 323, in doUpgrade\\n    self.updateV1alpha1ImageRegistryResources()\\n\", \"  File \\\"/usr/lib/vmware-wcp/objects/image-registry-operator/imageregistry_upgrade.py\\\", line 171, in updateV1alpha1ImageRegistryResources\\n    self.updateV1alpha1Resource('contentlibraries', True)\\n\", \"  File \\\"/usr/lib/vmware-wcp/objects/image-registry-operator/imageregistry_upgrade.py\\\", line 193, in updateV1alpha1Resource\\n    patch_list = self.getResourcePatchBody(status, resource_kind)\\n\", \"  File \\\"/usr/lib/vmware-wcp/objects/image-registry-operator/imageregistry_upgrade.py\\\", line 218, in getResourcePatchBody\\n    if 'UTC' in creation_time:\\n\"]}\n</code></pre> <p>The error <code>argument of type 'NoneType' is not iterable</code> first indicated that the root cause was because of the missing Availability Zone as mentioned earlier. But digging into the logs of the <code>imageregistry-controller-manager</code> we could see the following errors:</p> <pre><code>\"msg\"=\"Reconciler error\" \"error\"=\"The underlying content library with ID 5f773a1c-5aa3-4268-871a-359401c55950 does not exist in vSphere\"\n</code></pre>"},{"location":"tanzu/tkgs/supervisor-cluster-upgrade-failed-in-vsphere-8/#root-cause","title":"Root Cause","text":"<p>There is an operator introduced in vCenter 8.x called <code>ImageRegistryOperator</code> that takes over some of the responsibilities of <code>VMoperator</code>. Part of the upgrade script is to migrate VMoperator's <code>ContentSource</code> and <code>ContentSourceBindings</code> to newer CRDs (<code>ContentLibrary</code>, <code>ClusterContentLibrary</code>, <code>ContentLibraryItems</code> etc.).</p> <p>There was a bug in the upgrade script (already fixed in newer versions of vCenter) that is exposed when a Supervisor namespace references a content library that does not exist, e.g. because it was removed from vCenter. Other component upgrades stalled out because they depend on <code>ImageRegistryUpgrade</code> to be finished - this also caused the <code>CrashLoopBackOff</code> of the <code>csi-controller</code> because <code>VmOperatorUpgrade</code> hasn't started yet and has not yet created the missing <code>AvailabilityZone</code> custom resource on the Supervisor Cluster.</p> <p>In this case, the content library <code>5f773a1c-5aa3-4268-871a-359401c55950</code> has been already deleted in vCenter, but it's still associated on some namespaces. This can be seen using the VMware Datacenter CLI (dcli):</p> <pre><code>dcli&gt; namespaces instances get --namespace &lt;vsphere-namespace-name&gt;\n</code></pre> <p>or when executing <code>kubectl get contentsourcebinding -A</code> on the Supervisor Cluster. Also, the orphaned <code>ContentLibrary</code> resource is still present on the Supervisor cluster when executing <code>kubectl get contentlibrary</code>.</p>"},{"location":"tanzu/tkgs/supervisor-cluster-upgrade-failed-in-vsphere-8/#resolution","title":"Resolution","text":"<ol> <li> <p>delete the content library reference on all vSphere namespaces using dcli:</p> <pre><code>dcli&gt; namespaces instances update --namespace &lt;vsphere-namespace-name&gt; --content-libraries '[]'\n</code></pre> </li> <li> <p>delete all <code>contentsourcbindings</code> related to the orphaned Content Library with</p> <pre><code>kubectl delete contentsourcebinding -n &lt;vsphere-namespace-name&gt; 5f773a1c-5aa3-4268-871a-359401c55950\n</code></pre> </li> <li> <p>delete the orphaned <code>contentlibrary</code> reference on the Supervisor cluster with</p> <pre><code>kubectl delete contentlibrary &lt;name&gt;\n</code></pre> </li> <li> <p>ensure the last step also deleted the corresponding <code>contentsource</code> with <code>kubectl get contentsource</code>.</p> </li> </ol> <p>The Supervisor Cluster upgrade process will be retried automatically. You can follow the progress again with</p> <pre><code>/usr/lib/vmware-wcp/upgrade/upgrade-ctl.py get-status \\\n  | jq '.progress | to_entries | .[] | \"\\(.value.status) - \\(.key)\"' \\\n  | sort\n</code></pre> <p>and you should see that <code>ImageRegistryUpgrade</code> process should complete successfully. After <code>VmOperatorUpgrade</code> has been completed you should also see the missing <code>AvailabilityZone</code> with <code>kubectl get az</code> and the <code>csi-controller</code> pods running successfully.</p>"},{"location":"tanzu/tkgs/tkgs-airgapped/","title":"Set up an airgapped TKGS homelab environment","text":"<p>See Set up an airgapped TKGS homelab environment.</p>"},{"location":"tanzu/tkgs/tkgs-nsxt-integration/","title":"vSphere with Tanzu integration with NSX-T","text":"<p>This page explains how Tanzu Kubernetes Clusters, sometimes referred to as guest clusters or workload clusters, integrate with NSX-T to provide L4 Load Balancing services via Kubernetes Services of type LoadBalancer.</p>"},{"location":"tanzu/tkgs/tkgs-nsxt-integration/#general-concept-of-load-balancer-integration","title":"General concept of Load Balancer integration","text":"<p>A Kubernetes Service of type LoadBalancer is an upstream Kubernetes concept that assigns a routable external IP address to a Kubernetes Service and, thus, make the service available from outside of the cluster. This IP assignment has to happen by the underlying infrastructure, the underlying network infrastructure to be more specific. Of course Kubernetes does not know and care about the underlying infrastructure, so it is the responsibility of cloud providers and infrastructure providers to implement this functionality.</p> <p>If you've read Kubernetes Architecture Concepts you know that everything in Kubernetes is implemented as controllers. In essence, an infrastructure provider has to implement a load balancer controller and run it on the cluster:</p> <p></p> <p>The load balancer controller watches for <code>Create</code>, <code>Update</code> and <code>Delete</code> events of Services of type <code>LoadBalancer</code>, interacts with the underlying network infrastructure provider and reports back status.</p> <p>As an example:</p> <ol> <li>a user creates a service of type <code>LoadBalancer</code></li> <li>the load balancer controller reacts on this event and determines it is a <code>Create</code> event</li> <li>load balancer controller communicates with the network infrastructure API and creates a Load Balancer, or Virtual Service or similar</li> <li>the network infrastructure API responds with an external IP address</li> <li>the load balancer controller updates the service's <code>status.loadBalancer</code> field and especially updates <code>status.loadBalancer.ingress.ip</code> with this external IP address.</li> </ol>"},{"location":"tanzu/tkgs/tkgs-nsxt-integration/#integration-in-aws","title":"Integration in AWS","text":"<p>When you use Amazon Elastic Kubernetes Service (EKS) there is the AWS Load Balancer Controller running on the EKS cluster responsible for provisioning Network Load Balancers and Application Load Balancers.</p>"},{"location":"tanzu/tkgs/tkgs-nsxt-integration/#integration-in-vsphere-with-tanzu-with-nsx-t","title":"Integration in vSphere with Tanzu with NSX-T","text":"<p>The Load Balancer integration on Tanzu Kubernetes Clusters is not as easy and straightforward as explained above. Indeed, there is a NSX-T load balancer controller, the so-called NSX-T Container Plugin (NCP), which you could deploy directly on a Tanzu Kubernetes Cluster. But, vSphere with Tanzu does not deploy it on the guest cluster because it would require to store NSX-T credentials on that cluster, too.</p> <p>One of the main philosophies in vSphere with Tanzu is having different personas with different responsibilities: a vSphere administrator, a DevOps engineer (or cluster admin) and an application developer. Tanzu Kubernetes Clusters are purposed for DevOps engineers, sometimes even application developers, giving them full cluster admin permissions on that cluster and consequently empowering full flexibility. The NSX-T infrastructure, though, is maintained by vSphere Administrators, or even another dedicated networking team, but is not maintained by those DevOps engineers or application developers. Therefore, credentials to NSX-T must not be exposed on Tanzu Kubernetes Clusters.</p> <p>The solution to still be able to integrate with NSX-T and to still be able to dynamically deploy services of type <code>LoadBalancer</code> and having the full flexibility as a DevOps engineer working with Kubernetes, vSphere with Tanzu realizes it using the following approach:</p> <p></p> <p>Let's see what happens when a user creates a <code>Service of type LoadBalancer</code> on a Tanzu Kubernetes Cluster (the same principle applies for <code>update</code> and <code>delete</code>):</p> <ol> <li>on the guest cluster, there is a controller called <code>guest-cluster-cloud-provider</code> that watches for Kubernetes Service's <code>Create</code>, <code>Update</code> and <code>Delete</code> events. On the <code>Create</code> event it creates a <code>VirtualMachineService</code> Custom Resource on the Supervisor cluster by calling the Supervisor Cluster Kubernetes API. This <code>VirtualMachineService</code> can have different types, in this example it is of type <code>LoadBalancer</code>.</li> <li>On the Supervisor cluster there is a controller called <code>vmop-controller-manager</code> that has multiple responsibilities with watching <code>Create</code>, <code>Update</code> and <code>Delete</code> events of the <code>VirtualMachineService</code> being one of them. On the <code>Create</code> event it creates a Service of type <code>LoadBalancer</code> on the Supervisor cluster which is essentially a mirror of the same service deployed on the guest cluster.</li> <li>From this step onwards, the common techniques applies as described above: the <code>NSX-T Container Plugin (NCP)</code>, the \"NSX-T Load Balancer Controller\" running on the Supervisor cluster, watches for Services and interacts with the NSX-T API. In NSX-T, it creates a Load Balancer and a Virtual Service.</li> <li>NSX-T built-in IP address management system assigns an external IP address to this Virtual Service and responds it back to the API Call initiated by NCP.</li> <li>NCP writes the external IP address to the Service's <code>status.loadBalancer.ingress.ip</code> on the Supervisor cluster.</li> <li>the <code>vmop-controller-manager</code> reacts on this <code>Update</code> event and reports back that status to the <code>VirtualMachineService</code>.</li> <li>the <code>guest-cluster-cloud-provider</code>, which regularly polls the <code>VirtualMachineService</code> (it can't subscribe to <code>Create</code>, <code>Update</code> and <code>Delete</code> as it is running on a different Kubernetes Cluster), finally reports back the status, and especially the external IP address, to the original service of type <code>LoadBalancer</code> which has been created by the end user.</li> </ol> <p>Info</p> <p>As you can see in the image, the actual network traffic from the intranet/internet to the target Kubernetes Service running on the Tanzu Kubernetes Cluster goes directly from NSX-T to the target guest cluster, and not going the route through the Supervisor Cluster. This \"hop\" architecture is only used for creating, updating and deleting services.</p>"}]}